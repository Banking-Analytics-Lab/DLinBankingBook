{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Banking-Analytics-Lab/DLinBankingBook/blob/main/Labs/TextBook_Lab_Chap5_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5tJ_odVX3iYk",
      "metadata": {
        "id": "5tJ_odVX3iYk"
      },
      "source": [
        "# Default correlation in mortgage lending\n",
        "\n",
        "This notebook demonstrates the application of Graph Neural Networks (GNNs) for predicting mortgage defaults using loan-level data from Freddie Mac.\n",
        "\n",
        "The notebook is in two parts which you can run independently. The first part focuses on processing the data to create the networks whereas the second part focuses on building and training the GNN models using PyTorch Geometric.\n",
        "\n",
        "We start by loading the necessary libraries and the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xzTOryZV2oHu",
      "metadata": {
        "id": "xzTOryZV2oHu"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9baad39a",
      "metadata": {
        "id": "9baad39a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "import PIL\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc,accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.utils import from_networkx, degree\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomNodeSplit\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fs9BNL3KWSJo",
      "metadata": {
        "id": "Fs9BNL3KWSJo"
      },
      "outputs": [],
      "source": [
        "!gdown '1YadoR0hR_uZJe5XyGhh0PUwjqfkeJu7k'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6z06ZZdR9uEx",
      "metadata": {
        "id": "6z06ZZdR9uEx"
      },
      "source": [
        "## Part 1: Data processing and network building\n",
        "A dataset similar to the one for the time series lab has been prepared. We read it in and look at the first few lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WGCu63vXV5Jo",
      "metadata": {
        "id": "WGCu63vXV5Jo"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(\"graph_df.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ml1_buiX-Tt6",
      "metadata": {
        "id": "ml1_buiX-Tt6"
      },
      "source": [
        "We will use the AREA and and PROVIDER variables as connector variables to build our network. They are both categorical variables. But we need to do some preprocessing before we continue\n",
        "\n",
        "First, we will remove all the categories which only have a single borrower as the would end up a singletons in our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YLJJVmyfWheF",
      "metadata": {
        "id": "YLJJVmyfWheF"
      },
      "outputs": [],
      "source": [
        "area_counts = data['AREA'].value_counts()\n",
        "data = data[data['AREA'].isin(area_counts[area_counts > 1].index)].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QEGdNbhz-yIZ",
      "metadata": {
        "id": "QEGdNbhz-yIZ"
      },
      "source": [
        "For the PROVIDER variable we remove the value Other sellers as it may introduce unnecessary noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eVaadknlWjvK",
      "metadata": {
        "id": "eVaadknlWjvK"
      },
      "outputs": [],
      "source": [
        "data = data[data['PROVIDER'] != 'Other sellers'].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0qVCtNIL_NU2",
      "metadata": {
        "id": "0qVCtNIL_NU2"
      },
      "source": [
        "Now, we create the edge lists for our networks. To do this, we loop over all the different values in the connector variables and identify which borrowers share values. We do this separately for the AREA variable and the PROVIDER variable.\n",
        "\n",
        "Note that there are two variants in the next two cells. One to create the full edgelist which is used in the analyses, and one to create a smaller edge list (with only 1000 borrowers) which is used to plot the network. You can comment out the lines you do not need.\n",
        "\n",
        "Running the cells for the full edge list can take a long time and therefore we recommend saving the result at the end. This line is commented out below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eGrH-fMkWoJD",
      "metadata": {
        "id": "eGrH-fMkWoJD"
      },
      "outputs": [],
      "source": [
        "#area edge list\n",
        "el = []\n",
        "#for i in tqdm(range(len(data))): #For full edgelist\n",
        "#   for j in range(len(data)): #For full edgelist\n",
        "for i in tqdm(range(1000)): #For smaller edgelist\n",
        "    for j in range(1000): #For smaller edgelist\n",
        "        if data.loc[i, 'AREA'] == data.loc[j, 'AREA']:\n",
        "            el.append((data.loc[i,'LOAN_NUMBER'], data.loc[j,'LOAN_NUMBER']))\n",
        "el_df = pd.DataFrame(el)\n",
        "index = el_df[el_df.loc[:, 0] == el_df.loc[:, 1]].index\n",
        "el_df.drop(index, inplace = True)\n",
        "el_df.reset_index(drop = True, inplace = True)\n",
        "el_df.rename(columns = {0:'source', 1:'target'}, inplace = True)\n",
        "#el_df.to_csv('areaEdgeList.csv', index = False)\n",
        "area=el_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mMvlswaJWulq",
      "metadata": {
        "id": "mMvlswaJWulq"
      },
      "outputs": [],
      "source": [
        "el = []\n",
        "#for i in tqdm(range(len(data))):#For full edgelist\n",
        "    #for j in range(len(data)):#For full edgelist\n",
        "for i in tqdm(range(1000)):#For smaller edgelist\n",
        "   for j in range(1000):#For smaller edgelist\n",
        "        if data.loc[i, 'PROVIDER'] == data.loc[j, 'PROVIDER']:\n",
        "            el.append((data.loc[i,'LOAN_NUMBER'], data.loc[j,'LOAN_NUMBER']))\n",
        "el_df = pd.DataFrame(el)\n",
        "index = el_df[el_df.loc[:, 0] == el_df.loc[:, 1]].index\n",
        "el_df.drop(index, inplace = True)\n",
        "el_df.reset_index(drop = True, inplace = True)\n",
        "el_df.rename(columns = {0:'source', 1:'target'}, inplace = True)\n",
        "#el_df.to_csv('providerEdgeList.csv', index = False)\n",
        "comp=el_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QRuce821BG3l",
      "metadata": {
        "id": "QRuce821BG3l"
      },
      "source": [
        "Next, we combine the two edgelists into one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t6wtg1vMW-Jl",
      "metadata": {
        "id": "t6wtg1vMW-Jl"
      },
      "outputs": [],
      "source": [
        "edge_df = pd.concat([area, comp], axis = 0)\n",
        "edge_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a-TaYZWnBN1x",
      "metadata": {
        "id": "a-TaYZWnBN1x"
      },
      "source": [
        "We create a network object from our edgelist and add the features as node attributes and then relabel the nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HvhZfeB6XAtv",
      "metadata": {
        "id": "HvhZfeB6XAtv"
      },
      "outputs": [],
      "source": [
        "G_areacomp = nx.from_pandas_edgelist(edge_df, source = \"source\", target = \"target\")\n",
        "G_areacomp.number_of_nodes(), G_areacomp.number_of_edges()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RcwACAYsXC5F",
      "metadata": {
        "id": "RcwACAYsXC5F"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(G_areacomp.nodes()):\n",
        "    id_loan = data[data['LOAN_NUMBER'] == i]['LOAN_NUMBER'].tolist()[0]\n",
        "    for f in data.columns[0:23]:\n",
        "\n",
        "\n",
        "        G_areacomp.nodes[i][f] = data[f][data['LOAN_NUMBER'] == i].tolist()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l9yMc1YIXE58",
      "metadata": {
        "id": "l9yMc1YIXE58"
      },
      "outputs": [],
      "source": [
        "mapping = {old_label: new_label for new_label, old_label in enumerate(G_areacomp.nodes())}\n",
        "H_relabel = nx.relabel_nodes(G_areacomp, mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IXjThr32Bk7E",
      "metadata": {
        "id": "IXjThr32Bk7E"
      },
      "source": [
        "We can also plot our network (this is only recommneded for the smaller network). We can see that defaulted borrowers (the dark gray nodes) are somewhat clustered together. This indicates that  default is correlated and motivates our approach of using the network information to predict default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H94AtwMiXOOB",
      "metadata": {
        "id": "H94AtwMiXOOB"
      },
      "outputs": [],
      "source": [
        "node_colors = [\n",
        "    'black' if H_relabel.nodes[n].get('target', 0) == 1 else 'grey'\n",
        "    for n in H_relabel.nodes()\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(H_relabel, seed=42)\n",
        "nx.draw(\n",
        "    H_relabel,\n",
        "    pos,\n",
        "    node_size=40,\n",
        "    node_color=node_colors,\n",
        "    edge_color='lightgray',\n",
        "    width=0.5,\n",
        "    with_labels=False\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-VHyTu_nCHVV",
      "metadata": {
        "id": "-VHyTu_nCHVV"
      },
      "source": [
        "To avoid long running times, we are here loading the edgelists of the full dataset. You can therefore skip the long-running for loops above.\n",
        "We then continue to create the network and add the features as node attributes as above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N12yZa4xaCfK",
      "metadata": {
        "id": "N12yZa4xaCfK"
      },
      "outputs": [],
      "source": [
        "!gdown '17wG2Bo0ENVU74Rz5UI2y2jbZc4rfNSHU'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E6UMYKhhaZmR",
      "metadata": {
        "id": "E6UMYKhhaZmR"
      },
      "outputs": [],
      "source": [
        "!gdown '1Nk_eMZLrkfCmdwHv5OCax2gh52mUZVxw'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lKsFX1EoW10A",
      "metadata": {
        "id": "lKsFX1EoW10A"
      },
      "outputs": [],
      "source": [
        "area    = pd.read_csv('areaEdgeList.csv')\n",
        "comp    = pd.read_csv('providerEdgeList.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wiIRlqecYpQ9",
      "metadata": {
        "id": "wiIRlqecYpQ9"
      },
      "outputs": [],
      "source": [
        "edge_df = pd.concat([area, comp], axis = 0)\n",
        "edge_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5y4skJsYsWa",
      "metadata": {
        "id": "d5y4skJsYsWa"
      },
      "outputs": [],
      "source": [
        "G_areacomp = nx.from_pandas_edgelist(edge_df, source = \"source\", target = \"target\")\n",
        "G_areacomp.number_of_nodes(), G_areacomp.number_of_edges()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ius6K7rhYwXC",
      "metadata": {
        "id": "ius6K7rhYwXC"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(G_areacomp.nodes()):\n",
        "    id_loan = data[data['LOAN_NUMBER'] == i]['LOAN_NUMBER'].tolist()[0]\n",
        "    for f in data.columns[0:23]:\n",
        "        G_areacomp.nodes[i][f] = data[f][data['LOAN_NUMBER'] == i].tolist()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "re3sQM6qYzkz",
      "metadata": {
        "id": "re3sQM6qYzkz"
      },
      "outputs": [],
      "source": [
        "mapping = {old_label: new_label for new_label, old_label in enumerate(G_areacomp.nodes())}\n",
        "H_relabel = nx.relabel_nodes(G_areacomp, mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DhPcp32OCuRb",
      "metadata": {
        "id": "DhPcp32OCuRb"
      },
      "source": [
        "Finally, we save our network object to not have to run the above code again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qoh8YeclY2bf",
      "metadata": {
        "id": "Qoh8YeclY2bf"
      },
      "outputs": [],
      "source": [
        "with open('network.gpickle', 'wb') as f:\n",
        "    pickle.dump(H_relabel, f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FaBvHllNC1ik",
      "metadata": {
        "id": "FaBvHllNC1ik"
      },
      "source": [
        "## Part 2: GNN models\n",
        "To start our model building we load the network object we created in the previous part.\n",
        "\n",
        "Note: you can start running the notebook from here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X-XdhvK_2hsB",
      "metadata": {
        "id": "X-XdhvK_2hsB"
      },
      "outputs": [],
      "source": [
        "!gdown 'https://drive.google.com/uc?id=1-AOl7sCEusPqmctnStdPg8Frc9XEOcKf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3798347f",
      "metadata": {
        "id": "3798347f"
      },
      "outputs": [],
      "source": [
        "with open('network.gpickle', 'rb') as f:\n",
        "    G = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "Xjsb5ShmBc2l"
      },
      "id": "Xjsb5ShmBc2l"
    },
    {
      "cell_type": "markdown",
      "id": "jKcr-KVSFv_X",
      "metadata": {
        "id": "jKcr-KVSFv_X"
      },
      "source": [
        "Next, we create a PyG Data object, starting with adding the following elements\n",
        "- edgelist: the argument `edge_index`\n",
        "- target variable: the argument `y`\n",
        "- node attributes: the argument `x`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11cb5120",
      "metadata": {
        "id": "11cb5120"
      },
      "outputs": [],
      "source": [
        "edge_list = nx.to_pandas_edgelist(G)\n",
        "edge_i = torch.tensor(edge_list.values, dtype=torch.long).t().contiguous()\n",
        "myData = Data(edge_index=edge_i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63664dc5",
      "metadata": {
        "id": "63664dc5"
      },
      "outputs": [],
      "source": [
        "target_attribute = list(nx.get_node_attributes(G, 'target').values())\n",
        "myData.y = torch.tensor(target_attribute, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9a3078a",
      "metadata": {
        "id": "a9a3078a"
      },
      "outputs": [],
      "source": [
        "node_data = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')\n",
        "tmp=node_data.iloc[:,0:21]\n",
        "myData.x=torch.tensor(tmp.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aFQJEjqGAAJ",
      "metadata": {
        "id": "8aFQJEjqGAAJ"
      },
      "source": [
        "To facilitate the GNNs learning we add the nodes' degree as a node attribute.\n",
        "\n",
        "\n",
        "\n",
        "This step can be skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569ecc0c",
      "metadata": {
        "id": "569ecc0c"
      },
      "outputs": [],
      "source": [
        "edge_index = myData.edge_index\n",
        "num_nodes = myData.num_nodes\n",
        "deg = degree(edge_index[0], num_nodes=num_nodes)\n",
        "deg = deg.view(-1, 1).float()\n",
        "myData.x = torch.cat([myData.x, deg], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7111537e",
      "metadata": {
        "id": "7111537e"
      },
      "outputs": [],
      "source": [
        "myData.x.shape  # Check the shape of the updated node features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LrLCLsKXGKa8",
      "metadata": {
        "id": "LrLCLsKXGKa8"
      },
      "source": [
        "Next we create the masks for training, validation and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e52c1f",
      "metadata": {
        "id": "10e52c1f"
      },
      "outputs": [],
      "source": [
        "node_transform = RandomNodeSplit(split='train_rest',num_val=10000, num_test=10000,num_train_per_class=900)\n",
        "node_splits = node_transform(myData)\n",
        "node_splits.x = node_splits.x.float()\n",
        "node_splits.y = node_splits.y.float()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a sanity check, we do some inspection of our network."
      ],
      "metadata": {
        "id": "3rbzqrFdAhug"
      },
      "id": "3rbzqrFdAhug"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c08e8417",
      "metadata": {
        "id": "c08e8417"
      },
      "outputs": [],
      "source": [
        "print()\n",
        "print(f'Dataset: {node_splits}:')\n",
        "print('======================')\n",
        "print(f'Number of features: {node_splits.num_features}')\n",
        "#print(f'Number of classes: {myData.num_classes}')\n",
        "print(f'Number of defaults: {(node_splits.y.sum())}')\n",
        "#===========================================================================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {node_splits.num_nodes}')\n",
        "print(f'Number of edges: {node_splits.num_edges}')\n",
        "print(f'Average node degree: {node_splits.num_edges / node_splits.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {node_splits.train_mask.sum()}')\n",
        "print(f'Number of training nodes: {node_splits.val_mask.sum()}')\n",
        "print(f'Number of training nodes: {node_splits.test_mask.sum()}')\n",
        "print(f'Training default rate: {node_splits.y[node_splits.train_mask].sum() / node_splits.train_mask.sum():.2f}')\n",
        "print(f'Validation default rate: {node_splits.y[node_splits.val_mask].sum() / node_splits.val_mask.sum():.2f}')\n",
        "print(f'Test default rate: {node_splits.y[node_splits.test_mask].sum() / node_splits.test_mask.sum():.2f}')\n",
        "print(f'Has isolated nodes: {node_splits.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {node_splits.has_self_loops()}')\n",
        "print(f'Is undirected: {node_splits.is_undirected()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we scale the node attributes which is usually a good idea before training any neural network."
      ],
      "metadata": {
        "id": "X0govqvQBC7N"
      },
      "id": "X0govqvQBC7N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dfc50db",
      "metadata": {
        "id": "1dfc50db"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "node_splits.x = torch.tensor(scaler.fit_transform(node_splits.x.numpy()), dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition\n",
        "\n",
        "We are now ready to define our GNN models. We use three different GNN models\n",
        "1. GraphSAGE\n",
        "2. Graph Convolutional Networks (GCN)\n",
        "3. Graph Attention Networks (GAT)\n",
        "\n",
        "We create a separate class for each model.\n",
        "The classes contain a constructor and a forward method an give a single output which is the predicted probability of default.\n",
        "All the models have the same decoder.\n",
        "\n",
        "The models as defined here have gone through parameter tuning, where amongst others different number of GNN and linear layers, different dimensions of hidden and dense layers and differnt levels of dropout where tried. You can add more layers or other functionality to the architecture to see how it imapcts the performance."
      ],
      "metadata": {
        "id": "xZWaOazTBPR1"
      },
      "id": "xZWaOazTBPR1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95c20983",
      "metadata": {
        "id": "95c20983"
      },
      "outputs": [],
      "source": [
        "# 1. GraphSAGE model\n",
        "class BinaryGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels,dense_channels):\n",
        "        super(BinaryGraphSAGE, self).__init__()\n",
        "        self.sage1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.sage2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "\n",
        "\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, dense_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=0.5),\n",
        "            torch.nn.Linear(dense_channels, 1)\n",
        "        )\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.sage1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.sage2(x, edge_index)\n",
        "        x=F.relu(x)\n",
        "        out = self.decoder(x)\n",
        "        return out.view(-1)\n",
        "\n",
        "# 2. GCN model\n",
        "class BinaryGCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels,hidden_dense):\n",
        "        super(BinaryGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, hidden_dense),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=0.5),\n",
        "            torch.nn.Linear(hidden_dense, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        out = self.decoder(x)\n",
        "        return out.view(-1)\n",
        "\n",
        "# 3. GAT model\n",
        "class BinaryGAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, dense_channels,heads):\n",
        "        super(BinaryGAT, self).__init__()\n",
        "        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads,concat=False, dropout=0.5)\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, dense_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=0.5),\n",
        "            torch.nn.Linear(dense_channels, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        out = self.decoder(x)\n",
        "        return out.view(-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training procedure\n",
        "Next we set up our procedure to train the models.\n",
        "\n",
        "- In each epoch, the data is passed through the model, the loss is calculated and backpropagated through the model.\n",
        "- We evaluate the model using accuracy, auc and the confusion matrix.\n",
        "- The training function includes an early stopping functionality with patience\n",
        "- At the end, the best model is loaded and evaluated using the test set.\n",
        "\n",
        "In the following cells we define our loss function and set up our learning."
      ],
      "metadata": {
        "id": "ml7w9KmlDId6"
      },
      "id": "ml7w9KmlDId6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "078ee501",
      "metadata": {
        "id": "078ee501"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, data, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask].float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data, mask, loss_fn):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    probs = torch.sigmoid(out)\n",
        "    preds = (probs > 0.5).float()\n",
        "    y_true = data.y[mask].cpu()\n",
        "    y_probs = probs[mask].cpu()\n",
        "    y_preds = preds[mask].cpu()\n",
        "    acc = accuracy_score(y_true, y_preds)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_probs)\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
        "\n",
        "    except ValueError:\n",
        "        auc = float('nan')\n",
        "\n",
        "    conf_mat = confusion_matrix(y_true, y_preds)\n",
        "\n",
        "    val_loss = loss_fn(out[mask], data.y[mask].float()).item()\n",
        "    return acc, auc, fpr,tpr, conf_mat, val_loss\n",
        "\n",
        "\n",
        "def train_model(model, data, loss_fn, optimizer, patience=20,\n",
        "                max_epochs=1000, save_path=\"best_model.pt\"):\n",
        "    best_val_loss = 1e10\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        loss = train_one_epoch(model, data, optimizer, loss_fn)\n",
        "        losses.append(loss)\n",
        "        _, val_auc,val_fpr,val_tpr, _, val_loss = evaluate(model, data, data.val_mask, loss_fn)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), save_path)  # Save best model to disk\n",
        "\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(save_path))  # Load best model from disk\n",
        "    test_acc, test_auc, test_fpr,test_tpr,conf_mat, _ = evaluate(model, data, data.test_mask, loss_fn)\n",
        "    return losses, val_losses, test_acc, test_auc, test_fpr,test_tpr,conf_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set things up with the appropriate device and send the data there, as is standard.\n"
      ],
      "metadata": {
        "id": "qiafMczfvAt4"
      },
      "id": "qiafMczfvAt4"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = node_splits.to(device)"
      ],
      "metadata": {
        "id": "tgq__99UsqYz"
      },
      "id": "tgq__99UsqYz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data is heavily imbalanced, with only 2% default rate. Therefore we use the binary cross entropy loss function with sigmoid activation where we also use the `pos_weight` argument, which we set equal to the imbalance ratio in our training set."
      ],
      "metadata": {
        "id": "SQ1cB0imvJoZ"
      },
      "id": "SQ1cB0imvJoZ"
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = data.y[data.train_mask]\n",
        "num_pos = (y_train == 1).sum().item()\n",
        "num_neg = (y_train == 0).sum().item()\n",
        "pos_weight = torch.tensor([num_neg / num_pos], device=device)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
      ],
      "metadata": {
        "id": "paIaSeNKu9KE"
      },
      "id": "paIaSeNKu9KE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training GraphSAGE\n",
        "We train the GraphSAGE model and print the performance. The parameters are as follows:\n",
        "- `in_channels`=22: the number of node attributes.\n",
        "- `hidden_channels`=4: The dimension of the GNN embeddings. This model has 2 GNN layers.\n",
        "- `dense_channels`=4: The dimension of the dense embeddigns in the decoder.\n",
        "\n",
        "The loss curves show that the model learns quite well, and we get a good performance, with AUC of around 0.80 on the test set."
      ],
      "metadata": {
        "id": "VoLMl5pkwM4C"
      },
      "id": "VoLMl5pkwM4C"
    },
    {
      "cell_type": "code",
      "source": [
        "model = BinaryGraphSAGE(22, 4,4).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "losses, val_losses, acc, auc,fpr,tpr, cm = train_model(model, data, loss_fn, optimizer,\n",
        "                                                   patience=100, max_epochs=1000,\n",
        "                                                   save_path=f\"best_GraphSAGE.pt\")\n",
        "print(f\"Test Accuracy: {acc:.3f}\")\n",
        "print(f\"Test AUC:      {auc:.3f}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
        "# Plot Loss Curves\n",
        "axes[0].plot(losses, label=\"Train\")\n",
        "axes[0].plot(val_losses, label=\"Val\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].set_title(\"GraphSAGE Loss Curves\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot ROC Curve\n",
        "axes[1].plot(fpr, tpr, lw=2, label=f'AUC = {auc:.3f}')\n",
        "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[1].set_xlim([0.0, 1.0])\n",
        "axes[1].set_ylim([0.0, 1.05])\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('GraphSAGE Receiver Operating Characteristic (ROC) Curve')\n",
        "axes[1].legend(loc=\"lower right\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', ax=axes[2])\n",
        "axes[2].set_xlabel('Predicted')\n",
        "axes[2].set_ylabel('True')\n",
        "axes[2].set_title('GraphSAGE Confusion Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "afS6HPlbti9g"
      },
      "id": "afS6HPlbti9g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training GCN\n",
        "We train the GCN model and print the performance. The parameters are as follows:\n",
        "- `in_channels`=22: the number of node attributes\n",
        "- `hidden_channels`=16: The dimension of the GNN embeddings\n",
        "- `dense_channels`=4: The dimension of the dense embeddigns in the decoder\n",
        "\n",
        "The learning parameters here are very different from those of the GraphSAGE and GAT models, as this model struggles to learn the structure in the data. This is evident from the loss curves and the low AUC value."
      ],
      "metadata": {
        "id": "4qLw0ipus-Tb"
      },
      "id": "4qLw0ipus-Tb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3lKKQot1Q6u8",
      "metadata": {
        "id": "3lKKQot1Q6u8"
      },
      "outputs": [],
      "source": [
        "model = BinaryGCN(22, 16, 4).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "losses, val_losses, acc, auc,fpr,tpr, cm = train_model(model, data, loss_fn, optimizer,\n",
        "                                                   patience=100, max_epochs=100000,\n",
        "                                                   save_path=f\"best_GCN.pt\")\n",
        "\n",
        "print(f\"Test Accuracy: {acc:.3f}\")\n",
        "print(f\"Test AUC:      {auc:.3f}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
        "# Plot Loss Curves\n",
        "axes[0].plot(losses, label=\"Train\")\n",
        "axes[0].plot(val_losses, label=\"Val\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].set_title(\"GCN Loss Curves\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot ROC Curves\n",
        "axes[1].plot(fpr, tpr, lw=2, label=f'AUC = {auc:.3f}')\n",
        "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[1].set_xlim([0.0, 1.0])\n",
        "axes[1].set_ylim([0.0, 1.05])\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('GCN Receiver Operating Characteristic (ROC) Curve')\n",
        "axes[1].legend(loc=\"lower right\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', ax=axes[2])\n",
        "axes[2].set_xlabel('Predicted')\n",
        "axes[2].set_ylabel('True')\n",
        "axes[2].set_title('GCN Confusion Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training GAT\n",
        "We train the GAT model and print the performance. The parameters are as follows:\n",
        "- `in_channels`=22: the number of node attributes\n",
        "- `hidden_channels`=8: The dimension of the GNN embeddings\n",
        "- `dense_channels`=4: The dimension of the dense embeddigns in the decoder\n",
        "- `heads`=2: The number of attention heads\n",
        "\n",
        "The loss curves show that the model learns quite well, and we get a good performance, with AUC slightly below that of GraphSAGE."
      ],
      "metadata": {
        "id": "zkHt60vxygKz"
      },
      "id": "zkHt60vxygKz"
    },
    {
      "cell_type": "code",
      "source": [
        "model = BinaryGAT(22, 8,4, 2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "losses, val_losses, acc, auc,fpr,tpr, cm = train_model(model, data, loss_fn, optimizer,\n",
        "                                                   patience=100, max_epochs=1000,\n",
        "                                                   save_path=f\"best_GraphSAGE.pt\")\n",
        "\n",
        "print(f\"Test Accuracy: {acc:.3f}\")\n",
        "print(f\"Test AUC:      {auc:.3f}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
        "# Plot Loss Curves\n",
        "axes[0].plot(losses, label=\"Train\")\n",
        "axes[0].plot(val_losses, label=\"Val\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].set_title(\"GAT Loss Curves\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot ROC Curve\n",
        "axes[1].plot(fpr, tpr, lw=2, label=f'AUC = {auc:.3f}')\n",
        "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[1].set_xlim([0.0, 1.0])\n",
        "axes[1].set_ylim([0.0, 1.05])\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('GAT Receiver Operating Characteristic (ROC) Curve')\n",
        "axes[1].legend(loc=\"lower right\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', ax=axes[2])\n",
        "axes[2].set_xlabel('Predicted')\n",
        "axes[2].set_ylabel('True')\n",
        "axes[2].set_title('GAT Confusion Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iiEfT-TDuKpx"
      },
      "id": "iiEfT-TDuKpx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now trained the three models on our network data. The high performance indicates that there is indeed financial contagtion happening faciliated by borrowers in the same area or having the same provider (or a combination of both)."
      ],
      "metadata": {
        "id": "6OWIhAZV1TEp"
      },
      "id": "6OWIhAZV1TEp"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}