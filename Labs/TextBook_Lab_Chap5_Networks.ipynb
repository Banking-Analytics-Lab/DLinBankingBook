{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Banking-Analytics-Lab/DLinBankingBook/blob/main/Labs/TextBook_Lab_Chap5_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph models\n",
        "\n",
        "This notebook demonstrates the application of Graph Neural Networks (GNNs) for predicting mortgage defaults using loan-level data from Freddie Mac. The central methodology involves transforming structured, tabular loan data into a graph structure to capture and leverage the relationships between individual loans. Nodes in the graph represent mortgages, while edges are established based on shared attributes, specifically the geographical area (AREA) of the property and the loan provider. This relational structure allows the GNN to learn from both the intrinsic features of each loan and the contextual information of its neighbors.\n",
        "\n",
        "The process begins with data loading and preprocessing, followed by the explicit construction of a graph where node features are derived from the loan's characteristics (e.g., credit score, DTI ratio) and augmented with a graph-based feature, the node's degree. The notebook then implements and compares the performance of three prominent GNN architectures—Graph Convolutional Network (GCN), Graph Attention Network (GAT), and GraphSAGE—on the binary classification task of default prediction. The models are trained in a transductive setting, utilizing node-level splits for training, validation, and testing. The chapter concludes by evaluating the models using standard metrics like the Area Under the ROC Curve (AUC) and accuracy, presenting a comprehensive workflow for applying GNNs in a financial risk context."
      ],
      "metadata": {
        "id": "5tJ_odVX3iYk"
      },
      "id": "5tJ_odVX3iYk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xzTOryZV2oHu",
      "metadata": {
        "id": "xzTOryZV2oHu"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9baad39a",
      "metadata": {
        "id": "9baad39a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "%matplotlib inline\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "# Import Pytorch lybraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# For validation\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_auc_score,roc_curve, auc,accuracy_score\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.utils import from_networkx, assortativity, degree, homophily\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomNodeSplit\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id '1YadoR0hR_uZJe5XyGhh0PUwjqfkeJu7k'"
      ],
      "metadata": {
        "id": "Fs9BNL3KWSJo"
      },
      "id": "Fs9BNL3KWSJo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df=pd.read_csv(\"/content/graph_df.csv\")\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "WGCu63vXV5Jo"
      },
      "id": "WGCu63vXV5Jo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "area_counts = merged_df['AREA'].value_counts()\n",
        "merged_df = merged_df[merged_df['AREA'].isin(area_counts[area_counts > 1].index)].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "YLJJVmyfWheF"
      },
      "id": "YLJJVmyfWheF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df[merged_df['PROVIDER'] != 'Other sellers'].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "eVaadknlWjvK"
      },
      "id": "eVaadknlWjvK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#area\n",
        "el = []\n",
        "#for i in tqdm(range(len(merged_df))):\n",
        "for i in tqdm(range(1000)):#len(merged_df))):\n",
        " #   for j in range(len(merged_df)):\n",
        "    for j in range(1000):#len(merged_df)):\n",
        "        if merged_df.loc[i, 'AREA'] == merged_df.loc[j, 'AREA']:\n",
        "            el.append((merged_df.loc[i,'LOAN_NUMBER'], merged_df.loc[j,'LOAN_NUMBER']))\n",
        "\n",
        "el_df = pd.DataFrame(el)\n",
        "index = el_df[el_df.loc[:, 0] == el_df.loc[:, 1]].index\n",
        "el_df.drop(index, inplace = True)\n",
        "el_df.reset_index(drop = True, inplace = True)\n",
        "el_df.rename(columns = {0:'source', 1:'target'}, inplace = True)\n",
        "#el_df.to_csv('areaEdgeList.csv', index = False)\n",
        "area=el_df"
      ],
      "metadata": {
        "id": "eGrH-fMkWoJD"
      },
      "id": "eGrH-fMkWoJD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "el = []\n",
        "for i in tqdm(range(1000)):#len(merged_df))):\n",
        "#for i in tqdm(range(len(merged_df))):\n",
        "    for j in range(1000):#len(merged_df)):\n",
        "    #for j in range(len(merged_df)):\n",
        "        if merged_df.loc[i, 'PROVIDER'] == merged_df.loc[j, 'PROVIDER']:\n",
        "            el.append((merged_df.loc[i,'LOAN_NUMBER'], merged_df.loc[j,'LOAN_NUMBER']))\n",
        "\n",
        "el_df = pd.DataFrame(el)\n",
        "index = el_df[el_df.loc[:, 0] == el_df.loc[:, 1]].index\n",
        "el_df.drop(index, inplace = True)\n",
        "el_df.reset_index(drop = True, inplace = True)\n",
        "el_df.rename(columns = {0:'source', 1:'target'}, inplace = True)\n",
        "#el_df.to_csv('providerEdgeList.csv', index = False)\n",
        "comp=el_df"
      ],
      "metadata": {
        "id": "mMvlswaJWulq"
      },
      "id": "mMvlswaJWulq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_df = pd.concat([area, comp], axis = 0)\n",
        "edge_df.shape"
      ],
      "metadata": {
        "id": "t6wtg1vMW-Jl"
      },
      "id": "t6wtg1vMW-Jl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_areacomp = nx.from_pandas_edgelist(edge_df, source = \"source\", target = \"target\")\n",
        "G_areacomp.number_of_nodes(), G_areacomp.number_of_edges()"
      ],
      "metadata": {
        "id": "HvhZfeB6XAtv"
      },
      "id": "HvhZfeB6XAtv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(G_areacomp.nodes()):\n",
        "    id_loan = merged_df[merged_df['LOAN_NUMBER'] == i]['LOAN_NUMBER'].tolist()[0]\n",
        "    for f in merged_df.columns[0:23]:\n",
        "\n",
        "\n",
        "        G_areacomp.nodes[i][f] = merged_df[f][merged_df['LOAN_NUMBER'] == i].tolist()[0]"
      ],
      "metadata": {
        "id": "RcwACAYsXC5F"
      },
      "id": "RcwACAYsXC5F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {old_label: new_label for new_label, old_label in enumerate(G_areacomp.nodes())}\n",
        "H_relabel = nx.relabel_nodes(G_areacomp, mapping)"
      ],
      "metadata": {
        "id": "l9yMc1YIXE58"
      },
      "id": "l9yMc1YIXE58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_colors = [\n",
        "    'black' if H_relabel.nodes[n].get('target', 0) == 1 else 'grey'\n",
        "    for n in H_relabel.nodes()\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(H_relabel, seed=42)\n",
        "nx.draw(\n",
        "    H_relabel,\n",
        "    pos,\n",
        "    node_size=40,\n",
        "    node_color=node_colors,\n",
        "    edge_color='lightgray',\n",
        "    width=0.5,\n",
        "    with_labels=False\n",
        ")\n",
        "#plt.title(\"Network Plot of H_relabel (target=1 in red)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H94AtwMiXOOB"
      },
      "id": "H94AtwMiXOOB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id '17wG2Bo0ENVU74Rz5UI2y2jbZc4rfNSHU'"
      ],
      "metadata": {
        "id": "N12yZa4xaCfK"
      },
      "id": "N12yZa4xaCfK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id '1Nk_eMZLrkfCmdwHv5OCax2gh52mUZVxw'"
      ],
      "metadata": {
        "id": "E6UMYKhhaZmR"
      },
      "id": "E6UMYKhhaZmR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "area    = pd.read_csv('/content/areaEdgeList.csv')\n",
        "comp    = pd.read_csv('/content/providerEdgeList.csv')"
      ],
      "metadata": {
        "id": "lKsFX1EoW10A"
      },
      "id": "lKsFX1EoW10A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_df = pd.concat([area, comp], axis = 0)\n",
        "edge_df.shape"
      ],
      "metadata": {
        "id": "wiIRlqecYpQ9"
      },
      "id": "wiIRlqecYpQ9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_areacomp = nx.from_pandas_edgelist(edge_df, source = \"source\", target = \"target\")\n",
        "G_areacomp.number_of_nodes(), G_areacomp.number_of_edges()"
      ],
      "metadata": {
        "id": "d5y4skJsYsWa"
      },
      "id": "d5y4skJsYsWa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(G_areacomp.nodes()):\n",
        "    id_loan = merged_df[merged_df['LOAN_NUMBER'] == i]['LOAN_NUMBER'].tolist()[0]\n",
        "    for f in merged_df.columns[0:23]:\n",
        "\n",
        "\n",
        "        G_areacomp.nodes[i][f] = merged_df[f][merged_df['LOAN_NUMBER'] == i].tolist()[0]"
      ],
      "metadata": {
        "id": "ius6K7rhYwXC"
      },
      "id": "ius6K7rhYwXC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {old_label: new_label for new_label, old_label in enumerate(G_areacomp.nodes())}\n",
        "H_relabel = nx.relabel_nodes(G_areacomp, mapping)"
      ],
      "metadata": {
        "id": "re3sQM6qYzkz"
      },
      "id": "re3sQM6qYzkz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('network.gpickle', 'wb') as f:\n",
        "    pickle.dump(H_relabel, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "Qoh8YeclY2bf"
      },
      "id": "Qoh8YeclY2bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X-XdhvK_2hsB",
      "metadata": {
        "id": "X-XdhvK_2hsB"
      },
      "outputs": [],
      "source": [
        "!gdown 'https://drive.google.com/uc?id=1-AOl7sCEusPqmctnStdPg8Frc9XEOcKf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3798347f",
      "metadata": {
        "id": "3798347f"
      },
      "outputs": [],
      "source": [
        "with open('network.gpickle', 'rb') as f:\n",
        "    G = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11cb5120",
      "metadata": {
        "id": "11cb5120"
      },
      "outputs": [],
      "source": [
        "edge_list = nx.to_pandas_edgelist(G)\n",
        "edge_i = torch.tensor(edge_list.values, dtype=torch.long).t().contiguous()\n",
        "myData = Data(edge_index=edge_i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63664dc5",
      "metadata": {
        "id": "63664dc5"
      },
      "outputs": [],
      "source": [
        "target_attribute = list(nx.get_node_attributes(G, 'target').values())\n",
        "myData.y = torch.tensor(target_attribute, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9a3078a",
      "metadata": {
        "id": "a9a3078a"
      },
      "outputs": [],
      "source": [
        "node_data = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')\n",
        "tmp=node_data.iloc[:,0:21]\n",
        "myData.x=torch.tensor(tmp.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569ecc0c",
      "metadata": {
        "id": "569ecc0c"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import degree\n",
        "\n",
        "edge_index = myData.edge_index\n",
        "num_nodes = myData.num_nodes\n",
        "\n",
        "# Step 1: Compute degree of each node (undirected graph assumed)\n",
        "deg = degree(edge_index[0], num_nodes=num_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "703b74c7",
      "metadata": {
        "id": "703b74c7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 2: Optionally normalize degree (e.g., log scale or min-max)\n",
        "deg = deg.view(-1, 1)  # Reshape to be a column vector\n",
        "deg = (deg - deg.min()) / (deg.max() - deg.min())\n",
        "\n",
        "# Step 3: Concatenate with existing node features\n",
        "myData.x = torch.cat([myData.x, deg], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7111537e",
      "metadata": {
        "id": "7111537e"
      },
      "outputs": [],
      "source": [
        "myData.x.shape  # Check the shape of the updated node features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e52c1f",
      "metadata": {
        "id": "10e52c1f"
      },
      "outputs": [],
      "source": [
        "node_transform = RandomNodeSplit(split='train_rest',num_val=10000, num_test=10000,num_train_per_class=900)\n",
        "node_splits = node_transform(myData)\n",
        "node_splits.x = node_splits.x.float()\n",
        "node_splits.y = node_splits.y.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c08e8417",
      "metadata": {
        "id": "c08e8417"
      },
      "outputs": [],
      "source": [
        "print()\n",
        "print(f'Dataset: {node_splits}:')\n",
        "print('======================')\n",
        "print(f'Number of features: {node_splits.num_features}')\n",
        "#print(f'Number of classes: {myData.num_classes}')\n",
        "print(f'Number of defaults: {(node_splits.y.sum())}')\n",
        "#===========================================================================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {node_splits.num_nodes}')\n",
        "print(f'Number of edges: {node_splits.num_edges}')\n",
        "print(f'Average node degree: {node_splits.num_edges / node_splits.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {node_splits.train_mask.sum()}')\n",
        "print(f'Number of training nodes: {node_splits.val_mask.sum()}')\n",
        "print(f'Number of training nodes: {node_splits.test_mask.sum()}')\n",
        "print(f'Training node label rate: {node_splits.y[node_splits.train_mask].sum() / node_splits.train_mask.sum():.2f}')\n",
        "print(f'Has isolated nodes: {node_splits.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {node_splits.has_self_loops()}')\n",
        "print(f'Is undirected: {node_splits.is_undirected()}')\n",
        "#print(f'Node homopily:{homophily(myData.edge_index, myData.y, method='node')}')\n",
        "#print(f'Edge homopily: {homophily(myData.edge_index, myData.y, method='edge')}')\n",
        "#print(f'Assortativity:{assortativity(myData.edge_index)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dfc50db",
      "metadata": {
        "id": "1dfc50db"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "node_splits.x = torch.tensor(scaler.fit_transform(node_splits.x.numpy()), dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95c20983",
      "metadata": {
        "id": "95c20983"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# Model Definitions\n",
        "# ==========================\n",
        "\n",
        "class BinaryGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels,dense_channels):\n",
        "        super(BinaryGraphSAGE, self).__init__()\n",
        "        self.sage1 = SAGEConv(in_channels, hidden_channels)\n",
        "        #self.bn1 = BatchNorm(hidden_channels)\n",
        "        self.sage2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "\n",
        "\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, dense_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=0.5),\n",
        "            torch.nn.Linear(dense_channels, 1)\n",
        "        )\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.sage1(x, edge_index)\n",
        "        #x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.sage2(x, edge_index)\n",
        "        x=F.relu(x)\n",
        "        out = self.decoder(x)\n",
        "        return out.view(-1)\n",
        "\n",
        "class BinaryGCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels,hidden_dense):\n",
        "        super(BinaryGCN, self).__init__()\n",
        "        #self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        #self.bn1 = BatchNorm(hidden_channels)\n",
        "        #self.conv2 = GCNConv(hidden_channels, hidden_channels2)\n",
        "        #self.Linear\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        # Second graph convolutional layer\n",
        "       # self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        # A simple decoder with a ReLU activation\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, hidden_dense),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=0.5),\n",
        "            torch.nn.Linear(hidden_dense, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        " # Pass data through the first GCN layer and apply ReLU activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Pass data through the second GCN layer and apply ReLU activation\n",
        "    #    x = self.conv2(x, edge_index)\n",
        "     #   x = F.relu(x)\n",
        "\n",
        "        # Pass the node embeddings through the decoder\n",
        "        out = self.decoder(x)\n",
        "        return out.view(-1)\n",
        "\n",
        "\n",
        "class BinaryGAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, dense_channels,heads):\n",
        "        super(BinaryGAT, self).__init__()\n",
        "        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads,concat=False, dropout=0.5)\n",
        "       # self.bn1 = BatchNorm(hidden_channels * heads)\n",
        "      #  self.gat2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False)\n",
        "\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, dense_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=0.5),\n",
        "            torch.nn.Linear(dense_channels, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.gat1(x, edge_index)\n",
        "      #  x = self.bn1(x)\n",
        "        x = F.elu(x)\n",
        "      #  x = F.dropout(x, p=0.5, training=self.training)\n",
        "      #  x = self.gat2(x, edge_index)\n",
        "       # x = F.elu(x)\n",
        "        out = self.decoder(x)\n",
        "        return out.view(-1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "078ee501",
      "metadata": {
        "id": "078ee501"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# Training and Evaluation Functions\n",
        "# ==========================\n",
        "\n",
        "def train_one_epoch(model, data, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask].float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data, mask, loss_fn):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    probs = torch.sigmoid(out)\n",
        "    preds = (probs > 0.5).float()\n",
        "    y_true = data.y[mask].cpu() # Move to CPU\n",
        "    y_probs = probs[mask].cpu() # Move to CPU\n",
        "    y_preds = preds[mask].cpu() # Move to CPU\n",
        "    #print(y_probs.min())\n",
        "    #print(y_probs.max())\n",
        "    acc = accuracy_score(y_true, y_preds)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_probs)\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
        "\n",
        "    except ValueError:\n",
        "        auc = float('nan')\n",
        "\n",
        "    conf_mat = confusion_matrix(y_true, y_preds)\n",
        "\n",
        "    val_loss = loss_fn(out[mask], data.y[mask].float()).item()\n",
        "    return acc, auc, fpr,tpr,y_probs, conf_mat, val_loss\n",
        "\n",
        "\n",
        "def train_model(model, data, loss_fn, optimizer, patience=20,\n",
        "                max_epochs=1000, save_path=\"best_model.pt\"):\n",
        "    best_val_loss = 1e10\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        loss = train_one_epoch(model, data, optimizer, loss_fn)\n",
        "        losses.append(loss)\n",
        "        _, val_auc,val_fpr,val_tpr,val_y_probs, _, val_loss = evaluate(model, data, data.val_mask, loss_fn)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), save_path)  # Save best model to disk\n",
        "\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(save_path))  # Load best model from disk\n",
        "    test_acc, test_auc, test_fpr,test_tpr,test_y_probs,conf_mat, _ = evaluate(model, data, data.test_mask, loss_fn)\n",
        "    return losses, val_losses, test_acc, test_auc, test_fpr,test_tpr,test_y_probs,conf_mat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ],
      "metadata": {
        "id": "1P7vAjqH9ZGw"
      },
      "id": "1P7vAjqH9ZGw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1e64fa",
      "metadata": {
        "id": "9a1e64fa"
      },
      "outputs": [],
      "source": [
        "#0.76\n",
        "LEARNING_RATE = 0.01\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS = 1000\n",
        "PATIENCE = 100\n",
        "HIDDEN_GNN = 4\n",
        "HIDDEN_DENSE =4\n",
        "HEADS=2\n",
        "\n",
        "# GraphSage: LR 0.01,1000 e, 100 pat, 8, 8.\n",
        "# GCN: LR 0.0005, 1000 epocs, 100 patience, 1 gcnconv 16, dense 4: AUCH 0.53\n",
        "# GAT: LR:0.01, 1000 epoch, 100 patence, 1 gatconv 8, do=0.5, 2 heads, dense 4: 0.76\n",
        "\n",
        "# Empty VRAM\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = node_splits.to(device)\n",
        "\n",
        "#\n",
        "y_train = data.y[data.train_mask]\n",
        "num_pos = (y_train == 1).sum().item()\n",
        "num_neg = (y_train == 0).sum().item()\n",
        "pos_weight = torch.tensor([num_neg / num_pos], device=device)\n",
        "#\n",
        "models = {\n",
        "      \"GraphSAGE\": BinaryGraphSAGE(22, HIDDEN_GNN,HIDDEN_DENSE).to(device),\n",
        "     #\"GCN\": BinaryGCN(22, HIDDEN_GNN, HIDDEN_DENSE).to(device),\n",
        "    # \"GAT\": BinaryGAT(22, HIDDEN_GNN,HIDDEN_DENSE, HEADS).to(device),\n",
        "\n",
        " }\n",
        "#\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,\n",
        "                                 weight_decay=WEIGHT_DECAY)\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    losses, val_losses, acc, auc,fpr,tpr,y_probs, cm = train_model(model, data, loss_fn, optimizer,\n",
        "                                                   patience=PATIENCE, max_epochs=EPOCHS,\n",
        "                                                   save_path=f\"best_{name}.pt\")\n",
        "\n",
        "    results[name] = {\"losses\": losses, \"val_losses\": val_losses, \"acc\": acc, \"auc\": auc, \"fpr\": fpr,\"tpr\":tpr,\"y_probs\":y_probs,\"conf_mat\": cm}\n",
        "    plt.plot(losses, label=f\"{name} Train\")\n",
        "    plt.plot(val_losses, label=f\"{name} Val\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "    print(f\"Test AUC:      {auc:.4f}\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "   # plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        " #   plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#0.76\n",
        "LEARNING_RATE = 0.0005\n",
        "WEIGHT_DECAY = 1e-4\n",
        "EPOCHS = 100000\n",
        "PATIENCE = 100\n",
        "HIDDEN_GNN = 16\n",
        "HIDDEN_DENSE =4\n",
        "HEADS=2\n",
        "\n",
        "\n",
        "# GCN: LR 0.0005, 1000 epocs, 100 patience, 1 gcnconv 16, dense 4: AUCH 0.53\n",
        "# GAT: LR:0.01, 1000 epoch, 100 patence, 1 gatconv 8, do=0.5, 2 heads, dense 4: 0.76\n",
        "\n",
        "# Empty VRAM\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = node_splits.to(device)\n",
        "\n",
        "#\n",
        "y_train = data.y[data.train_mask]\n",
        "num_pos = (y_train == 1).sum().item()\n",
        "num_neg = (y_train == 0).sum().item()\n",
        "pos_weight = torch.tensor([num_neg / num_pos], device=device)\n",
        "#\n",
        "models = {\n",
        "     \"GCN\": BinaryGCN(22, HIDDEN_GNN, HIDDEN_DENSE).to(device),\n",
        "    # \"GAT\": BinaryGAT(22, HIDDEN_GNN,HIDDEN_DENSE, HEADS).to(device),\n",
        "   #  \"GraphSAGE\": BinaryGraphSAGE(22, HIDDEN_GNN,HIDDEN_DENSE).to(device),\n",
        " }\n",
        "#\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,\n",
        "                                 weight_decay=WEIGHT_DECAY)\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    losses, val_losses, acc, auc,fpr,tpr,y_probs, cm = train_model(model, data, loss_fn, optimizer,\n",
        "                                                   patience=PATIENCE, max_epochs=EPOCHS,\n",
        "                                                   save_path=f\"best_{name}.pt\")\n",
        "\n",
        "    results[name] = {\"losses\": losses, \"val_losses\": val_losses, \"acc\": acc, \"auc\": auc, \"fpr\": fpr,\"tpr\":tpr,\"y_probs\":y_probs,\"conf_mat\": cm}\n",
        "    plt.plot(losses, label=f\"{name} Train\")\n",
        "    plt.plot(val_losses, label=f\"{name} Val\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "    print(f\"Test AUC:      {auc:.4f}\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "  #  plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        " #   plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3lKKQot1Q6u8"
      },
      "id": "3lKKQot1Q6u8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0.76\n",
        "LEARNING_RATE = 0.01\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS = 1000\n",
        "PATIENCE = 100\n",
        "HIDDEN_GNN = 8\n",
        "HIDDEN_DENSE =4\n",
        "HEADS=2\n",
        "\n",
        "\n",
        "# GCN: LR 0.0005, 1000 epocs, 100 patience, 1 gcnconv 16, dense 4: AUCH 0.53\n",
        "# GAT: LR:0.01, 1000 epoch, 100 patence, 1 gatconv 8, do=0.5, 2 heads, dense 4: 0.76\n",
        "\n",
        "# Empty VRAM\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = node_splits.to(device)\n",
        "\n",
        "#\n",
        "y_train = data.y[data.train_mask]\n",
        "num_pos = (y_train == 1).sum().item()\n",
        "num_neg = (y_train == 0).sum().item()\n",
        "pos_weight = torch.tensor([num_neg / num_pos], device=device)\n",
        "#\n",
        "models = {\n",
        "     #\"GCN\": BinaryGCN(22, HIDDEN_GNN, HIDDEN_DENSE).to(device),\n",
        "     \"GAT\": BinaryGAT(22, HIDDEN_GNN,HIDDEN_DENSE, HEADS).to(device),\n",
        "   #  \"GraphSAGE\": BinaryGraphSAGE(22, HIDDEN_GNN,HIDDEN_DENSE).to(device),\n",
        " }\n",
        "#\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,\n",
        "                                 weight_decay=WEIGHT_DECAY)\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    losses, val_losses, acc, auc,fpr,tpr,y_probs, cm = train_model(model, data, loss_fn, optimizer,\n",
        "                                                   patience=PATIENCE, max_epochs=EPOCHS,\n",
        "                                                   save_path=f\"best_{name}.pt\")\n",
        "\n",
        "    results[name] = {\"losses\": losses, \"val_losses\": val_losses, \"acc\": acc, \"auc\": auc, \"fpr\": fpr,\"tpr\":tpr,\"y_probs\":y_probs,\"conf_mat\": cm}\n",
        "    plt.plot(losses, label=f\"{name} Train\")\n",
        "    plt.plot(val_losses, label=f\"{name} Val\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "    print(f\"Test AUC:      {auc:.4f}\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "   # plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        " #   plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4b_9hjSgicmO"
      },
      "id": "4b_9hjSgicmO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}