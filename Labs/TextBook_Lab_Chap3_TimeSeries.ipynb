{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Banking-Analytics-Lab/DLinBankingBook/blob/main/Labs/TextBook_Lab_Chap3_TimeSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Financial time series prediction with LSTM and GRU\n",
        "In this lab, we will start working with the time series of loan delinquency status, in order to predict them.\n",
        "\n",
        "As always, let's first import the packages we will use and the data."
      ],
      "metadata": {
        "id": "3Q86gaLqlY-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N46PTRdy374"
      },
      "outputs": [],
      "source": [
        "# Install necessasary packages, if not done before\n",
        "!pip install torchview\n",
        "!pip install livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy 'https://drive.google.com/file/d/18ERhjQQg-PB6a5wEyUTNecJv8zkz0GkK/view?usp=sharing'\n",
        "!gdown --fuzzy 'https://drive.google.com/file/d/1B2R1nCGIJ4TURvh7RL-aoLHAe8oP-uxO/view?usp=sharing'"
      ],
      "metadata": {
        "id": "DD0l5BTQdd9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5REx_I63y8nJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# For validation\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "from torchview import draw_graph\n",
        "import graphviz\n",
        "from livelossplot import PlotLosses\n",
        "graphviz.set_jupyter_format('png')\n",
        "%matplotlib inline\n",
        "\n",
        "# Import Pytorch lybraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import _LRScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we load the dataset, which originates from Freddie Mac. It contains monthly loan status updates spanning from November 2021 to June 2024."
      ],
      "metadata": {
        "id": "YDF0Eih3l3pU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcXco8zizCBJ"
      },
      "outputs": [],
      "source": [
        "# Read the data\n",
        "df = pd.read_csv('Time_Series_Lab_sampled.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assign a value of 1 to loans that were delinquent at least once in the last three months (April to June 2024). Our goal is to predict whether a loan will become delinquent in the next quarter."
      ],
      "metadata": {
        "id": "JUQHpR-Um0MK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YAKxYnN1Fss"
      },
      "outputs": [],
      "source": [
        "# Convert REPORTING_PERIOD to datetime for sorting\n",
        "df[\"REPORTING_PERIOD\"] = pd.to_datetime(df[\"REPORTING_PERIOD\"], format='%Y%m')\n",
        "\n",
        "df[\"CURRENT_LOAN_DELINQUENCY_STATUS\"] = pd.to_numeric(df[\"CURRENT_LOAN_DELINQUENCY_STATUS\"], errors='coerce').fillna(1)\n",
        "\n",
        "# Sort data by Loan Number and Reporting Period (chronological order)\n",
        "df = df.sort_values(by=[\"LOAN_NUMBER\", \"REPORTING_PERIOD\"])\n",
        "\n",
        "\n",
        "# Identify loans that were delinquent at least once in the period 2024-04 to 2024-06\n",
        "df[\"CURRENT_LOAN_DELINQUENCY_STATUS\"] = (df[\"CURRENT_LOAN_DELINQUENCY_STATUS\"] > 0).astype(int)\n",
        "mask = (df[\"REPORTING_PERIOD\"] >= \"2024-04-01\") & (df[\"REPORTING_PERIOD\"] <= \"2024-06-30\")\n",
        "df[\"target\"] = df.groupby(\"LOAN_NUMBER\")[\"CURRENT_LOAN_DELINQUENCY_STATUS\"].transform(lambda x: x[mask].max()).astype(int)\n",
        "\n",
        "\n",
        "# Remove the last three months (April 2024 - June 2024)\n",
        "df = df[~df[\"REPORTING_PERIOD\"].between(pd.Timestamp(\"2024-03-31\"), pd.Timestamp(\"2024-06-02\"))]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long-Short Term Memory (LSTM) Networks\n",
        "First, we will train an LSTM. The LSTM is a fairly complex model, so VRAM will be a signficant constraint. Let's first set up the train and test dataset. We will normalize the data and One Hot Encode the labels using sklearn's OneHotEncode function. We will also set the class weights as we have done before."
      ],
      "metadata": {
        "id": "uB0WE6DFnNqb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3MT5ewQ1PGj"
      },
      "outputs": [],
      "source": [
        "num_cols = [\n",
        "    \"CURRENT_ACTUAL_UPB\",\n",
        "    \"LOAN_AGE\",\n",
        "    \"REMAINING_MONTHS\",\n",
        "    \"CURRENT_INTEREST_RATE\",\n",
        "    \"CURRENT_NON_INTEREST_BEARING_UPB\",\n",
        "    \"ESTIMATED_LOAN_TO_VALUE\",\n",
        "    \"INTEREST_BEARING_UPB\",\n",
        "    \"CURRENT_LOAN_DELINQUENCY_STATUS\"\n",
        "]\n",
        "\n",
        "# Group by loan number for sequence creation\n",
        "grouped = df.groupby(\"LOAN_NUMBER\")\n",
        "\n",
        "# Store sequences and targets\n",
        "X_dict = {}\n",
        "y_dict = {}\n",
        "\n",
        "for loan_id, group in grouped:\n",
        "    group = group.sort_values(\"REPORTING_PERIOD\")  # Sort each loan's time series\n",
        "\n",
        "    # Extract features as separate sequences\n",
        "    X_dict[loan_id] = {col: group[col].values for col in num_cols}\n",
        "\n",
        "    # Assign existing target value (no need to recalculate)\n",
        "    y_dict[loan_id] = group[\"target\"].iloc[0]\n",
        "\n",
        "\n",
        "\n",
        "# Perform stratified split\n",
        "# Convert dictionary to DataFrame\n",
        "np.random.seed(42)\n",
        "x_df = pd.DataFrame.from_dict(X_dict, orient=\"index\")\n",
        "\n",
        "# Map target values from y_dict\n",
        "x_df[\"target\"] = x_df.index.map(y_dict)\n",
        "\n",
        "# Perform stratified split\n",
        "train_idx, test_idx = train_test_split(\n",
        "    x_df.index, test_size=0.2, stratify=x_df[\"target\"], random_state=42\n",
        ")\n",
        "\n",
        "# Assign 'if_test' column based on stratified split\n",
        "x_df[\"if_test\"] = 0  # Default to training\n",
        "x_df.loc[test_idx, \"if_test\"] = 1  # Mark test samples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_df.head()"
      ],
      "metadata": {
        "id": "1uwYtt7HybPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMftfWRtzEFi"
      },
      "outputs": [],
      "source": [
        "# Function to scale each feature separately\n",
        "def scale_dataframe_sequences(x_df, test_var=None):\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Split data into train and test\n",
        "    if test_var is not None:\n",
        "        x_train = x_df.loc[test_var == 0, :]\n",
        "        x_test = x_df.loc[test_var == 1, :]\n",
        "    else:\n",
        "        x_train = x_df\n",
        "\n",
        "    scaled_data = {}\n",
        "    scaled_data_test = {}\n",
        "\n",
        "    # Get sequence length from first row\n",
        "    seq_len = len(next(iter(x_train.iloc[0])))\n",
        "\n",
        "    for column in num_cols:\n",
        "        # Stack time series data for each loan into a 2D array\n",
        "        data = np.stack(x_train[column].values).reshape(-1, seq_len)\n",
        "        if column != \"CURRENT_LOAN_DELINQUENCY_STATUS\":\n",
        "          scaled_data[column] = scaler.fit_transform(data).reshape(-1, 1, seq_len).tolist()\n",
        "        else:\n",
        "          scaled_data[column] = data.reshape(-1, 1, seq_len).tolist()\n",
        "\n",
        "        # Scale test set if provided\n",
        "        if test_var is not None:\n",
        "            data_test = np.stack(x_test[column].values).reshape(-1, seq_len)\n",
        "            if column != \"CURRENT_LOAN_DELINQUENCY_STATUS\":\n",
        "              scaled_data_test[column] = scaler.transform(data_test).reshape(-1, 1, seq_len).tolist()\n",
        "            else:\n",
        "              scaled_data_test[column] = data_test.reshape(-1, 1, seq_len).tolist()\n",
        "\n",
        "    # Create new DataFrames with scaled data\n",
        "    scaled_df = pd.DataFrame(scaled_data)\n",
        "    scaled_df = scaled_df.map(lambda x: np.array(x[0]))\n",
        "\n",
        "    if test_var is not None:\n",
        "        scaled_df_test = pd.DataFrame(scaled_data_test)\n",
        "        scaled_df_test = scaled_df_test.map(lambda x: np.array(x[0]))\n",
        "        return scaled_df, scaled_df_test\n",
        "    else:\n",
        "        return scaled_df, _\n",
        "\n",
        "# Normalize train and test sets\n",
        "x_train, x_test = scale_dataframe_sequences(x_df[num_cols], x_df[\"if_test\"])\n",
        "\n",
        "# One-hot encode the target variable\n",
        "enc = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "y_train = enc.fit_transform(x_df.loc[x_df[\"if_test\"] == 0, \"target\"].values.reshape(-1, 1))[:, 1]\n",
        "y_test = enc.transform(x_df.loc[x_df[\"if_test\"] == 1, \"target\"].values.reshape(-1, 1))[:, 1]\n",
        "\n",
        "# Compute class weights\n",
        "pos_weight = np.sum(1 - y_train) / np.sum(y_train)\n",
        "pos_weight = torch.tensor(pos_weight, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "metadata": {
        "id": "tujoYKXmf_cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Atv27gbvzFfE"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create the LSTM. First, let's try to be naïve and see what happens if we simply try to train our LSTM using 128 cells and just passing the sequence. As we saw in the textbook, Google's LeNet showed that stacking parallel layers of convolution (so, not sequential models) seemed like a good idea. Would this idea work with LSTM?\n",
        "\n",
        "Let's find out. For this, we will create a pytorch model and it's corresponding forward pass. We will use an LSTM to create an output embedding of the time series, and then pass that to a dense layer with droput that will classify the sequence. Finally, we will determine if the sequence is a suggesting a failure or not with a sigmoid output layer."
      ],
      "metadata": {
        "id": "2aIwc295nYBj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbO6ZTui1nET"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, classifier_dim, output_dim):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        # Building your LSTM\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "\n",
        "        # Classifier network\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, classifier_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(classifier_dim, output_dim),\n",
        "            #nn.Softmax(dim=1) # No need for softmax with logit loss.\n",
        "       )\n",
        "\n",
        "    # Forward method\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        # Initialize cell state\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "        out = self.classifier(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's initatiate the model and see the diagram."
      ],
      "metadata": {
        "id": "TTp_dadOnrsI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-W4Nd8F66L6"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "\n",
        "n_vars = x_train.shape[1]\n",
        "model = LSTMClassifier(n_vars, 128, 1, 64, 1).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw the model\n",
        "model_graph = draw_graph(model, input_size=(1, 64, n_vars),\n",
        "                         device=device,\n",
        "                         expand_nested=True)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "ZKy2OZAj-FfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train. Recurrent models are very tricky in terms of their training, and the gradients usually present a very erratic behaviour. It is a good idea to clip the gradients, that is, to lower the value of the gradient so it does not explode. There is a Torch utility called clip_grad_norm, which allows to set a maximum value against the mean for the gradient.\n",
        "\n",
        "We would generally advice to use a value of around 1 to 2 if you are seeing erratic training behaviour.\n",
        "\n",
        "We will use the series-oriented RMSprop as our optimizer, and implemente norm clipping in the training loop."
      ],
      "metadata": {
        "id": "MMVz9DgBnv8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up optimizer and loss\n",
        "learning_rate = 0.0001\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
        "\n",
        "# Set global run parameters\n",
        "best_vloss = 10000000"
      ],
      "metadata": {
        "id": "BCqJQG1a7bDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to create the data loaders. These are pretty simple: Just read from the pandas dataset and split them into train and test. We can do that easily with the following code."
      ],
      "metadata": {
        "id": "SPyqVCLGn83v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_train_val_dataloaders(x_train_df, y_train, batch_size=64, val_size=0.33, seed=42):\n",
        "    \"\"\"\n",
        "    Create DataLoader instances for training and validation datasets suited for LSTM models,\n",
        "    ensuring stratified sampling for validation split.\n",
        "\n",
        "    Parameters:\n",
        "    x_train_df (pandas.DataFrame): DataFrame where each cell is a sequence (1D array or list) of length seq_len.\n",
        "    y_train (numpy.ndarray or list): The labels for training.\n",
        "    batch_size (int): The batch size for both train and validation loaders.\n",
        "    val_size (float): The fraction of the data to be used for validation.\n",
        "\n",
        "    Returns:\n",
        "    train_loader (DataLoader): DataLoader for the training set.\n",
        "    val_loader (DataLoader): DataLoader for the validation set.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Convert the DataFrame of sequences into a correctly shaped 3D numpy array\n",
        "    sequences = np.stack(x_train_df.apply(lambda s: np.stack(s.values).reshape(s.values[0].shape[0], -1), axis=1).values)\n",
        "\n",
        "    # Ensure y_train is a numpy array\n",
        "    y_train = np.array(y_train) if not isinstance(y_train, np.ndarray) else y_train\n",
        "\n",
        "    # Convert to 2D array if necessary\n",
        "    if y_train.ndim == 1:\n",
        "        y_train = y_train[:, None]\n",
        "\n",
        "    # Perform stratified split\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        np.arange(len(y_train)),  # Index-based split\n",
        "        test_size=val_size,\n",
        "        stratify=y_train,  # Ensures class proportions are maintained\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    # Subset the data based on stratified indices\n",
        "    x_train_tensor = torch.from_numpy(sequences[train_idx]).float()\n",
        "    y_train_tensor = torch.from_numpy(y_train[train_idx]).float()\n",
        "    x_val_tensor = torch.from_numpy(sequences[val_idx]).float()\n",
        "    y_val_tensor = torch.from_numpy(y_train[val_idx]).float()\n",
        "\n",
        "    # Create TensorDatasets\n",
        "    train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "    val_data = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n"
      ],
      "metadata": {
        "id": "0tyxplfs7uyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data loaders\n",
        "batch_size = 2028\n",
        "train_loader, val_loader = create_train_val_dataloaders(x_train, y_train.reshape(-1,1),\n",
        "                                                       batch_size=batch_size)\n",
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"validation\": val_loader\n",
        "}\n",
        "\n",
        "# Softmax function for the output\n",
        "softmax_func = np.vectorize(lambda x: 1/(1+np.exp(-1 * x)))"
      ],
      "metadata": {
        "id": "yRDHZn07-QUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are finally ready to train."
      ],
      "metadata": {
        "id": "JYRaOw4IoIy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 20\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Set run parameters\n",
        "n_epochs = 100\n",
        "liveloss = PlotLosses()\n",
        "\n",
        "# Train!\n",
        "for epoch in range(n_epochs):\n",
        "   # Run the epoch\n",
        "  logs = {}\n",
        "\n",
        "  # Run a train epoch, and then a validation epoch.\n",
        "  for phase in ['train', 'validation']:\n",
        "      if phase == 'train':\n",
        "          model.train()\n",
        "      else:\n",
        "          model.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      for i, data in enumerate(dataloaders[phase]):\n",
        "          inputs, labels = data\n",
        "          # print(labels)\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model(inputs).to(device)\n",
        "          loss = loss_fn(outputs, labels)\n",
        "\n",
        "          if phase == 'train':\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "\n",
        "              # Clip the gradient norm\n",
        "              nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "              # Backpropagate\n",
        "              optimizer.step()\n",
        "\n",
        "          preds = softmax_func(outputs.detach().cpu().numpy())\n",
        "          preds = np.round(preds)\n",
        "          running_loss += loss.detach() * inputs.size(0)\n",
        "          running_corrects += np.sum(preds.flatten() == labels.data.flatten().cpu().numpy())\n",
        "\n",
        "          if i % 10 == 9:\n",
        "            batch_loss = running_loss / (10 * (i+1))\n",
        "            print(f'{phase} batch {i+1} loss: {batch_loss:.3f}')\n",
        "            tb_x = epoch * len(dataloaders[phase]) + i + 1\n",
        "\n",
        "          # Delete the used VRAM\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "      epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "      epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
        "\n",
        "      prefix = ''\n",
        "      if phase == 'validation':\n",
        "          prefix = 'val_'\n",
        "\n",
        "          # Track best performance, and save the model's state\n",
        "          if epoch_loss < best_vloss:\n",
        "              best_vloss = epoch_loss\n",
        "              model_path = 'best_model.pth'\n",
        "              print(f'New best model found. Saving it as {model_path}')\n",
        "              torch.save(model.state_dict(), model_path)\n",
        "\n",
        "      logs[prefix + 'log loss'] = epoch_loss.item()\n",
        "      logs[prefix + 'accuracy'] = epoch_acc.item()\n",
        "\n",
        "  liveloss.update(logs)\n",
        "  liveloss.send()"
      ],
      "metadata": {
        "id": "cMR-ucv8mdgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Plot the loss and save it.\n",
        "  liveloss.send()\n",
        "  plt.savefig('LSTM.pdf')"
      ],
      "metadata": {
        "id": "LqJY_W0cxZCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training is progressing! Let's apply it to the test set and see what we get. You may also want to keep training, as the model is still learning after 100 epochs."
      ],
      "metadata": {
        "id": "2hpwQkqwoaT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_dataloader(x_test_df, y_test=None, batch_size=64, seed=42):\n",
        "    \"\"\"\n",
        "    Create DataLoader instance for the test dataset.\n",
        "\n",
        "    Parameters:\n",
        "    x_test_df (pandas.DataFrame): DataFrame where each cell is a sequence (1D array or list) of length seq_len.\n",
        "    y_test (numpy.ndarray, optional): The labels for testing. Pass None if there are no labels.\n",
        "    batch_size (int): The batch size for the test loader.\n",
        "\n",
        "    Returns:\n",
        "    test_loader (DataLoader): DataLoader for the test set.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Convert the DataFrame of sequences into a correctly shaped 3D numpy array\n",
        "    # sequences.shape should be (number of samples, seq_len, number of features per timestep)\n",
        "    sequences = np.stack(x_test_df.apply(lambda s: np.stack(s.values).reshape(s.values[0].shape[0], -1), axis=1).values)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    x_test_tensor = torch.from_numpy(sequences).float()\n",
        "\n",
        "    # Create a TensorDataset from the input data\n",
        "    if y_test is not None:\n",
        "        # Check if y_test is a numpy array, if not convert it\n",
        "        if not isinstance(y_test, np.ndarray):\n",
        "            y_test = np.array(y_test)\n",
        "\n",
        "        # Convert labels to a PyTorch tensor\n",
        "        y_test_tensor = torch.from_numpy(y_test).float()\n",
        "\n",
        "        # Assert that the number of samples matches\n",
        "        assert len(x_test_tensor) == len(y_test_tensor), \"The number of input samples and labels must be the same.\"\n",
        "        test_data = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "    else:\n",
        "        test_data = TensorDataset(x_test_tensor)\n",
        "\n",
        "    # Create the DataLoader for the test set\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return test_loader"
      ],
      "metadata": {
        "id": "9ly1-D96mgEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = create_test_dataloader(x_test, y_test.reshape(-1,1))"
      ],
      "metadata": {
        "id": "hLel-cFdmjES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('best_model.pth'))"
      ],
      "metadata": {
        "id": "pBNgLijFmkMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper to save memory by not recomputing gradients.\n",
        "with torch.no_grad():\n",
        "    # Set the model in evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    # Calculate running loss and accuracy\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    test_labels = np.array([])\n",
        "    test_probs = np.array([])\n",
        "    test_predictions = np.array([])\n",
        "\n",
        "    # Apply to the test set\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs, labels = data\n",
        "        test_labels = np.append(test_labels, labels.cpu().numpy())\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        test_probs = np.append(test_probs, outputs.cpu().numpy())\n",
        "        outputs = outputs.to(device)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        preds = softmax_func(outputs.cpu().numpy())\n",
        "        preds = np.round(preds)\n",
        "        test_predictions = np.append(test_predictions, preds)\n",
        "        running_loss += loss.detach() * inputs.size(0)\n",
        "        running_corrects += np.sum(preds.flatten() == labels.data.flatten().cpu().numpy())\n",
        "\n",
        "test_loss = running_loss / len(test_loader.dataset)\n",
        "test_acc = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "print(f'The test set accuracy is {test_acc*100:.2f}%')\n",
        "print(f'The test set loss is {test_loss:.3f}')"
      ],
      "metadata": {
        "id": "rQDnlYDyqBYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code evaluates the model's performance by computing and visualizing the confusion matrix as a heatmap."
      ],
      "metadata": {
        "id": "xGteMnpxpA90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_matrix_net = confusion_matrix(y_true = test_labels,\n",
        "                    y_pred = test_predictions)\n",
        "\n",
        "# Turn matrix to percentages\n",
        "confusion_matrix_net = confusion_matrix_net.astype('float') / confusion_matrix_net.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Turn to dataframe\n",
        "df_cm = pd.DataFrame(\n",
        "        confusion_matrix_net,\n",
        "        index=np.unique(test_labels),\n",
        "        columns=np.unique(test_labels),\n",
        ")\n",
        "\n",
        "# Parameters of the image\n",
        "figsize = (10,7)\n",
        "fontsize=14\n",
        "\n",
        "# Create image\n",
        "fig = plt.figure(figsize=figsize)\n",
        "heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n",
        "\n",
        "# Make it nicer\n",
        "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "\n",
        "# Add labels\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "# Plot!\n",
        "plt.savefig('AUC_LSTM.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1VmmsoOqqFFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(test_labels, test_probs)\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "auc = np.round(roc_auc_score(y_true = test_labels,\n",
        "                             y_score = test_probs),\n",
        "              decimals = 3)\n",
        "\n",
        "# Create and show the plot\n",
        "plt.plot(fpr,tpr,label=\"Delinquency Status, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.savefig('ROC_LSTM.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EjmW9X6QqGdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we calculate and visualize the ROC (Receiver Operating Characteristic) curve to evaluate the performance of the model."
      ],
      "metadata": {
        "id": "FgMA3yWYpVkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM with multiple layers\n",
        "To improve training, we can try to chain LSTMs. This can help as you won't need to add just one very large (thus intractable) LSTM and instead can train two smaller ones. This is easily done in pytorch simply by setting the **`layer_dim`** parameter to a higher number. Let's try to stack two LSTMs and see the performance."
      ],
      "metadata": {
        "id": "sIq3mMu_pd3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_lstm_model = LSTMClassifier(n_vars, 64, 2, 64, 1).to(device)\n",
        "print(multi_lstm_model)\n",
        "\n",
        "# Draw the model\n",
        "model_graph = draw_graph(multi_lstm_model, input_size=(1, 64, n_vars),\n",
        "                         device=device,\n",
        "                         expand_nested=True)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "c3MwncQQpbwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, this model now passes a much more reduced number of features to the LSTM layer, thus allowing for a reduced complexity. Of course, this will only be as good as the features are, so you'll need to experiment to get this right.\n",
        "\n",
        "Let's train the model.\n",
        "\n",
        "We will be very aggressive with the norm clipping now. You can identify the need for this if the losses are very unstable. Experiment with this value for your own applications! We'll also use Adam as our optimizer."
      ],
      "metadata": {
        "id": "4WTYgRerqALT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up optimizer and loss\n",
        "learning_rate = 0.0001\n",
        "optimizer = optim.Adam(multi_lstm_model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
        "\n",
        "# Set global run parameters\n",
        "best_vloss = 10000000"
      ],
      "metadata": {
        "id": "jsmWfdVcp-Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Create the data loaders\n",
        "train_loader, val_loader = create_train_val_dataloaders(x_train, y_train.reshape(-1,1),\n",
        "                                                       batch_size=batch_size)\n",
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"validation\": val_loader\n",
        "}"
      ],
      "metadata": {
        "id": "XM9yLRXVqQYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 20\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Set run parameters\n",
        "n_epochs = 100\n",
        "liveloss = PlotLosses()\n",
        "\n",
        "\n",
        "# Train!\n",
        "for epoch in range(n_epochs):\n",
        "   # Run the epoch\n",
        "  logs = {}\n",
        "\n",
        "  # Run a train epoch, and then a validation epoch.\n",
        "  for phase in ['train', 'validation']:\n",
        "      if phase == 'train':\n",
        "          multi_lstm_model.train()\n",
        "      else:\n",
        "          multi_lstm_model.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      for i, data in enumerate(dataloaders[phase]):\n",
        "          inputs, labels = data\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = multi_lstm_model(inputs).to(device)\n",
        "          loss = loss_fn(outputs, labels)\n",
        "\n",
        "          if phase == 'train':\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              # Clip the gradient norm\n",
        "              nn.utils.clip_grad_norm_(multi_lstm_model.parameters(), 0.1)\n",
        "              # Backpropagate\n",
        "              optimizer.step()\n",
        "\n",
        "          preds = softmax_func(outputs.detach().cpu().numpy())\n",
        "          preds = np.round(preds)\n",
        "          running_loss += loss.detach() * inputs.size(0)\n",
        "          running_corrects += np.sum(preds.flatten() == labels.data.flatten().cpu().numpy())\n",
        "\n",
        "          if i % 10 == 9:\n",
        "            batch_loss = running_loss / (10 * (i+1))\n",
        "            print(f'{phase} batch {i+1} loss: {batch_loss:.3f}')\n",
        "            tb_x = epoch * len(dataloaders[phase]) + i + 1\n",
        "\n",
        "          # Delete the used VRAM\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "      epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "      epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
        "\n",
        "      prefix = ''\n",
        "      if phase == 'validation':\n",
        "          prefix = 'val_'\n",
        "\n",
        "          # Track best performance, and save the model's state\n",
        "          if epoch_loss < best_vloss:\n",
        "              best_vloss = epoch_loss\n",
        "              model_path = 'best_multi_lstm_model.pth'\n",
        "              print(f'New best model found. Saving it as {model_path}')\n",
        "              torch.save(multi_lstm_model.state_dict(), model_path)\n",
        "\n",
        "      logs[prefix + 'log loss'] = epoch_loss.item()\n",
        "      logs[prefix + 'accuracy'] = epoch_acc.item()\n",
        "\n",
        "  liveloss.update(logs)\n",
        "  liveloss.send()"
      ],
      "metadata": {
        "id": "hAOLwsVCqU4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LSTM model now achieves a better loss. Keep in mind that this was done without any data cleaning—only standardization!\n",
        "\n",
        "Notably, the optimizer took some time to find the right direction of descent, emphasizing the importance of patience during training.\n",
        "\n",
        "It’s easy to introduce redundant layers or unnecessary time series features, which may impact performance. I encourage you to experiment with different architectures and continue training—it shouldn’t be too difficult to further improve the results.\n",
        "\n",
        "Now, let’s evaluate the model on the test set. We begin by loading the best model."
      ],
      "metadata": {
        "id": "gXhw2v8GykFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "liveloss.send()\n",
        "plt.savefig('MultiLSTM_Training.pdf')"
      ],
      "metadata": {
        "id": "390QJKmOytvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_lstm_model.load_state_dict(torch.load('best_multi_lstm_model.pth'))"
      ],
      "metadata": {
        "id": "-KtlcM52qiw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper to save memory by not recomputing gradients.\n",
        "with torch.no_grad():\n",
        "    # Set the model in evaluation mode.\n",
        "    multi_lstm_model.eval()\n",
        "\n",
        "    # Calculate running loss and accuracy\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    test_labels = np.array([])\n",
        "    test_probs = np.array([])\n",
        "    test_predictions = np.array([])\n",
        "\n",
        "    # Apply to the test set\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs, labels = data\n",
        "        test_labels = np.append(test_labels, labels.cpu().numpy())\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = multi_lstm_model(inputs)\n",
        "        test_probs = np.append(test_probs, outputs.cpu().numpy())\n",
        "        outputs = outputs.to(device)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        preds = softmax_func(outputs.cpu().numpy())\n",
        "        preds = np.round(preds)\n",
        "        test_predictions = np.append(test_predictions, preds)\n",
        "        running_loss += loss.detach() * inputs.size(0)\n",
        "        running_corrects += np.sum(preds.flatten() == labels.data.flatten().cpu().numpy())\n",
        "\n",
        "test_loss = running_loss / len(test_loader.dataset)\n",
        "test_acc = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "print(f'The test set accuracy is {test_acc*100:.2f}%')\n",
        "print(f'The test set loss is {test_loss:.3f}')"
      ],
      "metadata": {
        "id": "0LobcSntqq4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 71.61% of test accuracy. The ROC curve will give us a better view of what is happening."
      ],
      "metadata": {
        "id": "dmph2z-nzhaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate confusion matrix\n",
        "confusion_matrix_net = confusion_matrix(y_true = test_labels,\n",
        "                    y_pred = test_predictions)\n",
        "\n",
        "# Turn matrix to percentages\n",
        "confusion_matrix_net = confusion_matrix_net.astype('float') / confusion_matrix_net.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Turn to dataframe\n",
        "df_cm = pd.DataFrame(\n",
        "        confusion_matrix_net,\n",
        "        index=np.unique(test_labels),\n",
        "        columns=np.unique(test_labels),\n",
        ")\n",
        "\n",
        "# Parameters of the image\n",
        "figsize = (10,7)\n",
        "fontsize=14\n",
        "\n",
        "# Create image\n",
        "fig = plt.figure(figsize=figsize)\n",
        "heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n",
        "\n",
        "# Make it nicer\n",
        "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "\n",
        "# Add labels\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "# Plot!\n",
        "plt.savefig('AUC_MultiLSTM.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J6M_iVrQqswR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(test_labels, test_probs)\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "auc = np.round(roc_auc_score(y_true = test_labels,\n",
        "                             y_score = test_probs),\n",
        "              decimals = 3)\n",
        "\n",
        "# Create and show the plot\n",
        "plt.plot(fpr,tpr,label=\"Delinquency Status - Multi-LSTM, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.savefig('ROC_MultiLSTM.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LS2RmqhMqvJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We achieve a slightly better AUC! This was all done with very little processing. We are able to successfully combine eight different time series into a unique model."
      ],
      "metadata": {
        "id": "49jM81b1qs_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU\n",
        "Now we will train a GRU. In theory, a GRU will be able to reach the same results as the LSTM but using less parameters.\n",
        "\n",
        "GRUs are more efficient, so you can either use the same size that will run faster, or increase the size to try to learn more. Let's do the former."
      ],
      "metadata": {
        "id": "lmZAz_9J0PWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GRU model\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, classifier_dim, output_dim):\n",
        "        super(GRUClassifier, self).__init__()\n",
        "\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        # Building your GRU\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "\n",
        "        # Classifier network\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, classifier_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(classifier_dim, output_dim),\n",
        "            #nn.Softmax(dim=1) # No need for softmax with logit loss.\n",
        "       )\n",
        "\n",
        "    # Forward method\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        out, hn = self.gru(x, h0)\n",
        "        out = self.classifier(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "R6UToLs8qIaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "n_vars = x_train.shape[1]\n",
        "gru_model = GRUClassifier(n_vars, 64, 2, 64, 1).to(device)\n",
        "print(gru_model)"
      ],
      "metadata": {
        "id": "yoJ7iIHzbbNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! Let's train the model now. The process is the same as before."
      ],
      "metadata": {
        "id": "PQNnU5jl1BYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up optimizer and loss\n",
        "learning_rate = 0.0001\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
        "\n",
        "# Set global run parameters\n",
        "best_vloss = 10000000"
      ],
      "metadata": {
        "id": "-HjJmSxCbcij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data loaders\n",
        "train_loader, val_loader = create_train_val_dataloaders(x_train, y_train.reshape(-1,1),\n",
        "                                                       batch_size=batch_size)\n",
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"validation\": val_loader\n",
        "}"
      ],
      "metadata": {
        "id": "fyL0j41fbdX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 20\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Set run parameters\n",
        "n_epochs = 100\n",
        "liveloss = PlotLosses()\n",
        "\n",
        "# Train!\n",
        "for epoch in range(n_epochs):\n",
        "   # Run the epoch\n",
        "  logs = {}\n",
        "\n",
        "  # Run a train epoch, and then a validation epoch.\n",
        "  for phase in ['train', 'validation']:\n",
        "      if phase == 'train':\n",
        "          gru_model.train()\n",
        "      else:\n",
        "          gru_model.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      for i, data in enumerate(dataloaders[phase]):\n",
        "          inputs, labels = data\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = gru_model(inputs).to(device)\n",
        "          loss = loss_fn(outputs, labels)\n",
        "\n",
        "          if phase == 'train':\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              # Clip the gradient norm\n",
        "              nn.utils.clip_grad_norm_(gru_model.parameters(), 0.1)\n",
        "              # Backpropagate\n",
        "              optimizer.step()\n",
        "\n",
        "          preds = softmax_func(outputs.detach().cpu().numpy())\n",
        "          preds = np.round(preds)\n",
        "          running_loss += loss.detach() * inputs.size(0)\n",
        "          running_corrects += np.sum(preds.flatten() == labels.data.flatten().cpu().numpy())\n",
        "\n",
        "          if i % 10 == 9:\n",
        "            batch_loss = running_loss / (10 * (i+1))\n",
        "            print(f'{phase} batch {i+1} loss: {batch_loss:.3f}')\n",
        "            tb_x = epoch * len(dataloaders[phase]) + i + 1\n",
        "\n",
        "          # Delete the used VRAM\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "      epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "      epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
        "\n",
        "      prefix = ''\n",
        "      if phase == 'validation':\n",
        "          prefix = 'val_'\n",
        "\n",
        "          # Track best performance, and save the model's state\n",
        "          if epoch_loss < best_vloss:\n",
        "              best_vloss = epoch_loss\n",
        "              model_path = 'best_gru_model.pth'\n",
        "              print(f'New best model found. Saving it as {model_path}')\n",
        "              torch.save(gru_model.state_dict(), model_path)\n",
        "\n",
        "      logs[prefix + 'log loss'] = epoch_loss.item()\n",
        "      logs[prefix + 'accuracy'] = epoch_acc.item()\n",
        "\n",
        "  liveloss.update(logs)\n",
        "  liveloss.send()"
      ],
      "metadata": {
        "id": "qEpA_RPEbee1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model.load_state_dict(torch.load('best_gru_model.pth'))"
      ],
      "metadata": {
        "id": "UC_D70LBbilN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper to save memory by not recomputing gradients.\n",
        "with torch.no_grad():\n",
        "    # Set the model in evaluation mode.\n",
        "    gru_model.eval()\n",
        "\n",
        "    # Calculate running loss and accuracy\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    test_labels = np.array([])\n",
        "    test_probs = np.array([])\n",
        "    test_predictions = np.array([])\n",
        "\n",
        "    # Apply to the test set\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs, labels = data\n",
        "        test_labels = np.append(test_labels, labels.cpu().numpy())\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = gru_model(inputs)\n",
        "        test_probs = np.append(test_probs, outputs.cpu().numpy())\n",
        "        outputs = outputs.to(device)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        preds = softmax_func(outputs.cpu().numpy())\n",
        "        preds = np.round(preds)\n",
        "        test_predictions = np.append(test_predictions, preds)\n",
        "        running_loss += loss.detach() * inputs.size(0)\n",
        "        running_corrects += np.sum(preds.flatten() == labels.data.flatten().cpu().numpy())\n",
        "\n",
        "test_loss = running_loss / len(test_loader.dataset)\n",
        "test_acc = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "print(f'The test set accuracy is {test_acc*100:.2f}%')\n",
        "print(f'The test set loss is {test_loss:.3f}')"
      ],
      "metadata": {
        "id": "t0kByX1Tbp5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate confusion matrix\n",
        "confusion_matrix_net = confusion_matrix(y_true = test_labels,\n",
        "                    y_pred = test_predictions)\n",
        "\n",
        "# Turn matrix to percentages\n",
        "confusion_matrix_net = confusion_matrix_net.astype('float') / confusion_matrix_net.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Turn to dataframe\n",
        "df_cm = pd.DataFrame(\n",
        "        confusion_matrix_net,\n",
        "        index=np.unique(test_labels),\n",
        "        columns=np.unique(test_labels),\n",
        ")\n",
        "\n",
        "# Parameters of the image\n",
        "figsize = (10,7)\n",
        "fontsize=14\n",
        "\n",
        "# Create image\n",
        "fig = plt.figure(figsize=figsize)\n",
        "heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n",
        "\n",
        "# Make it nicer\n",
        "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "\n",
        "# Add labels\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "# Plot!\n",
        "plt.savefig('AUC_GRU.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JdRCe6esbqd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(test_labels, test_probs)\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "auc = np.round(roc_auc_score(y_true = test_labels,\n",
        "                             y_score = test_probs),\n",
        "              decimals = 3)\n",
        "\n",
        "# Create and show the plot\n",
        "plt.plot(fpr,tpr,label=\"Delinquency Status - GRU, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.savefig('ROC_GRU.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1PKjAPKubsDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got an even better model! Try to see if you can reach better ones! Let's load the optimal parameters and measure performance.\n",
        "\n",
        "Can you do better? Experiment with the parameters."
      ],
      "metadata": {
        "id": "J7xtZg-k1F5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining static variables: A hybrid GRU.\n",
        "\n",
        "In many cases, time series data includes static variables that remain constant over time. For example, in a loan dataset, static features such as credit score, property type, and occupancy status can provide valuable insights.\n",
        "\n",
        "In this section, we will integrate these static variables into a GRU model and evaluate whether they improve performance.\n",
        "\n",
        "First, we load and scale the original dataset, which contains information recorded at the time of loan application."
      ],
      "metadata": {
        "id": "e_TdiW1J2I8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load filtered static dataset\n",
        "static_df = pd.read_csv(\"TIme_Series_Lab_Origin_sampled.csv\")\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_cols = [\"CREDIT_SCORE\", \"CLTV\", \"DTI_RATIO\", \"ORIGINAL_UPB\", \"ORIGINAL_LOAN_TERM\"]  # Excluding \"NUMBER_OF_BORROWERS\"\n",
        "categorical_cols = [\"FIRST_TIME_HOMEBUYER\", \"OCCUPANCY_STATUS\", \"PROPERTY_TYPE\"]"
      ],
      "metadata": {
        "id": "LQofBdspbwcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add LOAN_NUMBER back before merging\n",
        "x_df[\"LOAN_NUMBER\"] = x_df.index  # Assign index (loan numbers) back to x_df\n",
        "\n",
        "# Assign LOAN_NUMBER to train and test sets\n",
        "loan_numbers_train = x_df.loc[x_df[\"if_test\"] == 0, \"LOAN_NUMBER\"]\n",
        "loan_numbers_test = x_df.loc[x_df[\"if_test\"] == 1, \"LOAN_NUMBER\"]\n",
        "\n",
        "# Split static data into train and test based on LOAN_NUMBER\n",
        "static_train = static_df[static_df[\"LOAN_NUMBER\"].isin(loan_numbers_train)]\n",
        "static_test = static_df[static_df[\"LOAN_NUMBER\"].isin(loan_numbers_test)]\n",
        "\n",
        "# --- SCALE NUMERIC FEATURES ---\n",
        "scaler = StandardScaler()\n",
        "static_train[numeric_cols] = scaler.fit_transform(static_train[numeric_cols])  # Fit on train\n",
        "static_test[numeric_cols] = scaler.transform(static_test[numeric_cols])  # Apply to test\n",
        "\n",
        "# --- ONE-HOT ENCODE CATEGORICAL FEATURES ---\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "encoded_train = encoder.fit_transform(static_train[categorical_cols])  # Fit on train\n",
        "encoded_test = encoder.transform(static_test[categorical_cols])  # Apply to test\n",
        "\n",
        "# Convert to DataFrame\n",
        "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(categorical_cols), index=static_train.index)\n",
        "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(categorical_cols), index=static_test.index)"
      ],
      "metadata": {
        "id": "MSxtZL5Rb984"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preserve LOAN_NUMBER and NUMBER_OF_BORROWERS (without scaling)\n",
        "static_train_final = pd.concat([\n",
        "    static_train[[\"LOAN_NUMBER\", \"NUMBER_OF_BORROWERS\"]],  # Keep unscaled\n",
        "    static_train[numeric_cols],  # Already scaled numeric features\n",
        "    encoded_train_df  # One-hot encoded categorical features\n",
        "], axis=1)\n",
        "\n",
        "static_test_final = pd.concat([\n",
        "    static_test[[\"LOAN_NUMBER\", \"NUMBER_OF_BORROWERS\"]],  # Keep unscaled\n",
        "    static_test[numeric_cols],  # Already scaled numeric features\n",
        "    encoded_test_df  # One-hot encoded categorical features\n",
        "], axis=1)\n",
        "\n",
        "print(f\"Processed Train Static Data Shape: {static_train_final.shape}\")\n",
        "print(f\"Processed Test Static Data Shape: {static_test_final.shape}\")"
      ],
      "metadata": {
        "id": "__5LmM1_2Uc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert index to DataFrame to allow merging\n",
        "x_train = x_train.copy()\n",
        "x_test = x_test.copy()\n",
        "\n",
        "# Add LOAN_NUMBER column before merging\n",
        "x_train[\"LOAN_NUMBER\"] = loan_numbers_train.values\n",
        "x_test[\"LOAN_NUMBER\"] = loan_numbers_test.values\n",
        "\n",
        "# Merge static data with LSTM sequences\n",
        "x_train = x_train.merge(static_train_final, on=\"LOAN_NUMBER\", how=\"left\")\n",
        "x_test = x_test.merge(static_test_final, on=\"LOAN_NUMBER\", how=\"left\")\n",
        "\n",
        "# Drop LOAN_NUMBER after merging\n",
        "x_train.drop(columns=[\"LOAN_NUMBER\"], inplace=True)\n",
        "x_test.drop(columns=[\"LOAN_NUMBER\"], inplace=True)\n",
        "\n",
        "print(f\"x_train Shape After Merge: {x_train.shape}\")\n",
        "print(f\"x_test Shape After Merge: {x_test.shape}\")"
      ],
      "metadata": {
        "id": "WJaVpGSA2bXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we merge the static dataset with the time series dataset to incorporate time-invariant features into our analysis."
      ],
      "metadata": {
        "id": "UqCJwKG73WUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several ways to integrate static variables into a time series model. One approach is to concatenate static features with the model’s output before the final layer. Another option is to inject static features at every timestep in the model input. However, the latter requires expanding static features to match the time series length, which increases memory usage.\n",
        "\n",
        "In this implementation, we will use the concatenation approach."
      ],
      "metadata": {
        "id": "DFtFOF2f3fgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Hybrid GRU model\n",
        "class HybridGRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, static_dim, classifier_dim, output_dim):\n",
        "        super(HybridGRU, self).__init__()\n",
        "\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        # GRU for time-series features\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "\n",
        "        # Fully connected layer for static features\n",
        "        self.fc_static = nn.Linear(static_dim, hidden_dim)\n",
        "\n",
        "        # Classifier network\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, classifier_dim),  # Double hidden_dim due to concatenation\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(classifier_dim, output_dim),\n",
        "            # nn.Softmax(dim=1) # No need for softmax with logit loss.\n",
        "        )\n",
        "\n",
        "    # Forward method\n",
        "    def forward(self, x_seq, x_static):\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.layer_dim, x_seq.size(0), self.hidden_dim).to(x_seq.device)\n",
        "\n",
        "        # Forward pass through GRU\n",
        "        out, hn = self.gru(x_seq, h0)\n",
        "        gru_features = out[:, -1, :]  # Extract the last hidden state\n",
        "\n",
        "        # Forward pass for static features\n",
        "        static_features = self.fc_static(x_static)\n",
        "\n",
        "        # Concatenate GRU output and static features\n",
        "        combined_features = torch.cat((gru_features, static_features), dim=1)\n",
        "\n",
        "        # Final classification layer\n",
        "        output = self.classifier(combined_features)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "jF3jYdhgcH1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "time_series_indices = list(range(0, 8))  # First 8 columns are time-series\n",
        "static_indices = list(range(8, x_train.shape[1]))  # Remaining columns are static features\n",
        "\n",
        "\n",
        "# Model Parameters\n",
        "input_size = len(time_series_indices)  # Features per timestep\n",
        "hidden_size = 64  # hidden units\n",
        "num_layers = 2  # GRU layers\n",
        "static_size = len(static_indices)  # Static feature size\n",
        "classifier_dim = 128\n",
        "output_size = 1  # Binary classification (0/1)\n",
        "\n",
        "# Initialize the model\n",
        "hybrid_model = HybridGRU(input_size, hidden_size, num_layers, static_size, classifier_dim, output_size).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "2gEqA9k5cKUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up optimizer and loss\n",
        "learning_rate = 0.0001\n",
        "optimizer = optim.Adam(hybrid_model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
        "\n",
        "# Set global run parameters\n",
        "best_vloss = 10000000"
      ],
      "metadata": {
        "id": "CT0YqWRycL2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to modify our train_val_dataloader function accordingly."
      ],
      "metadata": {
        "id": "74QQt_bR-KKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_train_val_dataloaders_with_static(\n",
        "    x_train_df, y_train, time_series_indices, static_indices, batch_size=64, val_size=0.33, seed=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Create DataLoader instances for training and validation datasets suited for Hybrid LSTM/GRU models,\n",
        "    ensuring stratified sampling for validation split.\n",
        "\n",
        "    Parameters:\n",
        "    x_train_df (pandas.DataFrame): DataFrame containing both time-series and static features.\n",
        "    y_train (numpy.ndarray or list): Labels for training, shape (num_samples, 1).\n",
        "    time_series_indices (list): List of column indices for time-series features.\n",
        "    static_indices (list): List of column indices for static features.\n",
        "    batch_size (int): The batch size for both train and validation loaders.\n",
        "    val_size (float): The fraction of the data to be used for validation.\n",
        "\n",
        "    Returns:\n",
        "    train_loader (DataLoader): DataLoader for the training set.\n",
        "    val_loader (DataLoader): DataLoader for the validation set.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Extract and correctly reshape time-series data\n",
        "    x_seq_train = np.stack([\n",
        "        np.column_stack(row) for row in x_train_df.iloc[:, time_series_indices].values\n",
        "    ])\n",
        "\n",
        "    # Extract static features separately\n",
        "    x_static_train = x_train_df.iloc[:, static_indices].values\n",
        "\n",
        "    # Ensure y_train is a numpy array\n",
        "    y_train = np.array(y_train) if not isinstance(y_train, np.ndarray) else y_train\n",
        "\n",
        "    # Convert to 2D array if necessary\n",
        "    if y_train.ndim == 1:\n",
        "        y_train = y_train[:, None]\n",
        "\n",
        "    # Perform stratified split\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        np.arange(len(y_train)),  # Index-based split\n",
        "        test_size=val_size,\n",
        "        stratify=y_train,  # Ensures class proportions are maintained\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    x_seq_tensor = torch.tensor(x_seq_train, dtype=torch.float32)\n",
        "    x_static_tensor = torch.tensor(x_static_train, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Use stratified indices for train and validation sets\n",
        "    x_seq_train_tensor, x_seq_val_tensor = x_seq_tensor[train_idx], x_seq_tensor[val_idx]\n",
        "    x_static_train_tensor, x_static_val_tensor = x_static_tensor[train_idx], x_static_tensor[val_idx]\n",
        "    y_train_tensor, y_val_tensor = y_tensor[train_idx], y_tensor[val_idx]\n",
        "\n",
        "    # Validate shape before training\n",
        "    print(f\"x_seq_train_tensor shape: {x_seq_train_tensor.shape}\")  # (train_samples, seq_len, num_features)\n",
        "    print(f\"x_static_train_tensor shape: {x_static_train_tensor.shape}\")  # (train_samples, num_static_features)\n",
        "    print(f\"y_train_tensor shape: {y_train_tensor.shape}\")  # (train_samples, 1)\n",
        "\n",
        "    # Create TensorDatasets with two inputs\n",
        "    train_data = TensorDataset(x_seq_train_tensor, x_static_train_tensor, y_train_tensor)\n",
        "    val_data = TensorDataset(x_seq_val_tensor, x_static_val_tensor, y_val_tensor)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n"
      ],
      "metadata": {
        "id": "JCX2VeDt7GpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data loaders\n",
        "batch_size = 2028\n",
        "\n",
        "\n",
        "# Create train and validation DataLoaders\n",
        "train_loader, val_loader = create_train_val_dataloaders_with_static(\n",
        "    x_train, y_train, time_series_indices, static_indices, batch_size=batch_size\n",
        ")\n",
        "\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"validation\": val_loader\n",
        "}\n",
        "\n",
        "# Softmax function for the output\n",
        "softmax_func = np.vectorize(lambda x: 1/(1+np.exp(-1 * x)))"
      ],
      "metadata": {
        "id": "XvqmHx67cN1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 20\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "\n",
        "# Initialize `livelossplot`\n",
        "liveloss = PlotLosses()\n",
        "\n",
        "# Run Parameters\n",
        "n_epochs = 100\n",
        "best_vloss = float('inf')\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(n_epochs):\n",
        "    logs = {}\n",
        "\n",
        "    for phase in [\"train\", \"validation\"]:\n",
        "        if phase == \"train\":\n",
        "            hybrid_model.train()\n",
        "        else:\n",
        "            hybrid_model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for i, (inputs_seq, inputs_static, labels) in enumerate(dataloaders[phase]):\n",
        "            inputs_seq, inputs_static, labels = (\n",
        "                inputs_seq.to(device),\n",
        "                inputs_static.to(device),\n",
        "                labels.to(device),\n",
        "            )\n",
        "\n",
        "            # Forward pass\n",
        "            with torch.set_grad_enabled(phase == \"train\"):\n",
        "                outputs = hybrid_model(inputs_seq, inputs_static)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "\n",
        "                if phase == \"train\":\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "\n",
        "                    # Gradient Clipping to avoid exploding gradients\n",
        "                    nn.utils.clip_grad_norm_(hybrid_model.parameters(), 0.1)\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            preds = softmax_func(outputs.detach().cpu().numpy())\n",
        "            preds = np.round(preds)  # Convert to binary classification (0 or 1)\n",
        "\n",
        "            # Track metrics\n",
        "            running_loss += loss.item() * inputs_seq.size(0)\n",
        "            running_corrects += np.sum(preds.flatten() == labels.data.flatten().cpu().numpy())\n",
        "\n",
        "            # Print batch loss every 10 steps\n",
        "            if i % 10 == 9:\n",
        "                batch_loss = running_loss / (10 * (i + 1))\n",
        "                print(f\"{phase} batch {i+1} loss: {batch_loss:.3f}\")\n",
        "\n",
        "            # Clear GPU memory\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Compute epoch loss and accuracy\n",
        "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "        epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
        "\n",
        "        prefix = \"\"\n",
        "        if phase == \"validation\":\n",
        "            prefix = \"val_\"\n",
        "\n",
        "            # Track best validation loss, save the best model\n",
        "            if epoch_loss < best_vloss:\n",
        "                best_vloss = epoch_loss\n",
        "                model_path = \"best_hybrid_model.pth\"\n",
        "                print(f\"New best model found. Saving it as {model_path}\")\n",
        "                torch.save(hybrid_model.state_dict(), model_path)\n",
        "\n",
        "        logs[prefix + \"log loss\"] = epoch_loss\n",
        "        logs[prefix + \"accuracy\"] = epoch_acc\n",
        "\n",
        "    # Update live loss plot\n",
        "    liveloss.update(logs)\n",
        "    liveloss.send()\n"
      ],
      "metadata": {
        "id": "REFK4a6tcQCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we need to modify our test_dataloader function."
      ],
      "metadata": {
        "id": "lBVFA0XR-PDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_dataloader_with_static(x_test_df, time_series_indices, static_indices, y_test=None, batch_size=64, seed=42):\n",
        "    \"\"\"\n",
        "    Create DataLoader instance for the test dataset, processing both time-series and static features.\n",
        "\n",
        "    Parameters:\n",
        "    x_test_df (pandas.DataFrame): DataFrame containing both time-series and static features.\n",
        "    time_series_indices (list): List of column indices for time-series features.\n",
        "    static_indices (list): List of column indices for static features.\n",
        "    y_test (numpy.ndarray, optional): Labels for testing. Pass None if there are no labels.\n",
        "    batch_size (int): The batch size for the test loader.\n",
        "    seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    test_loader (DataLoader): DataLoader for the test set.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Extract and correctly reshape time-series data\n",
        "    x_seq_test = np.stack([\n",
        "        np.column_stack(row) for row in x_test_df.iloc[:, time_series_indices].values\n",
        "    ])\n",
        "\n",
        "    # Extract static features separately\n",
        "    x_static_test = x_test_df.iloc[:, static_indices].values\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    x_seq_tensor = torch.tensor(x_seq_test, dtype=torch.float32)  # (num_samples, seq_len, num_features)\n",
        "    x_static_tensor = torch.tensor(x_static_test, dtype=torch.float32)  # (num_samples, num_static_features)\n",
        "\n",
        "    # Validate shape before using\n",
        "    print(f\"x_seq_tensor shape: {x_seq_tensor.shape}\")  # Expected: (num_samples, seq_len, num_features)\n",
        "    print(f\"x_static_tensor shape: {x_static_tensor.shape}\")  # Expected: (num_samples, num_static_features)\n",
        "\n",
        "    # If labels are provided, process them\n",
        "    if y_test is not None:\n",
        "        if not isinstance(y_test, np.ndarray):\n",
        "            y_test = np.array(y_test)\n",
        "\n",
        "        # Convert labels to PyTorch tensor\n",
        "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)  # Ensure (num_samples, 1)\n",
        "\n",
        "        # Ensure matching number of samples\n",
        "        assert len(x_seq_tensor) == len(y_test_tensor), \"Number of input samples and labels must match.\"\n",
        "        test_data = TensorDataset(x_seq_tensor, x_static_tensor, y_test_tensor)\n",
        "    else:\n",
        "        test_data = TensorDataset(x_seq_tensor, x_static_tensor)\n",
        "\n",
        "    # Create DataLoader\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return test_loader\n"
      ],
      "metadata": {
        "id": "aER6zSEV7RLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = create_test_dataloader_with_static(x_test, time_series_indices, static_indices, y_test=y_test)"
      ],
      "metadata": {
        "id": "qoxglZwTcTgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_model.load_state_dict(torch.load('best_hybrid_model.pth'))"
      ],
      "metadata": {
        "id": "qOIvVpsicWB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper to save memory by not recomputing gradients.\n",
        "with torch.no_grad():\n",
        "    # Set the model in evaluation mode.\n",
        "    hybrid_model.eval()\n",
        "\n",
        "    # Initialize loss and accuracy tracking\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    test_labels = np.array([])\n",
        "    test_probs = np.array([])\n",
        "    test_predictions = np.array([])\n",
        "\n",
        "    # Apply to the test set\n",
        "    for i, (inputs_seq, inputs_static, labels) in enumerate(test_loader):\n",
        "        test_labels = np.append(test_labels, labels.cpu().numpy())\n",
        "\n",
        "        # Move data to the appropriate device\n",
        "        inputs_seq, inputs_static, labels = (\n",
        "            inputs_seq.to(device),\n",
        "            inputs_static.to(device),\n",
        "            labels.to(device),\n",
        "        )\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = hybrid_model(inputs_seq, inputs_static)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Convert outputs to probabilities using sigmoid (since we use BCEWithLogitsLoss)\n",
        "        probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
        "        preds = np.round(probs)  # Convert to binary classification (0 or 1)\n",
        "\n",
        "        # Store predictions\n",
        "        test_probs = np.append(test_probs, probs)\n",
        "        test_predictions = np.append(test_predictions, preds)\n",
        "\n",
        "        # Track loss and accuracy\n",
        "        running_loss += loss.item() * inputs_seq.size(0)\n",
        "        running_corrects += np.sum(preds == labels.cpu().numpy().flatten())\n",
        "\n",
        "    # Compute final metrics\n",
        "    test_loss = running_loss / len(test_loader.dataset)\n",
        "    test_acc = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "# Print test results\n",
        "print(f'Test Accuracy: {test_acc * 100:.2f}%')\n",
        "print(f'Test Loss: {test_loss:.3f}')\n"
      ],
      "metadata": {
        "id": "tYAouTOvcW8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate confusion matrix\n",
        "confusion_matrix_net = confusion_matrix(y_true = test_labels,\n",
        "                    y_pred = test_predictions)\n",
        "\n",
        "# Turn matrix to percentages\n",
        "confusion_matrix_net = confusion_matrix_net.astype('float') / confusion_matrix_net.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Turn to dataframe\n",
        "df_cm = pd.DataFrame(\n",
        "        confusion_matrix_net,\n",
        "        index=np.unique(test_labels),\n",
        "        columns=np.unique(test_labels),\n",
        ")\n",
        "\n",
        "# Parameters of the image\n",
        "figsize = (10,7)\n",
        "fontsize=14\n",
        "\n",
        "# Create image\n",
        "fig = plt.figure(figsize=figsize)\n",
        "heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n",
        "\n",
        "# Make it nicer\n",
        "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "\n",
        "# Add labels\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "# Plot!\n",
        "plt.savefig('AUC_HybridGRU.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ixgOQDM0vli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(test_labels, test_probs)\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "auc = np.round(roc_auc_score(y_true = test_labels,\n",
        "                             y_score = test_probs),\n",
        "              decimals = 3)\n",
        "\n",
        "# Create and show the plot\n",
        "plt.plot(fpr,tpr,label=\"Delinquency Status - hybridGRU, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.savefig('ROC_HybridGRU.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mTS0RTztcaY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe an increase in AUC, indicating some improvement. More importantly, this improvement happens around the mid-part of the ROC curve, hinting it happens where prediction is harder. We can see how the hybrid model squeezes as much learning as possible from the data, by leveraging multiple data sources."
      ],
      "metadata": {
        "id": "TmUMKFtG2EJW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}