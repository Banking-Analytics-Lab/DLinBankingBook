{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Banking-Analytics-Lab/DLinBankingBook/blob/main/Labs/TextBook_Lab_Chap6_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUGkHUTWt5ws"
      },
      "source": [
        "# Chapter 6 Lab - Large Language Models\n",
        "\n",
        "In this lab, we will explore prompt engineering and fine-tuning for large language models (LLMs) using LLaMA 3.2 (3B) - Instruct.\n",
        "\n",
        "We will start by installing and importing the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ztso31tjOKZ"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers accelerate datasets peft\n",
        "!pip install -q -U bitsandbytes==0.45.2 trl fsspec==2025.3.2\n",
        "!pip install --no-build-isolation https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdzJJ8hZyhLS"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Set environmental variables\n",
        "from google.colab import userdata\n",
        "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"300\"  # Set timeout to 5 minutes\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_API_KEY')\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, log_loss, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "from IPython.display import Markdown, display\n",
        "import graphviz\n",
        "from matplotlib.colors import ListedColormap\n",
        "graphviz.set_jupyter_format('png')\n",
        "%matplotlib inline\n",
        "\n",
        "# Import Pytorch lybraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "# Huggingface\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from transformers import pipeline, logging, HfArgumentParser\n",
        "from transformers import set_seed\n",
        "import accelerate\n",
        "from datasets import load_dataset\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Shap\n",
        "import shap\n",
        "\n",
        "# peft\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "\n",
        "# wandb\n",
        "import wandb\n",
        "\n",
        "# trl\n",
        "from trl import SFTTrainer, setup_chat_format\n",
        "\n",
        "#Bitsandbytes\n",
        "import bitsandbytes as bnb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHALdHMuz1Oa"
      },
      "source": [
        "## Basic prompt engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZC6qjw7vvj3"
      },
      "source": [
        "To optimize model execution, we use Hugging Face's Accelerate library, which helps efficiently manage device placement (CPU/GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMghxFbhzMJu"
      },
      "outputs": [],
      "source": [
        "# Add accelerator\n",
        "accelerator = accelerate.Accelerator()\n",
        "device = accelerator.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04VhlS7uv29J"
      },
      "source": [
        "Before accessing restricted models or using private repositories, we need to authenticate with Hugging Face Hub.\n",
        "\n",
        "The following command prompts for your Hugging Face access token to enable secure model downloads and API access:\n",
        "\n",
        "How to Get Your Token?\n",
        "\n",
        "Visit Hugging Face Token Page.\n",
        "Generate a new token (select \"Write\" permissions if needed).\n",
        "Copy and paste the token when prompted in Colab.\n",
        "Once logged in, you can download gated models, access private datasets, and use Hugging Face services seamlessly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zUQiiwT4Quj"
      },
      "outputs": [],
      "source": [
        "login(token=os.environ[\"HF_TOKEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEnJRLhzwSYb"
      },
      "source": [
        "In this step, we load the LLaMA 3.2 (3B)-Instruct model using Hugging Face's transformers library and configure it for optimized execution on Google Colab's GPU.\n",
        "\n",
        "* The model we are using is LLaMA 3.2 (3B) Instruct, a fine-tuned version designed for instruction-following tasks.\n",
        "* We use [bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format) (Brain Floating Point 16-bit), a precision format that improves memory efficiency while maintaining accuracy.\n",
        "\n",
        "Downloading the model will take approximately 3 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M98gCfxm1tj6"
      },
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "dtype = torch.bfloat16\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype, device_map=\"auto\")\n",
        "model = accelerator.prepare(model)  # Ensure model is placed on the correct device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTkIqhoewxly"
      },
      "source": [
        "The following code displays the chat template used by the LLaMA 3.2 (3B) Instruct model to format conversations.\n",
        "\n",
        "What is a Chat Template?\n",
        "\n",
        "A chat template defines the structure of input messages before they are passed to the model. It ensures that the model correctly interprets the conversation flow and generates appropriate responses.\n",
        "\n",
        "If a chat template is not available, prompts must be manually formatted.\n",
        "In our case, we have a predefined chat template, simplifying the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa9BeejN7wTr"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0b9ZJIBxdsn"
      },
      "source": [
        "This code ensures that both the tokenizer and model configuration have a defined padding token to handle variable-length inputs properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSy4_rEmEauY"
      },
      "outputs": [],
      "source": [
        "# Ensure tokenizer has a padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9pvfCoQxfqb"
      },
      "source": [
        "Now, we initialize a text generation pipeline using the Hugging Face **`transformers`** library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEAYGv1MgiLl"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNI8OFZ1ORNT"
      },
      "source": [
        "Then, we format a conversation into a structured prompt using a chat template for the model.\n",
        "\n",
        "* The system message sets the modelâ€™s behavior.\n",
        "* The user message provides the specific query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg0VqphmEeor"
      },
      "outputs": [],
      "source": [
        "# Example Input\n",
        "system_message = \"You are an expert assistant specializing in Banking Analytics and Business Analytics. Provide structured and factual responses.\"\n",
        "user_prompt = \"Explain the key differences between Banking Analytics and Business Analytics\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_prompt,\n",
        "    },\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wkGJhXCOmpT"
      },
      "source": [
        "The following code generates text using a pre-trained language model via the pipe (text generation pipeline) and prints the generated response.\n",
        "\n",
        "\n",
        "* max_new_tokens=512 â†’ Limits the response length to 512 tokens.\n",
        "* do_sample=True â†’ Enables sampling (randomness) instead of deterministic outputs.\n",
        "* top_p=0.8 â†’ Uses nucleus sampling, meaning the model selects from the top 80% probability mass.\n",
        "* temperature=0.2 â†’ Controls randomness:\n",
        "\n",
        "    Lower values (e.g., 0.2) â†’ More focused and  deterministic responses.\n",
        "\n",
        "    Higher values (e.g., 1.0) â†’ More creative and diverse responses.\n",
        "\n",
        "* repetition_penalty=1.5 â†’ Penalizes repeated phrases to make the response more natural and varied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orlznIQhaB_8"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "outputs = pipe(prompt, max_new_tokens=512,\n",
        "               do_sample=True,\n",
        "               top_p=0.8,\n",
        "               temperature=0.2,\n",
        "               repetition_penalty=1.5)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5eLxeaWOzZd"
      },
      "source": [
        "It's working well! Let's improve readability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD028xJMah14"
      },
      "outputs": [],
      "source": [
        "display(\n",
        "    Markdown(\n",
        "            outputs[0][\"generated_text\"].split(\n",
        "                \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "            )[1]\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUpPM9Qby5vP"
      },
      "source": [
        "The response is well-written, but with prompt engineering, we can refine it further for improved structure and formatting.\n",
        "\n",
        "Now, let's enhance it by providing more specific instructions for the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv2ak99czkm7"
      },
      "outputs": [],
      "source": [
        "# Example Input\n",
        "system_message = \"\"\"STRICT INSTRUCTIONS:\n",
        "1. First, provide a **clear definition** of the topic.\n",
        "2. Then, explain **at least two key differences** in a **structured manner**.\n",
        "   - Each difference must be in a **separate paragraph**.\n",
        "   - Use **clear and concise language**.\n",
        "3. Do not include unnecessary information or extra details beyond the requested explanation.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "user_prompt = \"What is Banking Analytics, and how does it differ from general Business Analytics?\"\n",
        "\n",
        "\n",
        "# Use chat template for proper formatting\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_prompt,\n",
        "    },\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5bVv7eIbt41"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "outputs = pipe(prompt, max_new_tokens=512,\n",
        "               do_sample=True,\n",
        "               top_p=0.8,\n",
        "               temperature=0.2,\n",
        "               repetition_penalty=1.5)\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "            outputs[0][\"generated_text\"].split(\n",
        "                \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "            )[1]\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-RNjiXrzcS5"
      },
      "source": [
        "The model's response is now more structured and readable, accurately following the instructions provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Di3gfu1z-V8"
      },
      "source": [
        "## Prompt engineering with prediction model\n",
        "\n",
        "In this section, we will use prompt engineering to guide the LLM in explaining delinquency status predictions based on SHAP values.\n",
        "\n",
        "Let's begin by downloading the dataset to proceed with our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ii18tKhQg6h"
      },
      "outputs": [],
      "source": [
        "!gdown --fuzzy 'https://drive.google.com/file/d/1nrhxfnAkI0bZRXJiWu_JVKusAD9iHBpK/view?usp=sharing'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZcCeQXqaP4M"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('loan_app.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZCKq6L-0c9n"
      },
      "source": [
        "Next, we will prepare the dataset for training by applying feature encoding, scaling, and train-test splitting to ensure the model learns effectively from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5ztKZPHa0CX"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=[\"target\"])  # Features\n",
        "y = df[\"target\"]  # Target variable\n",
        "\n",
        "# Convert Categorical Features to Numerical\n",
        "categorical_columns = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "numerical_columns = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "X_encoded = encoder.fit_transform(X[categorical_columns])\n",
        "\n",
        "# Convert Encoded Data to DataFrame\n",
        "X_encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Drop original categorical columns and merge one-hot encoded features\n",
        "X = X.drop(columns=categorical_columns)\n",
        "X = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "# Train-Test Split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale Only Numerical Features (NOT one-hot encoded features)\n",
        "scaler = StandardScaler()\n",
        "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
        "X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])  # Use same scaler for test set\n",
        "\n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2AfK9R906UM"
      },
      "source": [
        "Now, we will train an XGBoost model to predict loan delinquency status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_pMwdEZcmH2"
      },
      "outputs": [],
      "source": [
        "negative_count = np.sum(y_train == 0)  # Count of class 0\n",
        "positive_count = np.sum(y_train == 1)  # Count of class 1\n",
        "scale_pos_weight = negative_count / positive_count  # Weight ratio\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_model = XGBClassifier(max_depth=3,\n",
        "                          learning_rate=0.01,\n",
        "                          n_estimators=200,\n",
        "                          verbosity=0,\n",
        "                          objective='binary:logistic',\n",
        "                          eval_metric=\"logloss\",\n",
        "                          booster='gbtree',\n",
        "                          n_jobs=-1,\n",
        "                          gamma=0.001,\n",
        "                          subsample=0.632,\n",
        "                          colsample_bytree=1,\n",
        "                          colsample_bylevel=1,\n",
        "                          colsample_bynode=1,\n",
        "                          reg_alpha=0,\n",
        "                          reg_lambda=0.1,\n",
        "                          random_state=428,\n",
        "                          tree_method=\"hist\",\n",
        "                          scale_pos_weight=scale_pos_weight\n",
        "                          )\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate Model Performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhqVwq6S1FqK"
      },
      "source": [
        "We analyzes feature importance in the XGBoost model using SHAP (SHapley Additive exPlanations) to explain why the model predicts a loan as delinquent or not.\n",
        "\n",
        "The SHAP summary plot provides:\n",
        "* Feature importance ranking (sorted by impact).\n",
        "* How each feature affects predictions (positive/negative impact).\n",
        "* Distribution of SHAP values for different feature values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0MU61D4egur"
      },
      "outputs": [],
      "source": [
        "# Explain Model Predictions using SHAP\n",
        "explainer = shap.Explainer(xgb_model, X_train)\n",
        "shap_values = explainer(X_test)  # SHAP values for test set\n",
        "\n",
        "# Summarize SHAP Values\n",
        "shap.summary_plot(shap_values, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCTEJcCZ1wPL"
      },
      "source": [
        "We retrieve and format SHAP values for one test sample, making it easier to generate human-readable explanations using Llama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEh8SfY_nPzG"
      },
      "outputs": [],
      "source": [
        "# Select an example loan case (e.g., first sample in test set)\n",
        "sample_index = 10  # Change this index if needed\n",
        "shap_values_sample = shap_values[sample_index].values\n",
        "sample_features = X_test.iloc[sample_index]\n",
        "\n",
        "# Convert SHAP values into dictionary format for input\n",
        "shap_dict = {feature: shap_values_sample[i] for i, feature in enumerate(sample_features.index)}\n",
        "\n",
        "# Target label for the selected sample\n",
        "pred_label = y_pred[sample_index]\n",
        "\n",
        "print(\"\\nSHAP Values for Sample Client:\\n\", shap_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGFSY1Lv12KK"
      },
      "source": [
        "Before proceeding to Llma, let's generate a bar plot of SHAP values for the sampled loan to visualize which features have the most impact on the delinquency prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj6EG7cToGmv"
      },
      "outputs": [],
      "source": [
        "shap.plots.bar(shap_values[sample_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVmEYmL92SwC"
      },
      "source": [
        "Finally, we format SHAP values into a structured prompt to enable Llama to generate an interpretable explanation of why a loan is predicted as delinquent or not.\n",
        "\n",
        "We first sort SHAP values by absolute magnitude, prioritizing the most impactful features. Then, we select the top 3 features to keep the explanation concise and focused.\n",
        "\n",
        "In the prompt, we will:\n",
        "\n",
        "* Ensure a structured format for clear and consistent LLaMA responses.\n",
        "* Eliminate unnecessary or unrelated information from the generated output.\n",
        "* Clearly state the model's decision using SHAP-based reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVo6Nqj0iE55"
      },
      "outputs": [],
      "source": [
        "def format_shap_explanation(system_message, sample_features, shap_dict, pred_label, scaler):\n",
        "    # Convert scaled numerical values back to original values\n",
        "    original_values = scaler.inverse_transform(sample_features[numerical_columns].values.reshape(1, -1))\n",
        "    original_feature_values = {feature: original_values[0][i] for i, feature in enumerate(numerical_columns)}\n",
        "\n",
        "    # Sort SHAP values by absolute magnitude (most impactful features first)\n",
        "    top_features = sorted(shap_dict.items(), key=lambda x: abs(x[1]), reverse=True)[:3]\n",
        "\n",
        "    # Generate SHAP explanation text\n",
        "    shap_text = \"\\n\".join(\n",
        "        [f\"{feature}: SHAP value = {shap_value:.4f}, feature value = {original_feature_values[feature]:.2f}\"\n",
        "         for feature, shap_value in top_features]\n",
        "    )\n",
        "\n",
        "    print(shap_text)\n",
        "    # Define loan delinquency status\n",
        "    delinquency_status = \"likely to be delinquent\" if pred_label == 1 else \"unlikely to be delinquent\"\n",
        "    print(\"predicted delinquency: \", delinquency_status)\n",
        "\n",
        "    # Construct a revised prompt with explicit instructions and structured format\n",
        "    user_prompt = f\"\"\"\n",
        "The model predicts that the client is {delinquency_status}.\n",
        "\n",
        "Here are the three most important features influencing the prediction:\n",
        "\n",
        "{shap_text}\n",
        "\n",
        "### Instructions:\n",
        "- Analyze how each of three features contributes to the prediction.\n",
        "- **Use correct feature names, not feature values.**\n",
        "- **Strictly follow the structured response format.**\n",
        "- **SHAP values must be interpreted correctly**:\n",
        "  - **A positive SHAP value means the feature increases delinquency risk.**\n",
        "  - **A negative SHAP value means the feature decreases delinquency risk.**\n",
        "\n",
        "### Response Format:\n",
        "Feature Name: [Feature Name]\n",
        "Effect on Risk: Explain whether a higher or lower value increases delinquency risk and why.\n",
        "SHAP Impact: Clearly state whether the SHAP value shows an increase or decrease in delinquency risk and explain its significance.\n",
        "\n",
        "### Example Response:\n",
        "\n",
        "Feature Name: Credit Score\n",
        "Effect on Risk: A higher credit score reduces delinquency risk because it indicates a strong repayment history and financial responsibility.\n",
        "SHAP Impact: The SHAP value **-0.5862** shows that **including credit score for this client decreases** the probability of delinquency, meaning the model considers this a strong indicator of financial reliability.\n",
        "\n",
        "Now begin your structured analysis:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWb36-LC3D1t"
      },
      "source": [
        "We also define instructions to guide the LLM in analyzing SHAP values for loan delinquency predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GhQDOqg9yP3"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "You are a financial risk analyst. Your job is to analyze SHAP values and provide structured, fact-based explanations of the model's predictions.\n",
        "\n",
        "Guidelines:\n",
        "1. Interpret SHAP values correctly:\n",
        "   - **A positive SHAP value means the feature increases delinquency risk.**\n",
        "   - **A negative SHAP value means the feature decreases delinquency risk.**\n",
        "2. **Do not contradict basic financial logic**:\n",
        "   - A **higher credit score should always reduce risk** unless explicitly stated otherwise.\n",
        "3. **Strictly follow the response format. Do not add extra text or repeat information.**\n",
        "4. **Do not argue against the given ranking of features.**\n",
        "5. **Avoid repetition, unnecessary details, or ranking errors.**\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "prompt = format_shap_explanation(system_message, sample_features, shap_dict, pred_label, scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFqZVg-WRxxZ"
      },
      "source": [
        "Finally, let's give it a try!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF9nUsS0oCxk"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "outputs = pipe(prompt, max_new_tokens=512,\n",
        "               do_sample=True,\n",
        "               top_p=0.5,\n",
        "               temperature=0.1,\n",
        "               repetition_penalty=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAupCdl6hOj9"
      },
      "outputs": [],
      "source": [
        "display(\n",
        "    Markdown(\n",
        "            outputs[0][\"generated_text\"].split(\n",
        "                \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "            )[1]\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk7l27wRR3xs"
      },
      "source": [
        "The model provides a fairly good interpretation of SHAP values, but the quality can be improved. We can enhance its performance through fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7-NlbLKq-Ms"
      },
      "source": [
        "## Fine-tuning Llama 3.2 - 3B Instruct\n",
        "\n",
        "In this section, we will fine-tune the LLaMA 3.2 - 3B Instruct model to enhance its ability to interpret SHAP values for loan delinquency predictions. Our goal is to tailor the model to generate clear, insightful explanations that financial institutions can use for credit scoring and risk assessment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OraR_ftSrPyL"
      },
      "source": [
        "The following code initializes Weights & Biases (W&B) for experiment tracking and logging during the fine-tuning process. A W&B authentication token is required to enable logging.\n",
        "\n",
        "To use W&B properly, ensure that you have set up an account and retrieved your authentication token from W&B before running wandb.login()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGtwFyWzreR5"
      },
      "outputs": [],
      "source": [
        "wandb.login()\n",
        "run = wandb.init(\n",
        "    project='Fine-tune Llama 3.2',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYJWBROerkEU"
      },
      "source": [
        "To proceed with fine-tuning, we need to download the dataset that will be used for training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9U2C_ERrjbm"
      },
      "outputs": [],
      "source": [
        "!gdown --fuzzy 'https://drive.google.com/file/d/15JtT_Jw9OdS3ZvecrYY8gB0s9jiyvEEa/view?usp=sharing'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1dVP4lnrxiy"
      },
      "source": [
        "We define the base model (Llama 3.2 3B Instruct), the name for the fine-tuned model, and the dataset used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rWohjH9rzJK"
      },
      "outputs": [],
      "source": [
        "base_model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "new_model = \"llama-3.2-3b-it-SHAP-Explainer\"\n",
        "dataset_name = \"llama3.2_finetune_data.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jogtn2Sur1kP"
      },
      "source": [
        "The following Checks the GPU's compute capability using **`torch.cuda.get_device_capability()`**.\n",
        "\n",
        "If the GPU supports compute capability 8.0 or higher (e.g., A100 on Colab Pro), it enables Flash Attention (flash_attention_2) for optimized training.\n",
        "Otherwise, it defaults to the standard (\"eager\") attention mechanism, ensuring compatibility with older GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu9mQAMLr4L0"
      },
      "outputs": [],
      "source": [
        "# Set torch dtype and attention implementation\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    torch_dtype = torch.bfloat16\n",
        "    attn_implementation = \"flash_attention_2\"\n",
        "\n",
        "else:\n",
        "    torch_dtype = torch.bfloat16\n",
        "    attn_implementation = \"eager\"\n",
        "\n",
        "print(f\"Using {attn_implementation} for training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIxLrIGVr73C"
      },
      "source": [
        "We set up and load a quantized LLaMA model using QLoRA (Quantized Low-Rank Adaptation) for memory-efficient fine-tuning with 4-bit precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq1y8Kl1r-n9"
      },
      "outputs": [],
      "source": [
        "# QLoRA config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=attn_implementation\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, device_map=\"auto\")\n",
        "\n",
        "# Ensure tokenizer has a padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
        "\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G49Jj6qqsCiw"
      },
      "source": [
        "We will load and prepare the fine-tuning dataset from a local JSON file containing 1,000 samples.\n",
        "\n",
        "It is then split into training (90%) and testing (10%) for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4phAkQNrsFPL"
      },
      "outputs": [],
      "source": [
        "# Load the local JSON dataset using pandas\n",
        "df_dataset = pd.read_json(\"llama3.2_finetune_data.json\")\n",
        "\n",
        "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(df_dataset)\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTb0SUKBsIev"
      },
      "source": [
        "Our dataset is structured as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiuSQGGqsKVG"
      },
      "outputs": [],
      "source": [
        "# Preview the dataset\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY2lMPq-sX-I"
      },
      "source": [
        "We will define the following system message to guide the model during fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fs7RuGGqshA0"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "You are a financial risk analyst. Your job is to analyze SHAP values and provide structured, fact-based explanations of the model's predictions.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx8De8x7sj45"
      },
      "source": [
        "We shuffle and format the dataset to align with the chat-based fine-tuning format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89juX1qnsm8S"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "def format_chat_template(row):\n",
        "\n",
        "    row_json = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": row['messages'][1]['content']},\n",
        "       {\"role\": \"assistant\", \"content\": row['messages'][2]['content']+\" <|eot_id|>\"}\n",
        "    ]\n",
        "\n",
        "    #row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
        "    row[\"text\"] = tokenizer.apply_chat_template(conversation=row_json, tokenize=False)\n",
        "\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    num_proc= 4,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGmfYuk8spSw"
      },
      "source": [
        "The following identifies all linear layers in the model that are quantized using BitsAndBytes (bnb) 4-bit precision and returns a list of their names. These module names are typically used when applying LoRA (Low-Rank Adaptation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj2AyxUHstBE"
      },
      "outputs": [],
      "source": [
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n",
        "modules = find_all_linear_names(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-hWbF1svg7"
      },
      "source": [
        "Now, we configure and apply LoRA to the model, optimizing it for efficient fine-tuning with reduced memory usage.\n",
        "\n",
        "* `r=16`: Rank of LoRA adaptation (controls memory usage vs. expressiveness).\n",
        "* `lora_alpha=32`: Scaling factor for LoRA layers.\n",
        "* `lora_dropout=0.05`: Dropout rate to prevent overfitting.\n",
        "* `bias=\"none\"`: Ensures no extra biases are added to the model.\n",
        "* `target_modules=modules`: LoRA is applied only to specific layers identified earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v_4vxnRs0fw"
      },
      "outputs": [],
      "source": [
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=modules,\n",
        ")\n",
        "\n",
        "# Reset chat template before reapplying it\n",
        "tokenizer.chat_template = None\n",
        "\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK75o7tfs4El"
      },
      "source": [
        "Now, let's defines the training hyperparameters for fine-tuning the model using the Hugging Face TrainingArguments class.\n",
        "\n",
        "\n",
        "* `Batch Sizes`: Processes one sample per GPU for both training and evaluation.\n",
        "* `Gradient Accumulation`: Accumulates gradients over one step before updating weights to reduce memory usage.\n",
        "* `Optimizer`: Uses Paged AdamW 32-bit, optimized for memory efficiency.\n",
        "* `Evaluation Strategy`: Evaluates every 20% of an epoch.\n",
        "* `Learning Rate & Warmup`: Starts with a 0.0002 learning rate and warms up for 10 steps.\n",
        "* `Precision`: Uses bfloat16 (BF16) for faster training and reduced memory consumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw8J8hRps3R1"
      },
      "outputs": [],
      "source": [
        "#Hyperparamter tuning. Batch size of 2 fits in 21.4 GB of VRAM. Increase if you have more.\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=new_model,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    num_train_epochs=1,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    logging_steps=1,\n",
        "    warmup_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    report_to=\"wandb\",\n",
        "    seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukbP6FUStCur"
      },
      "source": [
        "We initializes the SFTTrainer for supervised fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTt3fuwHtFhY"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Setting sft parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    args=training_arguments,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxZUc4f8tH6d"
      },
      "source": [
        "Everything is set up! Let's begin training! It takes approximately 7 minutes on an A100 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os2uCoUOtLDL"
      },
      "outputs": [],
      "source": [
        "# takes around 10 min with L4\n",
        "set_seed(42)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2odNJEV1tOHq"
      },
      "source": [
        "After training, let's properly close the Weights & Biases (wandb) logging session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA5XdH-dtQcg"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW-uOvN4tSjz"
      },
      "source": [
        "Now, let's test our fine-tuned model using the same dataset from before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqe_41fKtWrM"
      },
      "outputs": [],
      "source": [
        "!gdown --fuzzy 'https://drive.google.com/file/d/1nrhxfnAkI0bZRXJiWu_JVKusAD9iHBpK/view?usp=sharing'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOg__58Lt5Nz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('loan_app.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFnGTgpbt8uf"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=[\"target\"])  # Features\n",
        "y = df[\"target\"]  # Target variable\n",
        "\n",
        "# Convert Categorical Features to Numerical\n",
        "categorical_columns = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "numerical_columns = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "X_encoded = encoder.fit_transform(X[categorical_columns])\n",
        "\n",
        "# Convert Encoded Data to DataFrame\n",
        "X_encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Drop original categorical columns and merge one-hot encoded features\n",
        "X = X.drop(columns=categorical_columns)\n",
        "X = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "# Train-Test Split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale Only Numerical Features (NOT one-hot encoded features)\n",
        "scaler = StandardScaler()\n",
        "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
        "X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])  # Use same scaler for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5uc7J3PuGRj"
      },
      "outputs": [],
      "source": [
        "negative_count = np.sum(y_train == 0)  # Count of class 0\n",
        "positive_count = np.sum(y_train == 1)  # Count of class 1\n",
        "scale_pos_weight = negative_count / positive_count  # Weight ratio\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_model = XGBClassifier(max_depth=3,\n",
        "                          learning_rate=0.01,\n",
        "                          n_estimators=200,\n",
        "                          verbosity=0,\n",
        "                          objective='binary:logistic',\n",
        "                          eval_metric=\"logloss\",\n",
        "                          booster='gbtree',\n",
        "                          n_jobs=-1,\n",
        "                          gamma=0.001,\n",
        "                          subsample=0.632,\n",
        "                          colsample_bytree=1,\n",
        "                          colsample_bylevel=1,\n",
        "                          colsample_bynode=1,\n",
        "                          reg_alpha=0,\n",
        "                          reg_lambda=0.1,\n",
        "                          random_state=428,\n",
        "                          tree_method=\"hist\",\n",
        "                          scale_pos_weight=scale_pos_weight\n",
        "                          )\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate Model Performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLdGnufWv_-9"
      },
      "outputs": [],
      "source": [
        "# Explain Model Predictions using SHAP\n",
        "explainer = shap.Explainer(xgb_model, X_train)\n",
        "shap_values = explainer(X_test)  # SHAP values for test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjnIwpVetaKp"
      },
      "source": [
        "We will use the same test sample as before to enable a direct comparison of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmlMXgDYwDQV"
      },
      "outputs": [],
      "source": [
        "# Select an example loan case (e.g., first sample in test set)\n",
        "sample_index = 10  # Change this index if needed\n",
        "shap_values_sample = shap_values[sample_index].values\n",
        "sample_features = X_test.iloc[sample_index]\n",
        "\n",
        "# Convert SHAP values into dictionary format for input\n",
        "shap_dict = {feature: shap_values_sample[i] for i, feature in enumerate(sample_features.index)}\n",
        "\n",
        "# Target label for the selected sample\n",
        "pred_label = y_pred[sample_index]\n",
        "\n",
        "print(\"\\nSHAP Values for Sample Client:\\n\", shap_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxsxVP4NwHtq"
      },
      "outputs": [],
      "source": [
        "shap.plots.bar(shap_values[sample_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnADgc8twL-Q"
      },
      "outputs": [],
      "source": [
        "def format_shap_explanation(system_message, sample_features, shap_dict, pred_label, scaler):\n",
        "    # Convert scaled numerical values back to original values\n",
        "    original_values = scaler.inverse_transform(sample_features[numerical_columns].values.reshape(1, -1))\n",
        "    original_feature_values = {feature: original_values[0][i] for i, feature in enumerate(numerical_columns)}\n",
        "\n",
        "    # Sort SHAP values by absolute magnitude (most impactful features first)\n",
        "    top_features = sorted(shap_dict.items(), key=lambda x: abs(x[1]), reverse=True)[:3]\n",
        "\n",
        "    # Generate SHAP explanation text\n",
        "    shap_text = \"\\n\".join(\n",
        "        [f\"{feature}: SHAP value = {shap_value:.4f}, feature value = {original_feature_values[feature]:.2f}\"\n",
        "         for feature, shap_value in top_features]\n",
        "    )\n",
        "\n",
        "    print(shap_text)\n",
        "    # Define loan delinquency status\n",
        "    delinquency_status = \"likely to be delinquent\" if pred_label == 1 else \"unlikely to be delinquent\"\n",
        "    print(\"prediected delinquency: \", delinquency_status)\n",
        "\n",
        "    # Construct a revised prompt with explicit instructions and structured format\n",
        "    user_prompt = f\"\"\"\n",
        "The model predicts that the client is {delinquency_status}.\n",
        "\n",
        "Here are the three most important features influencing the prediction:\n",
        "\n",
        "{shap_text}\n",
        "\n",
        "### Instructions:\n",
        "- Analyze how each of three features contributes to the prediction.\n",
        "- **Use correct feature names, not feature values.**\n",
        "- **Strictly follow the structured response format.**\n",
        "- **SHAP values must be interpreted correctly**:\n",
        "  - **A positive SHAP value means the feature increases delinquency risk.**\n",
        "  - **A negative SHAP value means the feature decreases delinquency risk.**\n",
        "\n",
        "\n",
        "Now begin your structured analysis:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5UqcRTfwMxN"
      },
      "outputs": [],
      "source": [
        "system_message = system_message = \"\"\"\n",
        "You are a financial risk analyst. Your job is to analyze SHAP values and provide structured, fact-based explanations of the model's predictions.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkpSk5SdwSil"
      },
      "source": [
        "Finally, let's test the model and see how much it has improved compared to before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QB_Mk4FwSVW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Exq20llbwP8s"
      },
      "outputs": [],
      "source": [
        "set_seed(48)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    temperature=0.1,\n",
        "    repetition_penalty=1,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "text_raw = tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IBPAM1gwXbG"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "print(\"------------------------Fine-tuned model result---------------------------\")\n",
        "text = text_raw.split(\"assistant\")[1].strip()\n",
        "display(Markdown(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzZlPZG0wajV"
      },
      "source": [
        "Looks great! We can see improvements in both readability and reasoning. You can compare these results with those from the previous lab, which used the original Llama, in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJWOB1WAwXUl"
      },
      "outputs": [],
      "source": [
        "result_from_original_model =  \"\"\"Feature Name: Credit Score Effect on Risk: A higher credit score reduces delinquency risk because it indicates a strong repayment history and financial responsibility. SHAP Impact: The SHAP value 0.6744 shows that including credit score for this client increases the probability of delinquency, meaning the model considers this a moderate indicator of financial reliability.\n",
        "\n",
        "Feature Name: Number of Borrowers Effect on Risk: A higher number of borrowers increases delinquency risk because it may indicate financial strain and reduced ability to repay. SHAP Impact: The SHAP value 0.2054 shows that including number of borrowers for this client increases the probability of delinquency, meaning the model considers this a moderate indicator of financial strain.\n",
        "\n",
        "Feature Name: Original Loan Term Effect on Risk: A longer original loan term increases delinquency risk because it may lead to higher monthly payments and increased financial burden. SHAP Impact: The SHAP value 0.0350 shows that including original loan term for this client increases the probability of delinquency, meaning the model considers this a weak indicator of financial reliability.\"\"\"\n",
        "\n",
        "print(\"------------------------Original model result---------------------------\")\n",
        "display(\n",
        "    Markdown(result_from_original_model))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}