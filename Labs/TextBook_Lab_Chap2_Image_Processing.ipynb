{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Banking-Analytics-Lab/DLinBankingBook/blob/main/Labs/TextBook_Lab_Chap2_Image_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2D Convolutional Neural Networks\n",
        "In this case study, we will train a custom 2D CNN model with Lidar images. We will also study how to use Pytorch and torchvision, the image subpackage from torch.\n",
        "\n",
        "The task is to predict the loan delinquency ratio for a specific geographic area. Let's begin by importing the Python packages needed for this analysis."
      ],
      "metadata": {
        "id": "nC5uXc1L7wqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcam livelossplot"
      ],
      "metadata": {
        "id": "YRbHP9HVEVZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "\n",
        "# Torchvision\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from IPython.display import Image as DisplayImage"
      ],
      "metadata": {
        "id": "J8dc6F0v8C-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Loan and LiDAR Data\n",
        "\n",
        "We are downloading:\n",
        "- **Loan data** (CSV file)\n",
        "- **LiDAR images** (ZIP files) collected based on:\n",
        "  - **3-digit US ZIP codes**\n",
        "  - **Metropolitan Statistical Areas (MSA)**\n",
        "\n",
        "This dataset will be used for further analysis and modeling."
      ],
      "metadata": {
        "id": "TifmMzp2Zgrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy 'https://drive.google.com/file/d/1uDziz14xOgUTmW2DxPufuQMATL6O8Ial/view?usp=sharing'"
      ],
      "metadata": {
        "id": "YWnOM5Gy7hAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy 'https://drive.google.com/file/d/1-YWiPPtLQmkuWjcZcckM9vgv4Fsm68Je/view?usp=sharing'"
      ],
      "metadata": {
        "id": "IkqNB5f3_e06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzipping LiDAR Data\n",
        "All ZIP files will be **automatically extracted** into the `usgs_lidar/` folder.  "
      ],
      "metadata": {
        "id": "ns0qaA-IaA5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip usgs_lidar.zip -d usgs_lidar"
      ],
      "metadata": {
        "id": "j3-wuOb37hf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see an example of LiDAR image"
      ],
      "metadata": {
        "id": "PcBr4O2JaRXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DisplayImage(filename='usgs_lidar/10180_796_USGS_13_n33w101_20191031.jpg')"
      ],
      "metadata": {
        "id": "hN3udS577kQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is one of the layers you can use from LiDAR images. This one represents different densities in the sector, including bodies of water."
      ],
      "metadata": {
        "id": "vjJyOn1E8vBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the Loan Data in CSV format, using Pandas. The data comes from the [Single Family Loan Dataset](https://www.freddiemac.com/research/datasets/sf-loanlevel-dataset), from Freddie Mac."
      ],
      "metadata": {
        "id": "g3Jtth5Ta1Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('sampled_data.csv', dtype=str)\n",
        "df['loan_delinquency'] = df['loan_delinquency'].astype(float)\n",
        "df['LiDAR_File'] = df['LiDAR_File'].astype(str).str.replace(\"\\\\\", \"/\") ## following linux path\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rHHBEVRGmJa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is very simple.\n",
        "\n"
      ],
      "metadata": {
        "id": "dR-y-uU_9P4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading the dataset, we need to split it for **training, validation, and testing**."
      ],
      "metadata": {
        "id": "DCX6sFX3a829"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp, test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train, val = train_test_split(temp, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "uVQdVTOWmV-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's verify torch is running on GPU. The following code gets that and also allocates the device (the GPU) to a variable."
      ],
      "metadata": {
        "id": "DtkZV8-RbjZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "# Storing ID of current CUDA device\n",
        "cuda_id = torch.cuda.current_device()\n",
        "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
        "\n",
        "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n",
        "\n",
        "# Making the code device-agnostic\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"The default device is set to {device}\")"
      ],
      "metadata": {
        "id": "RchzrbgzmdZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing LiDAR Image Data for Deep Learning\n",
        "\n",
        "Before training a neural network, we need to **preprocess the LiDAR images**.  \n",
        "\n",
        "---\n",
        "\n",
        "### Image Transformations  \n",
        "We apply the following preprocessing steps to standardize all images:\n",
        "\n",
        "1. **Resize to (256, 256)** → Ensures all images have a fixed size.  \n",
        "2.  **Convert to Tensor** → Converts images into PyTorch tensors (`C x H x W` format).  \n",
        "3.  **Normalize (Mean=0.5, Std=0.5)** → Centers pixel values around zero for better training stability.  \n",
        "\n",
        "---\n",
        "\n",
        "### Creating a Custom Dataset for LiDAR Images  \n",
        "We define a `LiDARDataset` class to **load** and **process** images from a DataFrame.  \n",
        "Each dataset entry contains:\n",
        "-  **`LiDAR_File`** → The file path of the image.  \n",
        "-  **`loan_delinquency`** → The target variable, representing the loan delinquency raio (a float between 0 and 1).  \n"
      ],
      "metadata": {
        "id": "vyFMTpWObonq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize for CNN\n",
        "    transforms.ToTensor(),          # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize for stable training\n",
        "])\n",
        "\n",
        "\n",
        "class LiDARDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.df.iloc[idx][\"LiDAR_File\"]  # Path to JPG file\n",
        "        target = self.df.iloc[idx][\"loan_delinquency\"]  # Target\n",
        "\n",
        "        #  Load Image using PIL\n",
        "        img_pil = Image.open(file_path).convert(\"L\")  # Convert to grayscale\n",
        "\n",
        "        # Apply Transformations (Resize, ToTensor, Normalize)\n",
        "        img_tensor = transform(img_pil)\n",
        "\n",
        "        return img_tensor, torch.tensor(target, dtype=torch.float)"
      ],
      "metadata": {
        "id": "gACPpGWf5Tgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = LiDARDataset(train)\n",
        "val_dataset = LiDARDataset(val)\n",
        "test_dataset = LiDARDataset(test)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "OfcqMN4V5g1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize a few examples from DataLoader"
      ],
      "metadata": {
        "id": "29KSmtUGcbYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 20\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"\n",
        "    Display a single image. Supports grayscale (1-channel) and RGB (3-channel) images.\n",
        "    \"\"\"\n",
        "    inp = inp.cpu().numpy()  # Convert tensor to NumPy\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    if inp.shape[0] == 1:  # If 1-channel (grayscale)\n",
        "        inp = inp.squeeze(0)  # Remove channel dimension\n",
        "        plt.imshow(inp, cmap=\"gray\")  # Display as grayscale\n",
        "    else:  # If 3-channel (RGB)\n",
        "        inp = inp.transpose((1, 2, 0))  # Convert from CxHxW to HxWxC\n",
        "        plt.imshow(inp)\n",
        "\n",
        "    plt.axis(\"off\")\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def show_databatch(inputs, labels, num_images=4):\n",
        "    \"\"\"\n",
        "    Display a batch of JPG images. Supports grayscale and RGB images.\n",
        "    \"\"\"\n",
        "    inputs, labels = inputs.cpu(), labels.cpu()  # Move to CPU\n",
        "    inputs = inputs[:num_images]  # Limit to num_images\n",
        "    labels = labels[:num_images]\n",
        "\n",
        "    # Use torchvision make_grid but handle grayscale images properly\n",
        "    if inputs.shape[1] == 1:  # If grayscale images\n",
        "        out = torchvision.utils.make_grid(inputs, nrow=4, normalize=True)\n",
        "    else:  # If RGB images\n",
        "        out = torchvision.utils.make_grid(inputs, nrow=4, normalize=True, value_range=(0, 1))\n",
        "\n",
        "    imshow(out, title=[str(label.item()) for label in labels])\n",
        "\n",
        "#  Get a batch of training data\n",
        "inputs, labels = next(iter(train_loader))  # Fetch batch\n",
        "show_databatch(inputs, labels)  # Display images"
      ],
      "metadata": {
        "id": "vJzw3H785jtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Convolutional Neural Network (CNN) for LiDAR Images\n",
        "\n",
        "We define a **Simple CNN model** to predict the loan delinquency ratio from LiDAR images. The model performs regression, outputting a continuous value.\n",
        "\n",
        "---\n",
        "\n",
        "### Model Architecture  \n",
        "\n",
        "This CNN consists of **two main parts**:\n",
        "1. **Feature Extraction (Convolutional Layers)**\n",
        "   - **Detects patterns** in LiDAR images (edges, textures, shapes).\n",
        "   - Uses **Convolution, Batch Normalization, ReLU activation, and Max Pooling**.\n",
        "\n",
        "2. **Regression Head (Fully Connected Layers)**\n",
        "   - **Flattens** extracted features and passes them through **Dense (FC) layers**.\n",
        "   - Uses **Dropout** to prevent overfitting.\n",
        "   - Outputs a single continuous value representing predicted loan delinquency ratio.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "O995a67sdHzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2, input_channels=1):  # 3 for RGB, 1 for Grayscale\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "\n",
        "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(32 * 64 * 64, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 1)\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9T7_fD-z6UZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To monitor the training loop, we'll use the livelossplot package. It is a simple interface to visualize training as it happens."
      ],
      "metadata": {
        "id": "1P3LMkRqcxXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll define the training loop. The following code does EPOCHS rounds of training, showing the ongoing loss. Once per epoch, liveloss is updated."
      ],
      "metadata": {
        "id": "feBn9r6Jc418"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from livelossplot import PlotLosses\n",
        "from livelossplot.outputs import MatplotlibPlot\n",
        "\n",
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Check if images are grayscale or RGB\n",
        "input_channels = 1 if train_dataset[0][0].shape[0] == 1 else 3\n",
        "\n",
        "# Move Model to GPU if Available\n",
        "model = SimpleCNN(num_classes=1, input_channels=input_channels)\n",
        "print(f\"Using device: {device}\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "qNw3EhUq6J6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)  # Lower LR to avoid overfitting"
      ],
      "metadata": {
        "id": "6bBxEWN96maJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training & Validation Loss Convergence\n",
        "\n",
        "The training and validation loss curves **show convergence**, meaning the model is learning effectively.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "XbTZwfPfdtUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with Live Loss Plot\n",
        "liveloss = PlotLosses(outputs=[MatplotlibPlot(figpath =f\"ConvergenceSimple_{str(optimizer.param_groups[0]['lr'])}.pdf\")])\n",
        "EPOCHS = 100\n",
        "best_vloss = float(\"inf\")\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(EPOCHS):\n",
        "    logs = {}\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    logs[\"train_mse\"] = epoch_loss\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()  # Ensure model is in eval mode\n",
        "    running_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs).squeeze(1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    val_loss = running_loss / len(val_loader.dataset)\n",
        "\n",
        "\n",
        "    logs[\"val_mse\"] = val_loss\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {epoch_loss:.4f} \"\n",
        "          f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save Best Model\n",
        "    if val_loss < best_vloss:\n",
        "        best_vloss = val_loss\n",
        "        model_path = f\"best_simple_cnn.pth\"\n",
        "        print(f\"New best model saved as {model_path}\")\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # Update Live Loss Plot\n",
        "    liveloss.update(logs)\n",
        "    liveloss.send()\n",
        "\n",
        "print(\"Training Complete. Best Model Saved.\")"
      ],
      "metadata": {
        "id": "uZfMxsmv62uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Evaluating Model Performance on the Test Set\n",
        "\n",
        "Now that training is complete, we **evaluate the model on the test set** to measure its performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Disables Gradient Computation (`torch.no_grad()`)\n",
        "- Prevents unnecessary computations, saving memory during inference.\n",
        "- **No gradient updates** are needed since we're only making predictions.\n",
        "\n",
        "### 2. Sets Model to Evaluation Mode (`model.eval()`)\n",
        "- Disables dropout and batch normalization updates.\n",
        "- Ensures the model behaves consistently during inference.\n",
        "\n",
        "### 3. Runs Predictions on the Test Set\n",
        "- Loops through **test batches** and:\n",
        "  - **Moves images & targets to the correct device** (`CPU` or `GPU`).\n",
        "  - **Feeds images into the model** to get predictions (`outputs`).\n",
        "  - **Stores predictions and true targets** for later analysis.\n",
        "  - **Computes test RMSE**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-jvVj9RgefPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ogNdycW_mIRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/DL in Banking Book/DeepLearningInBankingBook/TextBook_Lab/c2_best_simple_cnn_1e-5_top.pth\""
      ],
      "metadata": {
        "id": "urUFgj4amNAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Wrapper to save memory by not recomputing gradients.\n",
        "with torch.no_grad():\n",
        "    # Set the model in evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    # Calculate running loss and accuracy\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    test_labels = np.array([])\n",
        "    test_probs = np.array([])\n",
        "    test_predictions = np.array([])\n",
        "\n",
        "    # Apply to the test set\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs, labels = data\n",
        "        test_labels = np.append(test_labels, labels.cpu().numpy())\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs).squeeze(1)\n",
        "        outputs = torch.clamp(outputs, min=0.0, max=1.0)\n",
        "        outputs = outputs.to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_predictions = np.append(test_predictions, outputs.cpu().numpy())\n",
        "\n",
        "        running_loss += loss.detach() * inputs.size(0)\n",
        "\n",
        "rmse = mean_squared_error(test_labels, test_predictions)\n",
        "\n",
        "print(f\"Test RMSE: {rmse ** 0.5:.4f}\")"
      ],
      "metadata": {
        "id": "Avo3Sv4tOpzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Learning\n",
        "As a final example. We will visualize the learning, to detect exactly what is happening.\n",
        "\n",
        "We will use SmoothCAM, a method that allows visualizing how one image activates the neural network by tracing the gradient activations. There are several Pytorch implementations and non of them is dominant. torch-cam comes with both activation-level and gradient-level models, so it serves our purposes. Let's install the paper."
      ],
      "metadata": {
        "id": "QeuGSUi8fSGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Model Decisions with Grad-CAM\n",
        "\n",
        "We use **Grad-CAM (Gradient-weighted Class Activation Mapping)** to highlight the **most important regions** in LiDAR images that influence the model’s prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Sets Up Grad-CAM**\n",
        "- Uses **`SmoothGradCAMpp`** to generate high-quality activation maps.\n",
        "- Targets **the last convolutional layer** (`model.conv_layers[4]`). In general, you want Grad-CAM to target higher abstractions.\n",
        "\n",
        "### **2. Selects a Random Test Image**\n",
        "- Picks a sample from the **test dataset**.\n",
        "- Converts it into the correct format for Grad-CAM.\n",
        "\n",
        "### **3. Runs Model Inference**\n",
        "- Performs a **forward pass** to get predictions.\n",
        "- Extracts **Grad-CAM activations** based on the most confident prediction.\n",
        "\n",
        "### **4. Generates and Enhances the Heatmap**\n",
        "- **Normalizes the activation map** for better visibility.\n",
        "- Uses **`cv2.COLORMAP_TURBO`** for a **high-contrast, striking heatmap**.\n",
        "- Overlays the heatmap on the original image with **adjustable opacity (`alpha=0.7`)**.\n",
        "\n",
        "### **5. Displays the Final Grad-CAM Visualization**\n",
        "- The resulting image shows **where the model is focusing** when making a prediction.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "lmMbLpxcf2Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from torchcam.methods import SmoothGradCAMpp\n",
        "from torchvision.io.image import read_image\n",
        "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
        "from torchcam.utils import overlay_mask\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "# torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Use the first convolutional layer for Grad-CAM\n",
        "target_layer = model.conv_layers[4]  # ✅ Last convolutional layer\n",
        "cam_extractor = SmoothGradCAMpp(model, target_layer=target_layer)\n",
        "\n",
        "# Select a random test image for Grad-CAM\n",
        "sample_idx = random.randint(0, len(test_dataset) - 1)\n",
        "sample_image, sample_label = test_dataset[sample_idx]\n",
        "\n",
        "# Ensure image is in correct shape (add batch dimension)\n",
        "sample_image = sample_image.unsqueeze(0).to(device)\n",
        "\n",
        "# Forward pass to get model predictions\n",
        "output = model(sample_image)  # Raw logits\n",
        "\n",
        "# Compute Grad-CAM activation map (Fix: Use dict format for targets)\n",
        "activation_map = cam_extractor([0], output)\n",
        "\n",
        "# Normalize activation map\n",
        "# Convert heatmap to NumPy and normalize\n",
        "heatmap = activation_map[0].squeeze().cpu().detach().numpy()\n",
        "heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())  # Normalize [0,1]\n",
        "\n",
        "# Apply a more striking colormap (TURBO or INFERNO)\n",
        "heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_TURBO)\n",
        "\n",
        "# Convert to PIL Image\n",
        "original_img = to_pil_image(sample_image.squeeze().cpu().detach())  # Convert test image to PIL\n",
        "original_img_np = np.array(original_img.convert(\"RGB\"))  # Ensure it's RGB\n",
        "\n",
        "# Resize heatmap to match the original image dimensions\n",
        "heatmap_colored = cv2.resize(heatmap_colored, (original_img_np.shape[1], original_img_np.shape[0]))\n",
        "\n",
        "# Blend Grad-CAM heatmap with the original image (increase `alpha` for emphasis)\n",
        "alpha = 0.7  # Opacity of heatmap\n",
        "overlay = cv2.addWeighted(original_img_np, 1 - alpha, heatmap_colored, alpha, 0)\n",
        "\n",
        "# Display the final result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(overlay)\n",
        "#plt.title(f\"Enhanced Grad-CAM (Last Conv Layer) for Loan Delinquency Ratio Prediction\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig('/content/drive/MyDrive/Colab Notebooks/DL in Banking Book/Images/C2_SmoothGradCam.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b-DBtyYf7HMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting the Grad-CAM Visualization\n",
        "\n",
        "This heatmap highlights **which areas in the LiDAR image** had the most influence on the model's **loan delinquency prediction**.\n",
        "\n",
        "---\n",
        "\n",
        "### What This Image Represents\n",
        "- **Bright/Warmer Regions (Yellow/Green/Red)** → Areas the model focused on **most** when making its prediction.  \n",
        "- **Darker Regions (Purple/Black)** → Areas that had little or no impact on the prediction.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hjfWzlrYgdlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a ResNet-50 for Loan Delinquency Prediction\n",
        "\n",
        "In this section, use a pretrained ResNet-50, a deep convolutional neural network known for its residual connections that help mitigate vanishing gradients in very deep architectures. Originally trained on ImageNet for image classification, ResNet-50 is adapted here for a regression task on LiDAR imagery.\n",
        "\n",
        "---\n",
        "\n",
        " Specifically, we replace the first convolutional layer to accept grayscale (1-channel) input, and modify the final fully connected layer to output a single continuous value representing the predicted loan delinquency ratio.\n",
        "\n"
      ],
      "metadata": {
        "id": "OFvBZUzqtXQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNet50Regressor(nn.Module):\n",
        "    def __init__(self, grayscale=False):\n",
        "        super(ResNet50Regressor, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet-50\n",
        "        self.backbone = models.resnet50(weights='DEFAULT')\n",
        "\n",
        "        # Adapt input layer for grayscale if needed\n",
        "        if grayscale:\n",
        "            self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        # Replace final fully connected layer with regression output\n",
        "        self.backbone.fc = nn.Sequential(\n",
        "            nn.Linear(self.backbone.fc.in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 1)   # Output is a scalar\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n"
      ],
      "metadata": {
        "id": "4-i-xgV_oBlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To leverage pretrained features while allowing task-specific learning, we freeze all model parameters except the first convolutional layer (adapted for grayscale), the final residual block (layer4), and the regression head."
      ],
      "metadata": {
        "id": "3UbtqQ9su941"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_channels = 1 if train_dataset[0][0].shape[0] == 1 else 3\n",
        "use_grayscale = input_channels == 1\n",
        "\n",
        "model = ResNet50Regressor(grayscale=use_grayscale).to(device)\n",
        "\n",
        "# Freeze all\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "# Unfreeze your custom conv1\n",
        "for param in model.backbone.conv1.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze last residual block and head\n",
        "for param in model.backbone.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.backbone.fc.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "P7uOgrkYoN20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the model architecture."
      ],
      "metadata": {
        "id": "fUJ1Jr7zvI63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "4A1Vblhap8wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train the model."
      ],
      "metadata": {
        "id": "v93CWS8ovxFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()  # Or L1Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "P7omc03loP6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the ResNet-50 regression model over 20 epochs using Mean Squared Error (MSE) as the loss function. The training and validation MSE are logged and visualized using LiveLossPlot. The best-performing model (based on validation loss) is saved during training."
      ],
      "metadata": {
        "id": "YkTkmhfdves-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with Live Loss Plot\n",
        "liveloss = PlotLosses()\n",
        "EPOCHS = 150\n",
        "best_vloss = float(\"inf\")\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/DL in Banking Book/DeepLearningInBankingBook/TextBook_Lab/c2_best_ResNet50.pth\"\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(EPOCHS):\n",
        "    logs = {}\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    logs[\"train_mse\"] = epoch_loss\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()  # Ensure model is in eval mode\n",
        "    running_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs).squeeze(1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    val_loss = running_loss / len(val_loader.dataset)\n",
        "\n",
        "\n",
        "    logs[\"val_mse\"] = val_loss\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {epoch_loss:.4f} \"\n",
        "          f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save Best Model\n",
        "    if val_loss < best_vloss:\n",
        "        best_vloss = val_loss\n",
        "        print(f\"New best model saved as {model_path}\")\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # Update Live Loss Plot\n",
        "    liveloss.update(logs)\n",
        "    liveloss.send()\n",
        "\n",
        "print(\"Training Complete. Best Model Saved.\")"
      ],
      "metadata": {
        "id": "9qxqxGVXpPcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, we can evaluated the model on the test set using Root Mean Squared Error (RMSE) as the performance metric."
      ],
      "metadata": {
        "id": "4dNRpmY8vuEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Wrapper to save memory by not recomputing gradients.\n",
        "with torch.no_grad():\n",
        "    # Set the model in evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    # Calculate running loss and accuracy\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    test_labels = np.array([])\n",
        "    test_probs = np.array([])\n",
        "    test_predictions = np.array([])\n",
        "\n",
        "    # Apply to the test set\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs, labels = data\n",
        "        test_labels = np.append(test_labels, labels.cpu().numpy())\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs).squeeze(1)\n",
        "        outputs = torch.clamp(outputs, min=0.0, max=1.0)\n",
        "        outputs = outputs.to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_predictions = np.append(test_predictions, outputs.cpu().numpy())\n",
        "\n",
        "        running_loss += loss.detach() * inputs.size(0)\n",
        "\n",
        "rmse = mean_squared_error(test_labels, test_predictions)\n",
        "\n",
        "print(f\"Test RMSE: {rmse ** 0.5:.4f}\")"
      ],
      "metadata": {
        "id": "L10A2mtSpWAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the test RMSE of ResNet-50 is higher than that of the Simple CNN. This may be due to ResNet-50's large capacity (~25 million parameters), which makes it prone to overfitting on small or low-variance datasets. Additionally, ResNet-50 is pretrained on ImageNet RGB images, whereas our input consists of grayscale LiDAR data. Although the first convolutional layer was modified to accept single-channel input, the pretrained filters in earlier layers may not transfer effectively to this domain."
      ],
      "metadata": {
        "id": "PJNaT_Hqv80J"
      }
    }
  ]
}