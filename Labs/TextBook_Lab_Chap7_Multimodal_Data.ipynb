{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Banking-Analytics-Lab/DLinBankingBook/blob/main/Labs/TextBook_Lab_Chap7_Multimodal_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7SVPKJMGngm"
      },
      "source": [
        "# Multimodal Model – Combining Time-Series, Static Variables, Images, and Text\n",
        "\n",
        "In this lab, we will explore multimodal models that integrate time-series data, static variables, images, and text, leveraging all the datasets we have used previously.\n",
        "\n",
        "First, we need to install and import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEZ2bVVh20vc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "bookPath = '/content/drive/MyDrive/Colab Notebooks/DL in Banking Book/DeepLearningInBankingBook/TextBook_Lab'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbQk9uXd6Rzb"
      },
      "outputs": [],
      "source": [
        "bookPath = '/content/drive/MyDrive/Colab Notebooks/DL in Banking Book/DeepLearningInBankingBook/TextBook_Lab'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooXxYW4mz0yB"
      },
      "outputs": [],
      "source": [
        "# Install necessasary packages, if not done before\n",
        "!pip install transformers evaluate accelerate hf_xet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8noMvsoz9nO"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# Import Pytorch lybraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "# Huggingface\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers import pipeline\n",
        "from transformers import set_seed\n",
        "from datasets import load_dataset, Dataset, Value, ClassLabel, Features, load_from_disk\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIm_Hc3HHLNp"
      },
      "source": [
        "## Cleaning Time Series data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2_YrOWsHOnQ"
      },
      "source": [
        "Let's start by downloading the time-series dataset from Freddie Mac. This dataset provides monthly loan status updates covering the period from November 2021 to June 2024."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMPoPT88z_9N"
      },
      "outputs": [],
      "source": [
        "!gdown --fuzzy 'https://drive.google.com/file/d/1cte8MKaSdt3LrqXSUo9oGTe2hnkz8JsU/view?usp=sharing'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdaeW87H0JVx"
      },
      "outputs": [],
      "source": [
        "# Read the data\n",
        "df = pd.read_csv('Multimodal_Lab_sampled.csv', low_memory=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeAt3DudHh1R"
      },
      "source": [
        "We assign a value of 1 to loans that were delinquent at least once in the last three months (April to June 2024). Our goal is to predict whether a loan will become delinquent in the next quarter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MgjEI8G0Qwp"
      },
      "outputs": [],
      "source": [
        "# Convert REPORTING_PERIOD to datetime for sorting\n",
        "df[\"REPORTING_PERIOD\"] = pd.to_datetime(df[\"REPORTING_PERIOD\"], format='%Y%m')\n",
        "\n",
        "df[\"CURRENT_LOAN_DELINQUENCY_STATUS\"] = pd.to_numeric(df[\"CURRENT_LOAN_DELINQUENCY_STATUS\"], errors='coerce').fillna(1)\n",
        "\n",
        "# Sort data by Loan Number and Reporting Period (chronological order)\n",
        "df = df.sort_values(by=[\"LOAN_NUMBER\", \"REPORTING_PERIOD\"])\n",
        "\n",
        "\n",
        "# Identify loans that were delinquent at least once in the period 2024-04 to 2024-06\n",
        "df[\"CURRENT_LOAN_DELINQUENCY_STATUS\"] = (df[\"CURRENT_LOAN_DELINQUENCY_STATUS\"] > 0).astype(int)\n",
        "mask = (df[\"REPORTING_PERIOD\"] >= \"2024-04-01\") & (df[\"REPORTING_PERIOD\"] <= \"2024-06-30\")\n",
        "df[\"target\"] = df.groupby(\"LOAN_NUMBER\")[\"CURRENT_LOAN_DELINQUENCY_STATUS\"].transform(lambda x: x[mask].max()).astype(int)\n",
        "\n",
        "\n",
        "# Remove the last three months (April 2024 - June 2024)\n",
        "df = df[~df[\"REPORTING_PERIOD\"].between(pd.Timestamp(\"2024-03-31\"), pd.Timestamp(\"2024-06-02\"))]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0nAKApNIKY-"
      },
      "source": [
        "We normalize the data and One Hot Encode the labels using sklearn's OneHotEncode function. We will also set the class weights as we have done before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LouB_3rf0dXz"
      },
      "outputs": [],
      "source": [
        "num_cols = [\n",
        "    \"CURRENT_ACTUAL_UPB\",\n",
        "    \"LOAN_AGE\",\n",
        "    \"REMAINING_MONTHS\",\n",
        "    \"CURRENT_INTEREST_RATE\",\n",
        "    \"CURRENT_NON_INTEREST_BEARING_UPB\",\n",
        "    \"ESTIMATED_LOAN_TO_VALUE\",\n",
        "    \"INTEREST_BEARING_UPB\",\n",
        "    \"CURRENT_LOAN_DELINQUENCY_STATUS\"\n",
        "]\n",
        "\n",
        "# Group by loan number for sequence creation\n",
        "grouped = df.groupby(\"LOAN_NUMBER\")\n",
        "\n",
        "# Store sequences and targets\n",
        "X_dict = {}\n",
        "y_dict = {}\n",
        "\n",
        "for loan_id, group in grouped:\n",
        "    group = group.sort_values(\"REPORTING_PERIOD\")  # Sort each loan's time series\n",
        "\n",
        "    # Extract features as separate sequences\n",
        "    X_dict[loan_id] = {col: group[col].values for col in num_cols}\n",
        "\n",
        "    # Assign existing target value (no need to recalculate)\n",
        "    y_dict[loan_id] = group[\"target\"].iloc[0]\n",
        "\n",
        "\n",
        "\n",
        "# Perform stratified split\n",
        "# Convert dictionary to DataFrame\n",
        "np.random.seed(42)\n",
        "x_df = pd.DataFrame.from_dict(X_dict, orient=\"index\")\n",
        "\n",
        "# Map target values from y_dict\n",
        "x_df[\"target\"] = x_df.index.map(y_dict)\n",
        "\n",
        "# Perform stratified split\n",
        "train_idx, test_idx = train_test_split(\n",
        "    x_df.index, test_size=0.2, stratify=x_df[\"target\"], random_state=42\n",
        ")\n",
        "\n",
        "# Assign 'if_test' column based on stratified split\n",
        "x_df[\"if_test\"] = 0  # Default to training\n",
        "x_df.loc[test_idx, \"if_test\"] = 1  # Mark test samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48J6inbz0fwJ"
      },
      "outputs": [],
      "source": [
        "# Function to scale each feature separately\n",
        "def scale_dataframe_sequences(x_df, test_var=None):\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Split data into train and test\n",
        "    if test_var is not None:\n",
        "        x_train = x_df.loc[test_var == 0, :]\n",
        "        x_test = x_df.loc[test_var == 1, :]\n",
        "    else:\n",
        "        x_train = x_df\n",
        "\n",
        "    scaled_data = {}\n",
        "    scaled_data_test = {}\n",
        "\n",
        "    # Get sequence length from first row\n",
        "    seq_len = len(next(iter(x_train.iloc[0])))\n",
        "\n",
        "    for column in num_cols:\n",
        "        # Stack time series data for each loan into a 2D array\n",
        "        data = np.stack(x_train[column].values).reshape(-1, seq_len)\n",
        "        if column != \"CURRENT_LOAN_DELINQUENCY_STATUS\":\n",
        "          scaled_data[column] = scaler.fit_transform(data).reshape(-1, 1, seq_len).tolist()\n",
        "        else:\n",
        "          scaled_data[column] = data.reshape(-1, 1, seq_len).tolist()\n",
        "\n",
        "        # Scale test set if provided\n",
        "        if test_var is not None:\n",
        "            data_test = np.stack(x_test[column].values).reshape(-1, seq_len)\n",
        "            if column != \"CURRENT_LOAN_DELINQUENCY_STATUS\":\n",
        "              scaled_data_test[column] = scaler.transform(data_test).reshape(-1, 1, seq_len).tolist()\n",
        "            else:\n",
        "              scaled_data_test[column] = data_test.reshape(-1, 1, seq_len).tolist()\n",
        "\n",
        "    # Create new DataFrames with scaled data\n",
        "    scaled_df = pd.DataFrame(scaled_data)\n",
        "    scaled_df = scaled_df.map(lambda x: np.array(x[0]))\n",
        "\n",
        "    if test_var is not None:\n",
        "        scaled_df_test = pd.DataFrame(scaled_data_test)\n",
        "        scaled_df_test = scaled_df_test.map(lambda x: np.array(x[0]))\n",
        "        return scaled_df, scaled_df_test\n",
        "    else:\n",
        "        return scaled_df, _\n",
        "\n",
        "# Normalize train and test sets\n",
        "x_train_val, x_test = scale_dataframe_sequences(x_df[num_cols], x_df[\"if_test\"])\n",
        "\n",
        "# One-hot encode the target variable\n",
        "enc = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "y_train_val = enc.fit_transform(x_df.loc[x_df[\"if_test\"] == 0, \"target\"].values.reshape(-1, 1))[:, 1]\n",
        "y_test = enc.transform(x_df.loc[x_df[\"if_test\"] == 1, \"target\"].values.reshape(-1, 1))[:, 1]\n",
        "\n",
        "# Compute class weights\n",
        "pos_weight = np.sum(1 - y_train_val) / np.sum(y_train_val)\n",
        "pos_weight = torch.tensor(pos_weight, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM-25SbNtQpH"
      },
      "outputs": [],
      "source": [
        "x_train_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx4c1KjnIFE6"
      },
      "source": [
        "## Cleaning static variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8xSwF39ITWt"
      },
      "source": [
        "Next, we will clean static features such as credit score, property type, and occupancy status, which were recorded at the time of the loan application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQfQ5xznHYaT"
      },
      "outputs": [],
      "source": [
        "!gdown --fuzzy 'https://drive.google.com/file/d/1lF1n0Ue-emaflJabOfmQI6CT0D3qAIYK/view?usp=sharing'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6_C2v1eFf40"
      },
      "outputs": [],
      "source": [
        "# Load filtered static dataset\n",
        "static_df = pd.read_csv(\"Multimodal_Lab_Origin_sampled.csv\", dtype=str)\n",
        "static_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwGmgYJkI0ZK"
      },
      "source": [
        "We normalize numeric variables and one-hot encode categorical variables to ensure consistency in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H2gSmKE0noL"
      },
      "outputs": [],
      "source": [
        "# Identify numeric and categorical columns\n",
        "numeric_cols = [\"CREDIT_SCORE\", \"CLTV\", \"DTI_RATIO\", \"ORIGINAL_UPB\", \"ORIGINAL_LOAN_TERM\", \"MI_PERCENTAGE\"]  # Excluding \"NUMBER_OF_BORROWERS\"\n",
        "categorical_cols = [\"FIRST_TIME_HOMEBUYER\", \"OCCUPANCY_STATUS\", \"PROPERTY_TYPE\"]\n",
        "\n",
        "\n",
        "static_df[\"ZIP3\"] = static_df[\"ZIP3\"].astype(str).str[:-2]\n",
        "static_df[numeric_cols] = static_df[numeric_cols].astype(float)\n",
        "static_df[\"NUMBER_OF_BORROWERS\"] = static_df[\"NUMBER_OF_BORROWERS\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk8EyWAL0wxx"
      },
      "outputs": [],
      "source": [
        "# Add LOAN_NUMBER back before merging\n",
        "x_df[\"LOAN_NUMBER\"] = x_df.index  # Assign index (loan numbers) back to x_df\n",
        "\n",
        "# Assign LOAN_NUMBER to train and test sets\n",
        "loan_numbers_train = x_df.loc[x_df[\"if_test\"] == 0, \"LOAN_NUMBER\"]\n",
        "loan_numbers_test = x_df.loc[x_df[\"if_test\"] == 1, \"LOAN_NUMBER\"]\n",
        "\n",
        "# Split static data into train and test based on LOAN_NUMBER\n",
        "static_train_val = static_df[static_df[\"LOAN_NUMBER\"].isin(loan_numbers_train)]\n",
        "static_test = static_df[static_df[\"LOAN_NUMBER\"].isin(loan_numbers_test)]\n",
        "\n",
        "# --- SCALE NUMERIC FEATURES ---\n",
        "scaler = StandardScaler()\n",
        "static_train_val[numeric_cols] = scaler.fit_transform(static_train_val[numeric_cols])  # Fit on train\n",
        "static_test[numeric_cols] = scaler.transform(static_test[numeric_cols])  # Apply to test\n",
        "\n",
        "# --- ONE-HOT ENCODE CATEGORICAL FEATURES ---\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "encoded_train = encoder.fit_transform(static_train_val[categorical_cols])  # Fit on train\n",
        "encoded_test = encoder.transform(static_test[categorical_cols])  # Apply to test\n",
        "\n",
        "# Convert to DataFrame\n",
        "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(categorical_cols), index=static_train_val.index)\n",
        "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(categorical_cols), index=static_test.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owxo02fnJHIP"
      },
      "source": [
        "We then merge the cleaned static variables with the time-series table to create a unified dataset for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUXcHmWlJI9F"
      },
      "outputs": [],
      "source": [
        "# Preserve LOAN_NUMBER and NUMBER_OF_BORROWERS (without scaling)\n",
        "static_train_final = pd.concat([\n",
        "    static_train_val[[\"LOAN_NUMBER\", \"NUMBER_OF_BORROWERS\",\"MSA\",\"ZIP3\"]],  # Keep unscaled\n",
        "    static_train_val[numeric_cols],  # Already scaled numeric features\n",
        "    encoded_train_df  # One-hot encoded categorical features\n",
        "], axis=1)\n",
        "\n",
        "static_test_final = pd.concat([\n",
        "    static_test[[\"LOAN_NUMBER\", \"NUMBER_OF_BORROWERS\",\"MSA\",\"ZIP3\"]],  # Keep unscaled\n",
        "    static_test[numeric_cols],  # Already scaled numeric features\n",
        "    encoded_test_df  # One-hot encoded categorical features\n",
        "], axis=1)\n",
        "\n",
        "print(f\"Processed Train Static Data Shape: {static_train_final.shape}\")\n",
        "print(f\"Processed Test Static Data Shape: {static_test_final.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KczXkFb6JMeB"
      },
      "outputs": [],
      "source": [
        "# Convert index to DataFrame to allow merging\n",
        "x_train_val = x_train_val.copy()\n",
        "x_test = x_test.copy()\n",
        "\n",
        "# Add LOAN_NUMBER column before merging\n",
        "x_train_val[\"LOAN_NUMBER\"] = loan_numbers_train.values\n",
        "x_test[\"LOAN_NUMBER\"] = loan_numbers_test.values\n",
        "\n",
        "# Merge static data with LSTM sequences\n",
        "x_train_val = x_train_val.merge(static_train_final, on=\"LOAN_NUMBER\", how=\"left\")\n",
        "x_test = x_test.merge(static_test_final, on=\"LOAN_NUMBER\", how=\"left\")\n",
        "\n",
        "# Drop LOAN_NUMBER after merging\n",
        "x_train_val.drop(columns=[\"LOAN_NUMBER\"], inplace=True)\n",
        "x_test.drop(columns=[\"LOAN_NUMBER\"], inplace=True)\n",
        "\n",
        "print(f\"x_train Shape After Merge: {x_train_val.shape}\")\n",
        "print(f\"x_test Shape After Merge: {x_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qitwMLL0JfFt"
      },
      "source": [
        "## Merging image dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZspr8x3Ja60"
      },
      "source": [
        "Next, we will merge the image mapping dataset using MSA (Metropolitan Statistical Area) and ZIP3 (the first three digits of the ZIP code) to align LiDAR images with the corresponding loan records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "469Bo8tOF5Q0"
      },
      "outputs": [],
      "source": [
        "!gdown --fuzzy 'https://drive.google.com/file/d/1cHLNcnTLdaKtedM_Uz-xmYaXXml0Ws-D/view?usp=sharing'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuKbt2dWF8GB"
      },
      "outputs": [],
      "source": [
        "!unzip usgs_lidar.zip -d usgs_lidar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jme-QaAZIMzP"
      },
      "outputs": [],
      "source": [
        "!gdown --fuzzy 'https://drive.google.com/file/d/1QyzNtHVurvrXvHG1j2oeq7rwfC6vnA1h/view?usp=sharing'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcOTi3XOGTIq"
      },
      "outputs": [],
      "source": [
        "image_df = pd.read_csv('msa_zip3_mapping_cleaned.csv', dtype=str)\n",
        "image_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1FdtpyfIIEt"
      },
      "outputs": [],
      "source": [
        "# Merge static data with LSTM sequences\n",
        "x_train_val = x_train_val.merge(image_df, on=[\"MSA\",\"ZIP3\"], how=\"left\")\n",
        "x_test = x_test.merge(image_df, on=[\"MSA\",\"ZIP3\"], how=\"left\")\n",
        "\n",
        "# Drop LOAN_NUMBER after merging\n",
        "x_train_val.drop(columns=[\"MSA\", \"ZIP3\"], inplace=True)\n",
        "x_test.drop(columns=[\"MSA\", \"ZIP3\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewv5YhQ7JxNf"
      },
      "source": [
        "Finally, we have established a comprehensive dataset that integrates time-series data, static variables, and image mapping, providing a unified foundation for our multimodal analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqxvSmd3cb3Z"
      },
      "outputs": [],
      "source": [
        "x_train_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMXV1p_clNIR"
      },
      "source": [
        "##  Get fixed textual data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qELKhCT8KUIa"
      },
      "source": [
        "We will use the most recent Federal Reserve speech from March 2024 as the fixed textual data for our analysis.\n",
        "\n",
        "**Scenario**:\n",
        "\n",
        "You are a data scientist at a bank, analyzing loan risk. After watching the Federal Reserve speech from March 2024, you aim to leverage multiple datasets—including time-series data from the loan dataset, static features from loan applications, LiDAR images, and the FED speech—to predict whether a loan will become delinquent in the next quarter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcf9UzgSfs9j"
      },
      "outputs": [],
      "source": [
        "!gdown --fuzzy 'https://drive.google.com/file/d/1uVt9BC2tgr-MWrFZvYvA_I8IzTabNZtL/view?usp=sharing'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8Ll-LLukH3w"
      },
      "outputs": [],
      "source": [
        "fed_speech = pd.read_csv(\"/content/fed_speeches.csv\", delimiter=\",\", on_bad_lines=\"skip\", engine=\"python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORNWnIFZkKt5"
      },
      "outputs": [],
      "source": [
        "# Create year and month columns\n",
        "fed_speech['date'] = pd.to_datetime(fed_speech['date'], errors='coerce')\n",
        "fed_speech[\"year\"] = fed_speech[\"date\"].dt.year  # Extract year\n",
        "fed_speech[\"month\"] = fed_speech[\"date\"].dt.month  # Extract month\n",
        "fed_speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqQ9fLnhkObx"
      },
      "outputs": [],
      "source": [
        "fed_speech[(fed_speech.year == 2024) & (fed_speech.month == 3)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvWqQoYmqbMg"
      },
      "outputs": [],
      "source": [
        "## taking the most recent speech\n",
        "fixed_fed_speech = fed_speech[(fed_speech.year == 2024) & (fed_speech.month == 3)][\"text\"].values[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5QdEtqFLf9p"
      },
      "source": [
        "In this step, we preprocess the speech text by **tokenizing, removing stopwords, and eliminating punctuation** to prepare the data for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ULH2L5YqmBp"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "    table = str.maketrans('', '', string.punctuation)  # Create a table for removing punctuation\n",
        "    filtered_tokens = [\n",
        "        token.translate(table) for token in tokens\n",
        "        if token.isalnum() and token not in stop_words  # Remove stop words here!\n",
        "    ]\n",
        "    cleaned_text = ' '.join(filtered_tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "fixed_fed_speech = clean_text(fixed_fed_speech)\n",
        "fixed_fed_speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QIp3-11L4Md"
      },
      "source": [
        "## Preparing Dataloader for multimodal model\n",
        "\n",
        "We create a **`Multimodal Dataset Class`**, integrating time-series data, static features, LiDAR images, and textual data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBrdvVq3kZWx"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "fixed_tokens = tokenizer(fixed_fed_speech, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "fixed_tokens = {key: val.squeeze(0) for key, val in fixed_tokens.items()}  # Store as dict\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, dataframe, y_values):\n",
        "        \"\"\"\n",
        "        dataframe  : DataFrame containing time series, static variables, and LiDAR file paths\n",
        "        fixed_text : Single fixed text string (e.g., FED speech that applies to all)\n",
        "        tokenizer  : Transformer tokenizer for text processing\n",
        "        max_length : Max token length for text processing\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe.reset_index(drop=True)\n",
        "        self.y_values = torch.tensor(y_values, dtype=torch.float32)  # Ensure target is float32\n",
        "\n",
        "\n",
        "        # Define image transformation\n",
        "        self.normalize_transform = transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "\n",
        "\n",
        "        # Identify column indices for time-series and static variables\n",
        "        self.time_series_start = 0\n",
        "        self.time_series_end = 8\n",
        "        self.static_start = 8\n",
        "        self.static_end = -1\n",
        "\n",
        "\n",
        "        # Define image loading. We use CV2 for faster loading\n",
        "    def load_image(self, image_path):\n",
        "        \"\"\"Load and preprocess image using OpenCV for speed.\"\"\"\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
        "        image = cv2.resize(image, (224, 224))  # Resize\n",
        "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0) / 255.0  # Normalize 0-1\n",
        "        return self.normalize_transform(image)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "\n",
        "        #  Ensure time-series to tensor\n",
        "        time_series = torch.tensor(\n",
        "            np.array(row.iloc[self.time_series_start:self.time_series_end].tolist()),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "\n",
        "        # Static variable to tensor\n",
        "        static_features = torch.tensor(\n",
        "            np.array(row.iloc[self.static_start:self.static_end].astype(float).values.tolist()),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # Load LiDAR image (grayscale)\n",
        "        image = self.load_image(row[\"LiDAR_File\"])\n",
        "\n",
        "        # Expand fixed text tokens to match batch size\n",
        "        batch_text_tokens = fixed_tokens\n",
        "\n",
        "        # Retrieve target\n",
        "        target = self.y_values[idx]\n",
        "\n",
        "        # Return dictionary (use precomputed text for all samples)\n",
        "        return {\n",
        "            \"time_series\": time_series,\n",
        "            \"static_features\": static_features,\n",
        "            \"image_data\": image,\n",
        "            \"text_data\": batch_text_tokens,\n",
        "            \"target\": target\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F1nM9F2M1Zj"
      },
      "source": [
        "This code splits the dataset, creates PyTorch dataset instances, and prepares data loaders for training, validation, and testing in a multimodal deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpBE7Q2lfa88"
      },
      "outputs": [],
      "source": [
        "def worker_init_fn(worker_id):\n",
        "    \"\"\"Ensure each worker has a different seed based on global seed.\"\"\"\n",
        "    seed = SEED + worker_id\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdo0oILXoXkz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train-val split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.33, random_state=42, stratify=y_train_val)\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Create dataset instances (pass y_train & y_test separately)\n",
        "train_dataset = MultimodalDataset(x_train, y_train)\n",
        "val_dataset = MultimodalDataset(x_val, y_val)\n",
        "test_dataset = MultimodalDataset(x_test, y_test)\n",
        "\n",
        "# Create PyTorch DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2,\n",
        "                          worker_init_fn=worker_init_fn, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Check batch structure\n",
        "batch = next(iter(train_loader))\n",
        "print(f\"Time-series data shape: {batch['time_series'].shape}\")  # (batch_size, num_features, seq_length)\n",
        "print(f\"Static feature shape: {batch['static_features'].shape}\")  # (batch_size, num_static_features)\n",
        "print(f\"Text input IDs shape: {batch['text_data']['input_ids'].shape}\")  # (batch_size, max_length)\n",
        "print(f\"Image data shape: {batch['image_data'].shape}\")  # (batch_size, 1, 256, 256)\n",
        "print(f\"Target shape: {batch['target'].shape}\")  # (batch_size,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcTh5nr3M8S_"
      },
      "source": [
        "## Multimodal deep learning model - intermediate fusion\n",
        "\n",
        "In this section, we will build a multimodal deep learning model that integrates four distinct data modalities:\n",
        "\n",
        "\n",
        "*   Time Series Data: Loan performance history over time with Transformer Encoder.\n",
        "*   Static Features: Borrower information, property details, and loan\n",
        "*   LiDAR Images: LiDAR images processed using a CNN.\n",
        "*   Textual Data: Federal Reserve speeches analyzed with DistilBERT.\n",
        "\n",
        "\n",
        "To predict whether a loan will become delinquent in the following quarter, we will utilize a combination of Transformers for time-series data, CNNs for image processing, and a fusion layer to integrate information from all modalities effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4htAs97vSXl"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "### Positional Encoding Module\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + 0.1 * self.pe[:, :x.size(1), :]\n",
        "\n",
        "### Time-Series Transformer Encoder with Positional Encoding\n",
        "class TimeSeriesModel(nn.Module):\n",
        "    def __init__(self, num_time_series_features, seq_length, dropout=0.1):\n",
        "        super(TimeSeriesModel, self).__init__()\n",
        "\n",
        "        self.proj = nn.Linear(num_time_series_features, 64)\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim=64, max_len=seq_length)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=128, dropout=dropout, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(seq_length * 64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.proj(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.transformer(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "### CNN Model for LiDAR Images (BatchNorm + Dropout)\n",
        "class LiDARCNN(nn.Module):\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super(LiDARCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 56 * 56, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "### Multimodal Model\n",
        "class MultimodalDelinquencyModel(nn.Module):\n",
        "    def __init__(self, num_time_series_features, seq_length, num_static_features, dropout=0.3):\n",
        "        super(MultimodalDelinquencyModel, self).__init__()\n",
        "\n",
        "        ### Time-Series Component (Transformer-Based with Positional Encoding)\n",
        "        self.time_series_model = TimeSeriesModel(num_time_series_features, seq_length, dropout)\n",
        "\n",
        "        ### Static Features Component (MLP)\n",
        "        self.static_fc = nn.Sequential(\n",
        "            nn.Linear(num_static_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        ### Text Component (DistilBERT)\n",
        "        self.text_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.text_fc = nn.Sequential(\n",
        "            nn.Linear(self.text_model.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        ### LiDAR Image Component (CNN)\n",
        "        self.image_model = LiDARCNN(dropout)\n",
        "\n",
        "        ### Fusion Layer\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(128 + 128 + 128 + 128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, time_series, static_features, text_inputs, image_data):\n",
        "        ### Time-Series Processing\n",
        "        time_series_out = self.time_series_model(time_series)\n",
        "\n",
        "        ### Static Feature Processing\n",
        "        static_out = self.static_fc(static_features)\n",
        "\n",
        "        ### Text Processing (DistilBERT)\n",
        "        text_out = self.text_model(**text_inputs).last_hidden_state[:, 0, :]\n",
        "        text_out = self.text_fc(text_out)\n",
        "\n",
        "        ### LiDAR Image Processing (CNN)\n",
        "        image_out = self.image_model(image_data)\n",
        "\n",
        "        ### Fusion\n",
        "        fused = torch.cat([time_series_out, static_out, text_out, image_out], dim=1)\n",
        "        output = self.fusion_layer(fused)\n",
        "\n",
        "        return output  # Raw logits (for BCEWithLogitsLoss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHxys2JzN8Jx"
      },
      "source": [
        "Now, let's begin training our multimodal deep learning model! Given its complexity, training may take some time.  \n",
        "\n",
        "To improve efficiency, we are leveraging **mixed precision training** with `torch.amp.GradScaler`, which accelerates computations while reducing memory consumption.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZc3R23exNkm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "num_time_series_features = 8\n",
        "seq_length = len(x_train.iloc[0, 0])  # Adjust based on your time-series length\n",
        "num_static_features = len(x_train.iloc[0, 8:-1])  # Adjust based on your dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultimodalDelinquencyModel(num_time_series_features, seq_length, num_static_features, dropout=0.3).to(device)\n",
        "\n",
        "# Loss function & optimizer\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)  # Using BCEWithLogitsLoss\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Mixed Precision Training (AMP)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "# Best validation loss tracking\n",
        "best_val_loss = float(\"inf\")\n",
        "checkpoint_path = \"best_multimodal_model.pth\"\n",
        "\n",
        "\n",
        "### Training Function with Validation\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    global best_val_loss\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_train_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        ### Training Loop\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move data to GPU\n",
        "            time_series = batch[\"time_series\"].to(device)\n",
        "            static_features = batch[\"static_features\"].to(device)\n",
        "            image_data = batch[\"image_data\"].to(device)\n",
        "            text_inputs = {key: val.to(device) for key, val in batch[\"text_data\"].items()}\n",
        "            targets = batch[\"target\"].to(device)\n",
        "\n",
        "            with torch.amp.autocast('cuda'):  # Mixed Precision Training\n",
        "                outputs = model(time_series, static_features, text_inputs, image_data).squeeze()\n",
        "                loss = criterion(outputs, targets.float())\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_train_loss += loss.detach()  # Reduce unnecessary GPU sync\n",
        "\n",
        "            # Convert logits to probabilities for accuracy calculation\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            predictions = (probs > 0.5).float()\n",
        "            total_correct += (predictions == targets).sum()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "        train_acc = 100 * total_correct.cpu().item() / total_samples\n",
        "        avg_train_loss = total_train_loss.item() / len(train_loader)\n",
        "\n",
        "        ### Validation Loop\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                time_series = batch[\"time_series\"].to(device)\n",
        "                static_features = batch[\"static_features\"].to(device)\n",
        "                image_data = batch[\"image_data\"].to(device)\n",
        "                text_inputs = {key: val.to(device) for key, val in batch[\"text_data\"].items()}\n",
        "                targets = batch[\"target\"].to(device)\n",
        "\n",
        "                outputs = model(time_series, static_features, text_inputs, image_data).squeeze()\n",
        "                loss = criterion(outputs, targets.float())\n",
        "                total_val_loss += loss.detach()\n",
        "\n",
        "                # Convert logits to probabilities for accuracy calculation\n",
        "                probs = torch.sigmoid(outputs)\n",
        "                predictions = (probs > 0.5).float()\n",
        "                correct += (predictions == targets).sum()\n",
        "                total += targets.size(0)\n",
        "\n",
        "        val_acc = 100 * correct.cpu().item() / total\n",
        "        avg_val_loss = total_val_loss.item() / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        ### Save best model checkpoint\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Best model saved at epoch {epoch+1} with val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    print(\"Training Complete!\")\n",
        "\n",
        "### One epoch takes 1.5 min with A100\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIocY_7ys_-Q"
      },
      "source": [
        "Training takes approximately **15 minutes**. However, we observe that further training may be beneficial, as the validation loss continues to decrease.\n",
        "\n",
        "\n",
        "Now, let's proceed with evaluating the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5B6zysX_jtp"
      },
      "outputs": [],
      "source": [
        "### Model Evaluation Function\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model and plots the confusion matrix (normalized) and ROC curve.\n",
        "\n",
        "    Args:\n",
        "        model: Trained PyTorch model.\n",
        "        test_loader: DataLoader with test samples.\n",
        "        device: Device (cuda or cpu).\n",
        "\n",
        "    Returns:\n",
        "        all_labels, all_preds, all_probs\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            # Move data to device\n",
        "            time_series = batch[\"time_series\"].to(device)\n",
        "            static_features = batch[\"static_features\"].to(device)\n",
        "            text_inputs = {key: val.to(device) for key, val in batch[\"text_data\"].items()}\n",
        "            image_data = batch[\"image_data\"].to(device)\n",
        "            targets = batch[\"target\"].to(device)\n",
        "\n",
        "            # Forward pass (raw logits)\n",
        "            logits = model(time_series, static_features, text_inputs, image_data).squeeze()\n",
        "\n",
        "            # Convert logits to probabilities using Sigmoid\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "            # Store results\n",
        "            all_probs.extend(probs)\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    # Compute binary predictions based on threshold 0.5\n",
        "    all_preds = (all_probs >= 0.5).astype(int)\n",
        "\n",
        "    return all_labels, all_preds, all_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc_ji5lk-h2e"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
        "print(\"Loaded best model for evaluation!\")\n",
        "\n",
        "# Evaluate on test data\n",
        "all_labels, all_preds, all_probs = evaluate_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGFUlvcvtLm4"
      },
      "source": [
        "Our model performs pretty well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnvoLGmT_E_O"
      },
      "outputs": [],
      "source": [
        "### Confusion Matrix (Normalized)\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "cm_rates = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_rates, annot=True, fmt=\".2f\", cmap=sns.light_palette(\"seagreen\", as_cmap=True),\n",
        "                xticklabels=[\"No Delinquency\", \"Delinquent\"],\n",
        "                yticklabels=[\"No Delinquency\", \"Delinquent\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Normalized Confusion Matrix (Rates)\")\n",
        "plt.savefig(os.path.join(bookPath, \"multimodal_confusion_matrix.pdf\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlRaZVYz_Nnr"
      },
      "outputs": [],
      "source": [
        "### ROC Curve & AUC Score\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "auc_score = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_score:.3f})\", linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Diagonal reference line\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ROC Curve - Delinquency Prediction\")\n",
        "plt.legend(loc=4)\n",
        "plt.grid()\n",
        "plt.savefig(os.path.join(bookPath, \"multimodal_roc_curves.pdf\"))\n",
        "plt.show()\n",
        "\n",
        "### Classification Report\n",
        "print(\"Classification Report:\\n\", classification_report(all_labels, all_preds, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AEsZ432WnmY"
      },
      "source": [
        "## Multimodal model with cross-attention\n",
        "\n",
        "In this section, we implement a multimodal deep learning model that integrates information from multiple data sources using cross-attention mechanisms. Unlike traditional models that process different modalities independently, cross-attention allows the model to dynamically learn relationships between feature representations, improving predictive performance.\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "Our model processes four distinct modalities:\n",
        "\n",
        "*  Time-Series Data: Loan performance over time, modeled using a Transformer with positional encoding.\n",
        "*  Static Features: Borrower and loan characteristics, processed through a Multi-Layer Perceptron (MLP).\n",
        "*  Text Data: Federal Reserve speeches, encoded using DistilBERT.\n",
        "*  LiDAR Images: Spatial features extracted via a CNN.\n",
        "\n",
        "\n",
        "To enhance feature interactions, cross-attention is applied at multiple levels:\n",
        "\n",
        "*  Text ↔ Time-Series: Captures relationships between Federal Reserve statements and historical loan performance.\n",
        "*  Static Features ↔ LiDAR Images: Links borrower characteristics with geographic-based insights.\n",
        "*  Time-Series ↔ Static Features: Models how borrower attributes influence loan behavior over time.\n",
        "\n",
        "### Fusion Strategy\n",
        "\n",
        "After applying cross-attention, the model undergoes two levels of feature refinement:\n",
        "\n",
        "*  Second-Level Cross-Attention: Further interactions between attended representations from different modalities.\n",
        "*  Weighted Fusion Mechanism: A learnable attention-based fusion layer assigns adaptive importance to each attended feature representation before making a final prediction.\n",
        "*  The fused representation is then passed through a fully connected prediction layer to estimate loan delinquency status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FevJ78D6z7D"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertModel\n",
        "import math\n",
        "\n",
        "### Cross-Attention Module\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n",
        "        self.gate = nn.Linear(embed_dim * 2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.scale = nn.Parameter(torch.ones(1))  # Learnable scaling factor\n",
        "        self.norm = nn.LayerNorm(embed_dim)  # Layer normalization\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Cross-Attention between two modalities.\n",
        "        Args:\n",
        "            x1 (tensor): Query features (batch, feature_dim).\n",
        "            x2 (tensor): Key and Value features (batch, feature_dim).\n",
        "        Returns:\n",
        "            tensor: Attended feature representation.\n",
        "        \"\"\"\n",
        "        gate_value = self.sigmoid(self.gate(torch.cat([x1, x2], dim=-1)))  # Compute gate\n",
        "        attn_output, _ = self.attention(x1.unsqueeze(0), x2.unsqueeze(0), x2.unsqueeze(0))\n",
        "        return self.norm(self.scale * (gate_value * attn_output.squeeze(0) + (1 - gate_value) * x1))\n",
        "\n",
        "\n",
        "### Positional Encoding Module\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a positional encoding matrix (max_len, embed_dim)\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Compute sine and cosine functions for even/odd indices\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Sin for even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Cos for odd indices\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "        self.register_buffer('pe', pe)  # Store as a buffer (not trainable)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (tensor): Shape (batch, seq_len, embed_dim)\n",
        "        Returns:\n",
        "            tensor: Positionally encoded input\n",
        "        \"\"\"\n",
        "        return x + 0.1 * self.pe[:, :x.size(1), :]\n",
        "\n",
        "\n",
        "### Time-Series Transformer Encoder with Positional Encoding\n",
        "class TimeSeriesModel(nn.Module):\n",
        "    def __init__(self, num_time_series_features, seq_length, dropout=0.1):\n",
        "        super(TimeSeriesModel, self).__init__()\n",
        "\n",
        "        # Projection Layer (Linear transformation for input features)\n",
        "        self.proj = nn.Linear(num_time_series_features, 64)\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim=64, max_len=seq_length)\n",
        "\n",
        "        # Transformer Encoder Layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=128, dropout=dropout, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        # Fully Connected Output Layer\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(seq_length * 64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch, seq_len, num_features)\n",
        "        x = self.proj(x)  # Project input features to embedding space\n",
        "        x = self.positional_encoding(x)  # Add positional encoding\n",
        "        x = self.transformer(x)  # Pass through Transformer Encoder\n",
        "        x = x.reshape(x.shape[0], -1)  # Flatten for fully connected layer\n",
        "        return self.fc(x)  # Fully Connected Output\n",
        "\n",
        "### Static Feature Model with Dropout\n",
        "class StaticModel(nn.Module):\n",
        "    def __init__(self, num_static_features, dropout=0.3):\n",
        "        super(StaticModel, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(num_static_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "### Text Model (DistilBERT) with Dropout\n",
        "class TextModel(nn.Module):\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super(TextModel, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.bert.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_inputs):\n",
        "        text_out = self.bert(**text_inputs).last_hidden_state[:, 0, :]\n",
        "        return self.fc(text_out)\n",
        "\n",
        "### CNN Model for LiDAR Images (BatchNorm + Dropout)\n",
        "class LiDARCNN(nn.Module):\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super(LiDARCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 56 * 56, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        return self.fc(x)\n",
        "\n",
        "### Final Multimodal Model with Cross-Attention\n",
        "class CrossAttentionFusionModel(nn.Module):\n",
        "    def __init__(self, num_time_series_features, seq_length, num_static_features, dropout=0.3):\n",
        "        super(CrossAttentionFusionModel, self).__init__()\n",
        "\n",
        "        self.time_series_model = TimeSeriesModel(num_time_series_features, seq_length, dropout)\n",
        "        self.static_model = StaticModel(num_static_features, dropout)\n",
        "        self.text_model = TextModel(dropout)\n",
        "        self.image_model = LiDARCNN(dropout)\n",
        "\n",
        "        # Cross-Attention Layers\n",
        "        self.attn_text_time = CrossAttention(embed_dim=128, num_heads=4, dropout=dropout)  # Text <-> Time-Series\n",
        "        self.attn_static_image = CrossAttention(embed_dim=128, num_heads=4, dropout=dropout)  # Static <-> Image\n",
        "        self.attn_time_static = CrossAttention(embed_dim=128, num_heads=4, dropout=dropout)  # Time-Series <-> Static\n",
        "\n",
        "        # Second-Level Cross-Attention\n",
        "        self.attn_fusion1 = CrossAttention(embed_dim=128, num_heads=4, dropout=dropout)  # Between attended_text_time and attended_static_image\n",
        "        self.attn_fusion2 = CrossAttention(embed_dim=128, num_heads=4, dropout=dropout)  # Between attended_time_static and attended_static_image\n",
        "\n",
        "\n",
        "\n",
        "        # Learnable Attention-Based Fusion Weights\n",
        "        self.attn_weights = nn.Linear(128 * 3, 3)\n",
        "\n",
        "        # Final Fusion Layer\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, time_series, static_features, text_inputs, image_data):\n",
        "        time_series_out = self.time_series_model(time_series)\n",
        "        static_out = self.static_model(static_features)\n",
        "        text_out = self.text_model(text_inputs)\n",
        "        image_out = self.image_model(image_data)\n",
        "\n",
        "        # Apply cross-attention (Text ↔ Time-Series, Static ↔ Image, Time-Series ↔ Static)\n",
        "        attended_text_time = self.attn_text_time(text_out, time_series_out)\n",
        "        attended_static_image = self.attn_static_image(static_out, image_out)\n",
        "        attended_time_static = self.attn_time_static(time_series_out, static_out)\n",
        "\n",
        "        # Apply second-level cross-attention for deeper interaction\n",
        "        attended_fusion1 = self.attn_fusion1(attended_text_time, attended_static_image)\n",
        "        attended_fusion2 = self.attn_fusion2(attended_time_static, attended_static_image)\n",
        "\n",
        "        # Weighted Fusion\n",
        "        weights = torch.softmax(self.attn_weights(torch.cat([attended_fusion1, attended_fusion2, attended_static_image], dim=1)), dim=1)\n",
        "        fused_weighted = (weights[:, 0].unsqueeze(1) * attended_fusion1 +\n",
        "                          weights[:, 1].unsqueeze(1) * attended_fusion2 +\n",
        "                          weights[:, 2].unsqueeze(1) * attended_static_image)\n",
        "\n",
        "        return self.fusion_layer(fused_weighted)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3_QdCuLASYa"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "del model\n",
        "del optimizer\n",
        "del train_loader\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "torch.cuda.reset_max_memory_cached()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q26xUl_S74lX"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # For multi-GPU\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Create PyTorch DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2,\n",
        "                          worker_init_fn=worker_init_fn, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize model\n",
        "num_time_series_features = 8\n",
        "seq_length = len(x_train.iloc[0, 0])  # Adjust based on your time-series length\n",
        "num_static_features = len(x_train.iloc[0, 8:-1])  # Adjust based on your dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CrossAttentionFusionModel(num_time_series_features, seq_length, num_static_features, dropout=0.3).to(device)\n",
        "\n",
        "# Loss function & optimizer\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)  # Using BCEWithLogitsLoss\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Best validation loss tracking\n",
        "best_val_loss = float(\"inf\")\n",
        "checkpoint_path = \"best_multimodal_cross_attention_model.pth\"\n",
        "\n",
        "# takes 1.5 min per epoch with A100\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfOkReUmAjB0"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
        "print(\"Loaded best model for evaluation!\")\n",
        "\n",
        "# Evaluate on test data\n",
        "all_labels, all_preds, all_probs = evaluate_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE5bouhdAoSE"
      },
      "outputs": [],
      "source": [
        "### Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "cm_rates = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_rates, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
        "                xticklabels=[\"No Delinquency\", \"Delinquent\"],\n",
        "                yticklabels=[\"No Delinquency\", \"Delinquent\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Normalized Confusion Matrix (Rates)\")\n",
        "plt.savefig(os.path.join(bookPath, \"multimodal_confusion_matrix_crossattention.pdf\"))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrFfCrIaAqP9"
      },
      "outputs": [],
      "source": [
        "### ROC Curve & AUC Score\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "auc_score = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_score:.3f})\", linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Diagonal reference line\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ROC Curve - Delinquency Prediction\")\n",
        "plt.legend(loc=4)\n",
        "plt.grid()\n",
        "plt.savefig(os.path.join(bookPath, \"multimodal_roc_curve_crossattention.pdf\"))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "### Classification Report\n",
        "print(\"Classification Report:\\n\", classification_report(all_labels, all_preds, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N47L67_fyIPG"
      },
      "source": [
        "We observe an improvement in the ROC AUC score."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}