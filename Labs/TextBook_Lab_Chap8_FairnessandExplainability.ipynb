{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Banking-Analytics-Lab/DLinBankingBook/blob/main/Labs/TextBook_Lab_Chap8_FairnessandExplainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b9f6a03",
      "metadata": {
        "id": "0b9f6a03"
      },
      "source": [
        "#  Fairness and Explainability\n",
        "This lab is divided into two parts.\n",
        "1. Fairness: We will assess the fairness of a credit scoring model\n",
        "2. Explainability: We will use the 'shap' library to generate explanations for the language model in the lab for Chapter 3.\n",
        "\n",
        "The two parts are independent meaining that the imports and code from one are not necessary to run the other.\n",
        "We start with fairness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf2408f5",
      "metadata": {
        "id": "bf2408f5"
      },
      "source": [
        "## Part 1: Fairness\n",
        "\n",
        "We start by importing necessary libraries and functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d5feebd",
      "metadata": {
        "id": "1d5feebd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import fairlearn\n",
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "%matplotlib inline\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import fairlearn\n",
        "from fairlearn.metrics import (\n",
        "    MetricFrame,\n",
        "    count,\n",
        "    equalized_odds_difference,\n",
        "    false_negative_rate,\n",
        "    false_positive_rate,\n",
        "    true_negative_rate,\n",
        "    true_positive_rate,\n",
        "    selection_rate,\n",
        ")\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a86e9789",
      "metadata": {
        "id": "a86e9789"
      },
      "source": [
        "In this section we will focus on assesing fairness of a credit scoring model.\n",
        "For this exercise we are going to assume we have already trained the model using variables such as income, credit score and interest rates. The target variable as two values that is, non-default, indicated by 0, and default, inidcated by 1.\n",
        "\n",
        "We are interested to know if this credit scoring model discriminates agains the sensitive attribute which is one of the variables. This variable is a proxy for race, so in essence we are measuring fairness with respect to the race of the people applying for credit.\n",
        "Race is a protected characteristic by law and can therefore not be used when training the model. There may however be hidden confounding variables, which make the model unfair. Our goal is to assess this.\n",
        "\n",
        "A dataset containing the neccessary information has been prepared\n",
        "\n",
        "We start by reading in the dataset, which contains four variables:\n",
        "- 'true_label': This it the ground truth label, that is, whether the customer defaulted or not.\n",
        "- Predicted label: The label which the model predicts: 'predicted_label'\n",
        "- 'predicted_probability'.  The probability of default predicted by the model. This is a number between 0 and 1.\n",
        "- 'sensitive_attribute': The sensitive attribute in the dataset which we want to study from the perspective of fairness. This variable has four levels\n",
        "  - 'a'\n",
        "  - 'b'\n",
        "  - 'c'\n",
        "  - 'd'   \n",
        "\n",
        "Note that the dataset was created for this exercise using mortgages with majorities from different ethnic profiles, using the first three digits of the postcode and census information.\n",
        "This should not be considered a representative sample of those sectors. It is an illustrative example of the techniques and methods, not a showcase of differences in these places.\n",
        "\n",
        "We start by reading in and inspecting the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d662f17",
      "metadata": {
        "id": "9d662f17"
      },
      "outputs": [],
      "source": [
        "# Read in the data\n",
        "fairness_df=pd.read_csv('FairnessDataFrame.csv')\n",
        "fairness_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc4b8964",
      "metadata": {
        "id": "fc4b8964"
      },
      "source": [
        "We look at the distribution of the groups in the sensitive attribute. The barplot below shows that most people belong to group 'b', followed by group 'c' and then 'a' and 'd' which have similar counts.\n",
        "We also calculate the default rate per group and see that group 'b' has the highest default rate and gorup 'c' the lowest, whereas groups 'a' and 'd' are similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312a5acd",
      "metadata": {
        "id": "312a5acd"
      },
      "outputs": [],
      "source": [
        "counts = fairness_df.groupby(['sensitive_attribute', 'true_label']).size().unstack(fill_value=0)\n",
        "counts.plot(kind='bar', stacked=False, color=['dimgray', 'darkgrey'])\n",
        "plt.xlabel('Sensitive Attribute')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Counts of True Label by Sensitive Attribute')\n",
        "plt.legend(title='True Label')\n",
        "plt.show()\n",
        "percentage_1 = counts[1] / (counts[0] + counts[1]) * 100\n",
        "print(percentage_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d8e0ca",
      "metadata": {
        "id": "e3d8e0ca"
      },
      "source": [
        "Our next task is to explore the predictions made by the model for the four groups in our dataset.\n",
        "For our analysis we are using the fairlearn library which allows us to inspect, measure and correct for fairness.\n",
        "\n",
        "We start by creating a metrics dictionary listing all the metrics we want to use. This can be expanded and customized at will. Although fairlearn has many built in metrics, we need to import some additional ones from sci-kit learn.\n",
        "\n",
        "Then we create a MetricFrame object indicating what metrics to use, and which variables for the true and predicted label and the sensitive feature. This object can be called in different ways to explore overall metrics, metrics by group as well as differences by group.\n",
        "\n",
        "In our exploration, we plot the per-group measures for each metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6268c632",
      "metadata": {
        "id": "6268c632"
      },
      "outputs": [],
      "source": [
        "metrics = {\n",
        "    \"Count\": count,\n",
        "    \"Accuracy\": accuracy_score,\n",
        "    \"Selection rate\": selection_rate,\n",
        "    \"Precision\": precision_score,\n",
        "    \"True positive rate\": true_positive_rate,\n",
        "    \"True negative rate\": true_negative_rate,\n",
        "    \"False positive rate\": false_positive_rate,\n",
        "    \"False negative rate\": false_negative_rate,\n",
        "\n",
        "}\n",
        "metric_frame = MetricFrame(\n",
        "    metrics=metrics, y_true=fairness_df['true_label'], y_pred=fairness_df['predicted_label'], sensitive_features=fairness_df['sensitive_attribute']\n",
        ")\n",
        "metric_frame.by_group.plot.bar(\n",
        "    subplots=True,\n",
        "    layout=[2, 4],\n",
        "    legend=False,\n",
        "    figsize=[12, 8],\n",
        "    color='darkgrey'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89c99f79",
      "metadata": {
        "id": "89c99f79"
      },
      "source": [
        "Looking at these plots, we can see some differences between the groups:\n",
        "\n",
        "* Count: Groups 'a' and 'd' are the smallest, followed by 'c' and 'b' which has the most people.\n",
        "* Accuracy: all gorups have similar accuracy although group 'c' is notably higher.  \n",
        "* Selection rate: This is the fraction of observations per group which the model predicts as default. Here we notice a clear descrepancy. Notably group 'c' has the lowest selection rate, meaning that the model is less likley to predict default for the people in this group. Groups 'a' and 'b' have the highest selection rate.  This measure is relevant for assessing the independence criteria.\n",
        "* Precision: This is the positive predicted value. Group 'b' has the highest value, followed by 'd', 'a' and 'c'. This measure is relevant for assessing the sufficiency criteria.\n",
        "* True positive rate: This is the recall. Group 'b' has the highest value and the other three all have similar values. This measure is relevant for assessing the separation criteria.\n",
        "* True negative rate: This is the specificity. All groups have very similar values.\n",
        "* False positive rate: This is also the type I error. Group 'c' has the lowest value and group 'd' the second lowest value. Groups 'a' and 'b' have the highest values. This measure is relevant for assessing the separation criteria.\n",
        "* False negative rate: This is also the type II error. Group 'c' has the highest value and group 'b' the lowest value. Groups 'a' and 'd' have similar values.\n",
        "\n",
        "We can also inspect the dataframe to get summary statistics about the differences, ratio, group minimum and maximum as follows.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d7361d2",
      "metadata": {
        "id": "0d7361d2"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'difference': metric_frame.difference(),\n",
        "              'ratio': metric_frame.ratio(),\n",
        "              'group_min': metric_frame.group_min(),\n",
        "              'group_max': metric_frame.group_max()}).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0618726",
      "metadata": {
        "id": "d0618726"
      },
      "outputs": [],
      "source": [
        "0.23/0.29"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74774abb",
      "metadata": {
        "id": "74774abb"
      },
      "source": [
        "Lets look more closely at the 3 fairness criteria.\n",
        "\n",
        "### Independence\n",
        "To assess the level of indepenece of the sensitive attribute in the model we measure the demopgraphic parity.\n",
        "\n",
        "First we measure the difference and then the ratio using the specialized methods in the fairlearn library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9990ecd0",
      "metadata": {
        "id": "9990ecd0"
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics import demographic_parity_difference\n",
        "from fairlearn.metrics import demographic_parity_ratio\n",
        "\n",
        "\n",
        "DPD = demographic_parity_difference(y_true=fairness_df['true_label'],\n",
        "                               y_pred=fairness_df['predicted_label'],\n",
        "                                sensitive_features=fairness_df['sensitive_attribute'],\n",
        "                                method=\"between_groups\")\n",
        "\n",
        "DPR = demographic_parity_ratio(y_true=fairness_df['true_label'],\n",
        "                               y_pred=fairness_df['predicted_label'],\n",
        "                                sensitive_features=fairness_df['sensitive_attribute'],\n",
        "                                method='between_groups')\n",
        "\n",
        "print(f'Demographic parity difference: {DPD:.3f}')\n",
        "print(f'Demographic parity ratio: {DPR:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad8968b0",
      "metadata": {
        "id": "ad8968b0"
      },
      "source": [
        "We see some slight evidence of demographic parity, as the DPD is greater than 0 and DPR is less than 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e4dde87",
      "metadata": {
        "id": "3e4dde87"
      },
      "source": [
        "### Separation\n",
        "Next we measure the separation using equalized odds and equal opportunity.\n",
        "\n",
        "Equalized odds looks at both false positive and true positive rates, and we can compute their difference and ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35482186",
      "metadata": {
        "id": "35482186"
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics import equalized_odds_difference\n",
        "from fairlearn.metrics import equalized_odds_ratio\n",
        "\n",
        "EOD=equalized_odds_difference(y_true=fairness_df['true_label'],\n",
        "                               y_pred=fairness_df['predicted_label'],\n",
        "                                sensitive_features=fairness_df['sensitive_attribute'],\n",
        "                                method=\"between_groups\")\n",
        "\n",
        "EOR=equalized_odds_ratio(y_true=fairness_df['true_label'],\n",
        "                               y_pred=fairness_df['predicted_label'],\n",
        "                                sensitive_features=fairness_df['sensitive_attribute'],\n",
        "                                method=\"between_groups\")\n",
        "\n",
        "print(f'Equalized odds difference: {EOD:.3f}')\n",
        "print(f'Equalized odds ratio: {EOR:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f97cf3",
      "metadata": {
        "id": "93f97cf3"
      },
      "source": [
        "Again, we see slight evidence of unfairness.\n",
        "\n",
        "Equal opportunity is based on the true positive rate only. We compute their difference and ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214ed0c6",
      "metadata": {
        "id": "214ed0c6"
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics import equal_opportunity_difference\n",
        "from fairlearn.metrics import equal_opportunity_ratio\n",
        "\n",
        "EOpD=equal_opportunity_difference(y_true=fairness_df['true_label'],\n",
        "                               y_pred=fairness_df['predicted_label'],\n",
        "                                sensitive_features=fairness_df['sensitive_attribute'],\n",
        "                                method=\"between_groups\")\n",
        "\n",
        "EOpR=equal_opportunity_ratio(y_true=fairness_df['true_label'],\n",
        "                               y_pred=fairness_df['predicted_label'],\n",
        "                                sensitive_features=fairness_df['sensitive_attribute'],\n",
        "                                method=\"between_groups\")\n",
        "print(f'Equal opportunity difference: {EOpD:.3f}')\n",
        "print(f'Equa opportunity ratio: {EOpR:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ba4e2b",
      "metadata": {
        "id": "a9ba4e2b"
      },
      "source": [
        "The results are very similar.\n",
        "\n",
        "\n",
        "#### ROC analysis of separation\n",
        "By plotting a ROC curve per group we can visally inspect the level of separation in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "717e0503",
      "metadata": {
        "id": "717e0503"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "groups = ['a','b','c','d']\n",
        "line_styles = ['-', '--', '-.', ':']\n",
        "line_color=['dimgray','gray','darkgrey','black'] # Different dash styles for each group\n",
        "\n",
        "for i, r in enumerate(groups):\n",
        "    indices = fairness_df['sensitive_attribute'] == r\n",
        "    y_true = fairness_df['true_label'][indices]\n",
        "    y_pred = fairness_df['predicted_probability'][indices]\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=line_color[i % len(line_color)], linestyle=line_styles[i % len(line_styles)], label=f'Group {r} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve by Group')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f388f67d",
      "metadata": {
        "id": "f388f67d"
      },
      "source": [
        "By shading the are that is under all the curves shows the feasible region where seperation could be achieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19e2e3de",
      "metadata": {
        "id": "19e2e3de"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "roc_curves = []\n",
        "\n",
        "groups = ['a','b','c','d']\n",
        "line_styles = ['-', '--', '-.', ':']\n",
        "line_color=['dimgray','gray','darkgrey','black']\n",
        "\n",
        "for i,r in enumerate(groups):\n",
        "    indices = fairness_df['sensitive_attribute'] == r\n",
        "    y_true = fairness_df['true_label'][indices]\n",
        "    y_pred = fairness_df['predicted_probability'][indices]\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=line_color[i % len(line_color)], linestyle=line_styles[i % len(line_styles)],label=f'Group {r} (AUC = {roc_auc:.3f})')\n",
        "    roc_curves.append((fpr, tpr))\n",
        "\n",
        "\n",
        "common_fpr = np.linspace(0, 1, 500)\n",
        "interp_tprs = [np.interp(common_fpr, fpr, tpr) for fpr, tpr in roc_curves]\n",
        "min_tpr = np.min(interp_tprs, axis=0)\n",
        "\n",
        "plt.fill_between(common_fpr, min_tpr, color='gainsboro', alpha=0.5, label='Feasible Area')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve by Group with Intersection Area Shaded')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed5e73d9",
      "metadata": {
        "id": "ed5e73d9"
      },
      "source": [
        "### Sufficiency\n",
        "Sufficiency requires independence between the sensitive attribute and the target variable, in the sense that people who get the same decision by the model have parity regardless of which group of $A$ they belong to.\n",
        "\n",
        "We assess sufficiecny by measuring the predictive rate parity which takes into account the precision of the models per group of the sensitive feature. We already have this measure in our dataframe.\n",
        "\n",
        "We see in the output below that the predictive reate parity close to 0, meaning that in this regard the model fullfills the sufficiency fairness criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4fe5a60",
      "metadata": {
        "id": "c4fe5a60"
      },
      "outputs": [],
      "source": [
        "metric_frame.difference()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe0ce58",
      "metadata": {
        "id": "5fe0ce58"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e877fbf",
      "metadata": {
        "id": "9e877fbf"
      },
      "source": [
        "### Explainability\n",
        "To create the plots in the book\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RE5QopHq_RiW",
      "metadata": {
        "id": "RE5QopHq_RiW"
      },
      "outputs": [],
      "source": [
        "%%Generated\n",
        "\n",
        "This chapter focuses on the critical topic of model explainability in the context of financial text analysis, demonstrating how to interpret the predictions of a sophisticated transformer model. Building on the previous chapter's case study, the notebook utilizes a pre-trained DistilBERT model that classifies Federal Reserve speeches as indicative of economic expansion or contraction. The primary goal is to move beyond \"black box\" predictions and provide actionable insights into why the model makes a particular decision. This is achieved through the implementation of SHAP (SHapley Additive exPlanations), a state-of-the-art, game-theoretic approach to explainable AI that assigns an importance value to each feature (in this case, words or tokens) for a given prediction.\n",
        "\n",
        "The notebook guides the user through the process of loading the fine-tuned classification model and a new set of recent, unseen Federal Reserve speeches. It then sets up a SHAP Explainer using the model's prediction pipeline. The core of the lab involves generating SHAP values for these new speeches to understand which specific words or phrases contributed most significantly to the model's output. The chapter culminates in the visualization of these explanations, using SHAP's text and bar plots to highlight the tokens that either support or oppose the predicted economic outcome. This hands-on application of XAI (Explainable AI) not only reinforces the principles of responsible AI development but also aligns with the Banking Analytics Lab's commitment to providing transparent and rigorous data-driven solutions for the financial industry.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "609dcc8b",
      "metadata": {
        "id": "609dcc8b"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shap\n",
        "\n",
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# Huggingface\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers.pipelines import pipeline\n",
        "from transformers import set_seed\n",
        "from datasets import load_dataset, Dataset, Value, ClassLabel, Features, load_from_disk\n",
        "#import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bJaccYkIWk_",
      "metadata": {
        "id": "1bJaccYkIWk_"
      },
      "outputs": [],
      "source": [
        "!gdown --id \"1_KqbURS8BVRck9mj4DXjuRbD9SJIuz5r\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4k8fM4MlOrxH",
      "metadata": {
        "id": "4k8fM4MlOrxH"
      },
      "outputs": [],
      "source": [
        "!gdown --id '1ps8vhnNrMFyNbhLtO79UbsNEpiNZesEd'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KZKNvO7bPCF0",
      "metadata": {
        "id": "KZKNvO7bPCF0"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/FEDSppechModel.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vO_MdLNpPvBj",
      "metadata": {
        "id": "vO_MdLNpPvBj"
      },
      "outputs": [],
      "source": [
        "!gdown --id '1cqnUWh84nbbwfkcEfUGyPztWOtIeqOEf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OFnFhcmuP1GQ",
      "metadata": {
        "id": "OFnFhcmuP1GQ"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/TokenizedData.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc279d3",
      "metadata": {
        "id": "2fc279d3"
      },
      "outputs": [],
      "source": [
        "FEDspeechesSHAP = pd.read_csv('/content/FEDSpeechesSHAP.csv')\n",
        "FEDspeechesSHAP.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ad22c1",
      "metadata": {
        "id": "f4ad22c1"
      },
      "outputs": [],
      "source": [
        "\n",
        "speech1 = FEDspeechesSHAP.iloc[0]['text_cleaned']\n",
        "speech2 = FEDspeechesSHAP.iloc[1]['text_cleaned']\n",
        "speech3 = FEDspeechesSHAP.iloc[2]['text_cleaned']\n",
        "\n",
        "print(len(speech1))\n",
        "print(len(speech2))\n",
        "print(len(speech3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ae864a",
      "metadata": {
        "id": "15ae864a"
      },
      "outputs": [],
      "source": [
        "# Create the dataset\n",
        "fed_speech_data = Dataset.from_pandas(FEDspeechesSHAP.loc[:,['text_cleaned']])\n",
        "fed_speech_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d61443",
      "metadata": {
        "id": "d9d61443"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\",  processing_class=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32aadca1",
      "metadata": {
        "id": "32aadca1"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text_cleaned\"], max_length=512, truncation=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf26676",
      "metadata": {
        "id": "9cf26676"
      },
      "outputs": [],
      "source": [
        "tokenized_fed_speech_data = fed_speech_data.map(preprocess_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2F5frS1ckKOa",
      "metadata": {
        "id": "2F5frS1ckKOa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd816e9e",
      "metadata": {
        "id": "dd816e9e"
      },
      "outputs": [],
      "source": [
        "pipe = transformers.pipelines.pipeline('text-classification', truncation= True,model='FEDSppechModel', tokenizer=tokenizer, top_k=None,\n",
        "                                  return_all_scores=True, device=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c21f29",
      "metadata": {
        "id": "10c21f29"
      },
      "outputs": [],
      "source": [
        "pipe(tokenized_fed_speech_data['text_cleaned'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eeabbc7",
      "metadata": {
        "id": "3eeabbc7"
      },
      "outputs": [],
      "source": [
        "explainer = shap.Explainer(pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c6417a6",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9c6417a6"
      },
      "outputs": [],
      "source": [
        "shap_values=explainer(tokenized_fed_speech_data['text_cleaned'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfdbd36d",
      "metadata": {
        "id": "cfdbd36d"
      },
      "outputs": [],
      "source": [
        "shap_values.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e067d98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8e067d98",
        "outputId": "b771f4cd-7d14-4e91-d16e-cf9c68214911"
      },
      "outputs": [],
      "source": [
        "shap.plots.text(shap_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85999bef",
      "metadata": {
        "id": "85999bef"
      },
      "outputs": [],
      "source": [
        "shap.plots.bar(shap_values[:,:,1].mean(0),order=shap.Explanation.argsort.flip, max_display=15)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}